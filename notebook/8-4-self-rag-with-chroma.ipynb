{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Implementing Self RAG with LangGraph\n\n## Introduction\n\n**Self-RAG** is an innovative strategy for **Retrieval-Augmented Generation (RAG)** that integrates `self-reflection` and `self-grading` mechanisms to enhance the accuracy and relevance of both retrieved documents and generated responses. This approach leverages the capabilities of large language models (LLMs) to autonomously assess and improve the quality of information retrieval and generation processes, ensuring that the final output is both reliable and pertinent to the user's query.\n\nIn the **Self-RAG** framework, several key decisions are systematically made to optimize the interaction between retrieval and generation:\n\n1. **Retrieval Decision**: Determines whether additional document chunks should be retrieved based on the initial question or the current generation output. This decision helps in managing the scope of information and ensures that only relevant data is considered.\n\n2. **Relevance Assessment of Retrieved Passages**: Evaluates each retrieved document chunk to ascertain its usefulness in addressing the user's question. By classifying documents as relevant or irrelevant, the system filters out noise and focuses on high-quality information sources.\n\n3. **Verification of LLM Generation Against Retrieved Chunks**: Assesses whether the statements generated by the LLM are fully supported, partially supported, or lack support from the retrieved documents. This step is crucial in identifying and mitigating hallucinations, thereby maintaining factual accuracy in the generated responses.\n\n4. **Usefulness Evaluation of Generated Responses**: Measures the overall usefulness of the LLM's generation in resolving the user's question. By scoring the response, the system ensures that the final answer is not only accurate but also effectively addresses the user's intent.\n\nThrough these self-regulatory steps, **Self-RAG** enhances the traditional RAG approach by embedding quality control directly into the retrieval and generation pipeline. This results in more trustworthy and contextually appropriate responses, making Self-RAG a robust solution for applications requiring high levels of information accuracy and reliability.","metadata":{}},{"cell_type":"markdown","source":"## Package Installation\n\nThis code block installs several Python libraries using `pip`, which are commonly used in building and working with language models and AI applications:\n\n1. **langchain-openai**: A library for integrating OpenAI's language models with the LangChain framework.\n2. **langchain-anthropic**: A library for integrating Anthropic's language models with LangChain.\n3. **langchain_community**: A community-driven library providing additional tools and integrations for LangChain.\n4. **langchain_experimental**: A library containing experimental features and extensions for LangChain.\n5. **langgraph**: A library for creating and managing graphs of language model interactions.\n6. **chromadb**: A vector database for storing and querying embeddings, often used in AI applications.\n7. **duckduckgo_search**: A library for performing web searches using DuckDuckGo.\n\nThese libraries are essential for building advanced AI applications, particularly those involving natural language processing, retrieval-augmented generation (RAG), and agent-based systems. The `-qU` flag ensures the installations are quiet (non-verbose) and upgrade existing installations if necessary.","metadata":{}},{"cell_type":"code","source":"!pip install -qU langchain-openai\n!pip install -qU langchain-anthropic\n!pip install -qU langchain_community\n!pip install -qU langchain_experimental\n!pip install -qU langgraph\n!pip install -qU chromadb\n!pip install -qU duckduckgo_search","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T18:00:18.725454Z","iopub.execute_input":"2025-01-16T18:00:18.725777Z","iopub.status.idle":"2025-01-16T18:01:35.467467Z","shell.execute_reply.started":"2025-01-16T18:00:18.725752Z","shell.execute_reply":"2025-01-16T18:01:35.466135Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Importing Libraries and Setting Up the Vector Store\n\nThis code block imports the necessary libraries and modules for the workflow. It sets up the OpenAI embeddings using a securely loaded API key from Kaggle secrets. It then defines a list of URLs to be indexed, loads the documents from these URLs, splits them into smaller chunks for better retrieval, and adds them to a Chroma vector store for efficient searching.","metadata":{}},{"cell_type":"code","source":"from langchain import hub\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_openai import OpenAIEmbeddings, ChatOpenAI\nfrom langgraph.graph import END, StateGraph, START\n\nfrom typing import List\nfrom typing_extensions import TypedDict\nfrom pydantic import BaseModel, Field\nfrom kaggle_secrets import UserSecretsClient\n\n# Load OpenAI API Key securely from Kaggle secrets\nmy_api_key = UserSecretsClient().get_secret(\"my-openai-api-key\")\n\n# Use OpenAI embeddings for vectorization\nembed = OpenAIEmbeddings(model=\"text-embedding-3-small\", api_key=my_api_key)\n\n# Retriever\n# Define URLs of blog posts to index\nurls = [\n    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n]\n\n# Load documents from URLs\ndocs = [WebBaseLoader(url).load() for url in urls]\ndocs_list = [item for sublist in docs for item in sublist]\n\n# Split documents into smaller chunks for better retrieval\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=250, chunk_overlap=0)\ndoc_splits = text_splitter.split_documents(docs_list)\n\n# Add documents to vector database (Chroma)\nvectorstore = Chroma.from_documents(documents=doc_splits, collection_name=\"rag-chroma\", embedding=embed)\nretriever = vectorstore.as_retriever()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T18:58:46.787286Z","iopub.execute_input":"2025-01-16T18:58:46.78768Z","iopub.status.idle":"2025-01-16T18:58:50.273784Z","shell.execute_reply.started":"2025-01-16T18:58:46.787654Z","shell.execute_reply":"2025-01-16T18:58:50.272448Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Retrieval Grader\n\nThis code block defines a retrieval grader using a Pydantic model to assess the relevance of retrieved documents to a user's question. It initializes an OpenAI LLM to perform the grading based on a system prompt that instructs the model to provide a binary score indicating relevance.","metadata":{}},{"cell_type":"code","source":"# Retrieval Grader\n# Define a Pydantic model to grade document relevance\nclass GradeDocuments(BaseModel):\n    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n\n    binary_score: str = Field(description=\"Documents are relevant to the question, 'yes' or 'no'\")\n\n# Initialize LLM for grading\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, api_key=my_api_key)\nstructured_llm_grader = llm.with_structured_output(GradeDocuments)\n\n# Define system prompt for grading relevance\nsystem = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\ngrade_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system),\n        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n    ]\n)\n\n# Create a chain for grading document relevance\nretrieval_grader = grade_prompt | structured_llm_grader\n\n# Test retrieval grader with a sample question\nquestion = \"agent memory\"\ndocs = retriever.invoke(question)\ndoc_txt = docs[1].page_content\n\nresult = retrieval_grader.invoke({\"question\": question, \"document\": doc_txt})\nprint(result)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T18:58:50.275561Z","iopub.execute_input":"2025-01-16T18:58:50.275972Z","iopub.status.idle":"2025-01-16T18:58:51.224782Z","shell.execute_reply.started":"2025-01-16T18:58:50.275943Z","shell.execute_reply":"2025-01-16T18:58:51.223525Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Generation with Retrieval-Augmented Generation (RAG) Chain\n\nThis code block sets up the generation phase using a Retrieval-Augmented Generation (RAG) chain. It pulls a RAG prompt from LangChain's hub, initializes an OpenAI LLM for generation, and creates a chain that generates answers based on the retrieved documents and the user's question.","metadata":{}},{"cell_type":"code","source":"# Generate\n# Pull a RAG prompt from the hub\nprompt = hub.pull(\"rlm/rag-prompt\")\n\n# Initialize LLM for generation\nllm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0, api_key=my_api_key)\n\n# Create a RAG chain for generating answers\nrag_chain = prompt | llm | StrOutputParser()\ngeneration = rag_chain.invoke({\"context\": docs, \"question\": question})\nprint(generation)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T18:58:51.227064Z","iopub.execute_input":"2025-01-16T18:58:51.227448Z","iopub.status.idle":"2025-01-16T18:58:53.428451Z","shell.execute_reply.started":"2025-01-16T18:58:51.227415Z","shell.execute_reply":"2025-01-16T18:58:53.42699Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Show what the prompt looks like\nprompt = hub.pull(\"rlm/rag-prompt\").pretty_print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T18:59:45.023088Z","iopub.execute_input":"2025-01-16T18:59:45.023514Z","iopub.status.idle":"2025-01-16T18:59:45.103348Z","shell.execute_reply.started":"2025-01-16T18:59:45.02347Z","shell.execute_reply":"2025-01-16T18:59:45.102071Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Hallucination Grader\n\nThis code block defines a hallucination grader to assess whether the generated answers are grounded in the retrieved documents. It uses a Pydantic model for binary scoring and initializes an OpenAI LLM with a system prompt that instructs the model to determine if the generation is supported by the provided facts.","metadata":{}},{"cell_type":"code","source":"# Hallucination Grader\n# Define a Pydantic model to grade hallucinations in generated answers\nclass GradeHallucinations(BaseModel):\n    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n\n    binary_score: str = Field(description=\"Answer is grounded in the facts, 'yes' or 'no'\")\n\n# Initialize LLM for hallucination grading\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, api_key=my_api_key)\nstructured_llm_grader = llm.with_structured_output(GradeHallucinations)\n\n# Define system prompt for hallucination grading\nsystem = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n \n     Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\"\"\nhallucination_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system),\n        (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n    ]\n)\n\n# Create a chain for hallucination grading\nhallucination_grader = hallucination_prompt | structured_llm_grader\nresult = hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})\nprint(result)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T18:58:53.430192Z","iopub.execute_input":"2025-01-16T18:58:53.430581Z","iopub.status.idle":"2025-01-16T18:58:54.198816Z","shell.execute_reply.started":"2025-01-16T18:58:53.430544Z","shell.execute_reply":"2025-01-16T18:58:54.197711Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Answer Grader\n\nThis code block defines an answer grader to evaluate whether the generated answer adequately addresses the user's question. It utilizes a Pydantic model for binary scoring and sets up an OpenAI LLM with a system prompt guiding the model to determine if the answer resolves the question.","metadata":{}},{"cell_type":"code","source":"# Answer Grader\n# Define a Pydantic model to grade whether the answer addresses the question\nclass GradeAnswer(BaseModel):\n    \"\"\"Binary score to assess answer addresses question.\"\"\"\n\n    binary_score: str = Field(description=\"Answer addresses the question, 'yes' or 'no'\")\n\n# Initialize LLM for answer grading\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, api_key=my_api_key)\nstructured_llm_grader = llm.with_structured_output(GradeAnswer)\n\n# Define system prompt for answer grading\nsystem = \"\"\"You are a grader assessing whether an answer addresses / resolves a question \\n \n     Give a binary score 'yes' or 'no'. 'Yes' means that the answer resolves the question.\"\"\"\nanswer_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system),\n        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n    ]\n)\n\n# Create a chain for answer grading\nanswer_grader = answer_prompt | structured_llm_grader\nresult = answer_grader.invoke({\"question\": question, \"generation\": generation})\nprint(result)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T18:58:54.19981Z","iopub.execute_input":"2025-01-16T18:58:54.200127Z","iopub.status.idle":"2025-01-16T18:58:54.938684Z","shell.execute_reply.started":"2025-01-16T18:58:54.200101Z","shell.execute_reply":"2025-01-16T18:58:54.937517Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Question Re-writer\n\nThis code block sets up a question re-writer to improve the user's input question for optimized retrieval from the vector store. It initializes an OpenAI LLM with a system prompt that instructs the model to rephrase the question to better capture its semantic intent.","metadata":{}},{"cell_type":"code","source":"# Question Re-writer\n# Initialize LLM for question rewriting\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, api_key=my_api_key)\n\n# Define system prompt for question rewriting\nsystem = \"\"\"You are a question re-writer that converts an input question to a better version that is optimized \\n \n     for vectorstore retrieval. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\nre_write_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system),\n        (\n            \"human\",\n            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n        ),\n    ]\n)\n\n# Create a chain for question rewriting\nquestion_rewriter = re_write_prompt | llm | StrOutputParser()\nresult = question_rewriter.invoke({\"question\": question})\nprint(result)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T18:58:54.940048Z","iopub.execute_input":"2025-01-16T18:58:54.940439Z","iopub.status.idle":"2025-01-16T18:58:55.797797Z","shell.execute_reply.started":"2025-01-16T18:58:54.940404Z","shell.execute_reply":"2025-01-16T18:58:55.796636Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Defining Graph State and Workflow Nodes\n\nThis code block defines the state and nodes for a state graph workflow using LangGraph. It specifies the structure of the graph state, implements functions for each node (retrieve, generate, grade documents, transform query), and defines the decision logic for transitioning between nodes based on the grading results.","metadata":{}},{"cell_type":"code","source":"from typing import List, Dict, Literal\nfrom typing_extensions import TypedDict\nfrom langchain.schema import Document\nfrom IPython.display import Image, display\n\n# Graph State\n# Define the state of the graph as a TypedDict\nclass GraphState(TypedDict):\n    \"\"\"\n    Represents the state of our graph.\n\n    Attributes:\n        question: The user's question.\n        generation: The LLM's generated answer.\n        documents: List of retrieved documents.\n    \"\"\"\n    question: str\n    generation: str\n    documents: List[Document]\n\n# Nodes\ndef retrieve_node(state: GraphState) -> GraphState:\n    \"\"\"\n    Retrieve documents relevant to the user's question.\n\n    Args:\n        state (GraphState): The current graph state.\n\n    Returns:\n        GraphState: Updated state with retrieved documents.\n    \"\"\"\n    print(\"---RETRIEVE---\")\n    question = state[\"question\"]\n\n    # Retrieve documents using the retriever\n    documents = retriever.invoke(question)\n    return {\"documents\": documents, \"question\": question}\n\ndef generate_node(state: GraphState) -> GraphState:\n    \"\"\"\n    Generate an answer using the RAG chain.\n\n    Args:\n        state (GraphState): The current graph state.\n\n    Returns:\n        GraphState: Updated state with generated answer.\n    \"\"\"\n    print(\"---GENERATE---\")\n    question = state[\"question\"]\n    documents = state[\"documents\"]\n\n    # Generate answer using the RAG chain\n    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n\ndef grade_documents_node(state: GraphState) -> GraphState:\n    \"\"\"\n    Grade the relevance of retrieved documents to the question.\n\n    Args:\n        state (GraphState): The current graph state.\n\n    Returns:\n        GraphState: Updated state with filtered relevant documents.\n    \"\"\"\n    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n    question = state[\"question\"]\n    documents = state[\"documents\"]\n\n    # Filter documents based on relevance\n    filtered_docs = []\n    for d in documents:\n        score = retrieval_grader.invoke(\n            {\"question\": question, \"document\": d.page_content}\n        )\n        grade = score.binary_score\n        if grade == \"yes\":\n            print(\"---GRADE: DOCUMENT RELEVANT---\")\n            filtered_docs.append(d)\n        else:\n            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n            continue\n    return {\"documents\": filtered_docs, \"question\": question}\n\ndef transform_query_node(state: GraphState) -> GraphState:\n    \"\"\"\n    Transform the user's question into a better version for retrieval.\n\n    Args:\n        state (GraphState): The current graph state.\n\n    Returns:\n        GraphState: Updated state with a rephrased question.\n    \"\"\"\n    print(\"---TRANSFORM QUERY---\")\n    question = state[\"question\"]\n    documents = state[\"documents\"]\n\n    # Rewrite the question for better retrieval\n    better_question = question_rewriter.invoke({\"question\": question})\n    return {\"documents\": documents, \"question\": better_question}\n\n# Edges\ndef decide_to_generate(state: GraphState) -> Literal[\"transform_query_node\", \"generate_node\"]:\n    \"\"\"\n    Decide whether to generate an answer or rephrase the question.\n\n    Args:\n        state (GraphState): The current graph state.\n\n    Returns:\n        Literal[\"transform_query_node\", \"generate_node\"]: Decision for the next node.\n    \"\"\"\n    print(\"---ASSESS GRADED DOCUMENTS---\")\n    filtered_documents = state[\"documents\"]\n\n    if not filtered_documents:\n        # If no relevant documents, rephrase the question\n        print(\"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\")\n        return \"transform_query_node\"\n    else:\n        # If relevant documents exist, generate an answer\n        print(\"---DECISION: GENERATE---\")\n        return \"generate_node\"\n\ndef decide_generation_useful(state: GraphState) -> Literal[\"generate_node\", \"transform_query_node\", END]:\n    \"\"\"\n    Decide whether the generated answer is useful or needs to be regenerated.\n\n    Args:\n        state (GraphState): The current graph state.\n\n    Returns:\n        Literal[\"generate_node\", \"transform_query_node\", END]: Decision for the next node.\n    \"\"\"\n    print(\"---CHECK HALLUCINATIONS---\")\n    question = state[\"question\"]\n    documents = state[\"documents\"]\n    generation = state[\"generation\"]\n\n    # Check if the generation is grounded in the documents\n    score = hallucination_grader.invoke({\"documents\": documents, \"generation\": generation})\n    grade = score.binary_score\n\n    if grade == \"yes\":\n        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n        # Check if the generation addresses the question\n        print(\"---GRADE GENERATION vs QUESTION---\")\n        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n        grade = score.binary_score\n        if grade == \"yes\":\n            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n            return END\n        else:\n            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n            return \"transform_query_node\"\n    else:\n        print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n        return \"generate_node\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T18:58:55.799085Z","iopub.execute_input":"2025-01-16T18:58:55.799431Z","iopub.status.idle":"2025-01-16T18:58:55.813329Z","shell.execute_reply.started":"2025-01-16T18:58:55.799405Z","shell.execute_reply":"2025-01-16T18:58:55.811837Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Building the Workflow Graph\n\nThis code block constructs the workflow graph using the previously defined nodes and decision functions. It establishes the flow of the workflow by adding nodes and defining the edges between them. Additionally, it attempts to visualize the graph if the necessary dependencies are available.","metadata":{}},{"cell_type":"code","source":"# Build Graph\nworkflow = StateGraph(GraphState)\n\n# Define the nodes\nworkflow.add_node(\"retrieve_node\", retrieve_node)                # Retrieve documents\nworkflow.add_node(\"grade_documents_node\", grade_documents_node)  # Grade document relevance\nworkflow.add_node(\"generate_node\", generate_node)                # Generate answer\nworkflow.add_node(\"transform_query_node\", transform_query_node)  # Transform query\n\n# Build graph edges\nworkflow.add_edge(START, \"retrieve_node\")\nworkflow.add_edge(\"retrieve_node\", \"grade_documents_node\")\nworkflow.add_conditional_edges(\"grade_documents_node\", decide_to_generate, [\"transform_query_node\", \"generate_node\"])\nworkflow.add_edge(\"transform_query_node\", \"retrieve_node\")\nworkflow.add_conditional_edges(\"generate_node\", decide_generation_useful, [\"generate_node\", \"transform_query_node\", END])\n\n# Compile the workflow\napp = workflow.compile()\n\n# Visualize the graph (optional, requires additional dependencies)\ntry:\n    display(Image(app.get_graph(xray=True).draw_mermaid_png()))\nexcept Exception:\n    pass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T18:58:55.816234Z","iopub.execute_input":"2025-01-16T18:58:55.816544Z","iopub.status.idle":"2025-01-16T18:58:56.715144Z","shell.execute_reply.started":"2025-01-16T18:58:55.81652Z","shell.execute_reply":"2025-01-16T18:58:56.713953Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Helper Functions for Serialization\n\nThis code block includes helper functions to suppress specific warnings and convert non-serializable objects into serializable formats (such as dictionaries). This ensures that the outputs from the workflow nodes can be easily serialized and displayed as JSON.","metadata":{}},{"cell_type":"code","source":"import json\nimport warnings\n\n# Suppress LangSmith API key warning (if not using LangSmith)\nwarnings.filterwarnings(\"ignore\", category=UserWarning, message=\"API key must be provided when using hosted LangSmith API\")\n\n# Helper function to convert non-serializable objects to dictionaries\ndef convert_to_serializable(obj):\n    if hasattr(obj, \"dict\"):              # Check if the object has a .dict() method\n        return obj.dict()\n    elif isinstance(obj, (list, tuple)):  # Handle lists and tuples\n        return [convert_to_serializable(item) for item in obj]\n    elif isinstance(obj, dict):           # Handle dictionaries\n        return {key: convert_to_serializable(value) for key, value in obj.items()}\n    else:                                 # Return the object as-is if it's already serializable\n        return obj","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T18:58:56.716441Z","iopub.execute_input":"2025-01-16T18:58:56.716734Z","iopub.status.idle":"2025-01-16T18:58:56.723469Z","shell.execute_reply.started":"2025-01-16T18:58:56.716711Z","shell.execute_reply":"2025-01-16T18:58:56.722487Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example 1: Question about agent memory\ninputs = {\"question\": \"Explain how the different types of agent memory work?\"}\nfor output in app.stream(inputs):\n    for key, value in output.items():\n        print(f\"Output from node '{key}':\")\n        print(\"-\"*80)\n        # Convert non-serializable objects to dictionaries\n        serializable_value = convert_to_serializable(value)\n        # Print the serialized value as a JSON string\n        print(json.dumps(serializable_value, indent=2))\n    print(\"=\"*80)\n\n# Print the final generation\nprint(f\"Final Generation:\\n{value['generation']}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T18:58:56.724425Z","iopub.execute_input":"2025-01-16T18:58:56.724673Z","iopub.status.idle":"2025-01-16T18:59:02.599785Z","shell.execute_reply.started":"2025-01-16T18:58:56.72465Z","shell.execute_reply":"2025-01-16T18:59:02.598599Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example 2: Question about adversarial attacks on LLMs\ninputs = {\"question\": \"How can adversarial attacks affect large language models?\"}\nfor output in app.stream(inputs):\n    for key, value in output.items():\n        print(f\"Output from node '{key}':\")\n        print(\"-\"*80)\n        # Convert non-serializable objects to dictionaries\n        serializable_value = convert_to_serializable(value)\n        # Print the serialized value as a JSON string\n        print(json.dumps(serializable_value, indent=2))\n    print(\"=\"*80)\n\n# Print the final generation\nprint(f\"Final Generation:\\n{value['generation']}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T18:59:02.60091Z","iopub.execute_input":"2025-01-16T18:59:02.601221Z","iopub.status.idle":"2025-01-16T18:59:09.760387Z","shell.execute_reply.started":"2025-01-16T18:59:02.601193Z","shell.execute_reply":"2025-01-16T18:59:09.759074Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example 3: Question about agentic agents\ninputs = {\"question\": \"What are the key components of an agentic agent?\"}\nfor output in app.stream(inputs):\n    for key, value in output.items():\n        print(f\"Output from node '{key}':\")\n        print(\"-\"*80)\n        # Convert non-serializable objects to dictionaries\n        serializable_value = convert_to_serializable(value)\n        # Print the serialized value as a JSON string\n        print(json.dumps(serializable_value, indent=2))\n    print(\"=\"*80)\n\n# Print the final generation\nprint(f\"Final Generation:\\n{value['generation']}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T18:59:09.761761Z","iopub.execute_input":"2025-01-16T18:59:09.762148Z","iopub.status.idle":"2025-01-16T18:59:15.392649Z","shell.execute_reply.started":"2025-01-16T18:59:09.762122Z","shell.execute_reply":"2025-01-16T18:59:15.391266Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Conclusion\n\nThe provided code sets up a comprehensive workflow for processing user questions using a Retrieval-Augmented Generation (RAG) approach. It involves installing necessary packages, loading and indexing documents, grading the relevance of retrieved documents, generating answers, checking for hallucinations, and ensuring that the final answer addresses the user's question. The workflow is structured as a state graph, allowing for flexible decision-making and iterative improvements to the query and generation process.","metadata":{}}]}