{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Implementing Corrective RAG: Improving Document Relevance in Language Models\n\n## Introduction\n\nIn the rapidly evolving landscape of natural language processing, **Retrieval-Augmented Generation (RAG)** has emerged as a powerful framework for enhancing the capabilities of language models by integrating external knowledge sources. Traditional RAG systems retrieve relevant documents to inform and improve the generation of responses, but they often lack mechanisms to assess the quality and relevance of the retrieved information. This limitation can lead to the generation of inaccurate or irrelevant answers, especially when the retrieved documents do not sufficiently address the user's query.\n\nTo address this challenge, the concept of **Corrective RAG (CRAG)** has been introduced. `CRAG` enhances the RAG framework by incorporating self-reflection and self-grading mechanisms to evaluate the relevance of retrieved documents. By assessing each document's pertinence to the user's question, `CRAG` ensures that only the most relevant information is utilized in generating responses. This approach not only improves the accuracy of the generated answers but also reduces the reliance on additional data sources.\n\nIn this example, we implement a simplified version of the `CRAG` strategy using LangGraph and LangChain. While the full `CRAG` methodology involves steps such as knowledge refinement and partitioning documents into \"knowledge strips,\" this implementation focuses on the core idea of grading document relevance and supplementing retrieval with web searches when necessary. By leveraging tools like DuckDuckGo for web searches and incorporating query rewriting for optimized searches, this workflow demonstrates how `CRAG` principles can be effectively applied to enhance RAG systems.","metadata":{}},{"cell_type":"markdown","source":"## Installing Required Packages\n\nThis code block installs the necessary Python packages quietly (`-q`) and upgrades them if they are already installed (`-U`). The packages include various components of `LangChain`, `LangGraph`, `ChromaDB`, and `DuckDuckGo` search tools, which are essential for building and running the Retrieval-Augmented Generation (RAG) workflow.","metadata":{}},{"cell_type":"code","source":"!pip install -qU langchain-openai\n!pip install -qU langchain-anthropic\n!pip install -qU langchain_community\n!pip install -qU langchain_experimental\n!pip install -qU langgraph\n!pip install -qU chromadb\n!pip install -qU duckduckgo_search","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-16T17:19:01.183516Z","iopub.execute_input":"2025-01-16T17:19:01.183865Z","iopub.status.idle":"2025-01-16T17:20:17.259106Z","shell.execute_reply.started":"2025-01-16T17:19:01.183834Z","shell.execute_reply":"2025-01-16T17:20:17.257733Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Loading and Preparing Documents\n\nThis block imports necessary modules and sets up the environment to load, split, and vectorize documents from specified URLs. It securely retrieves the OpenAI API key from Kaggle secrets, uses OpenAI embeddings for vectorization, loads documents from the provided URLs, splits them into smaller chunks for efficient processing, and stores them in a Chroma vector database for later retrieval.","metadata":{}},{"cell_type":"code","source":"from langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_openai import OpenAIEmbeddings\nfrom kaggle_secrets import UserSecretsClient\n\n# Load OpenAI API Key securely from Kaggle secrets\nmy_api_key = UserSecretsClient().get_secret(\"my-openai-api-key\")\n\n# Use OpenAI embeddings for vectorization\nembed = OpenAIEmbeddings(model=\"text-embedding-3-small\", api_key=my_api_key)\n\n# URLs of the blog posts to index\nurls = [\n    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n]\n\n# Load documents from the URLs\ndocs = [WebBaseLoader(url).load() for url in urls]\ndocs_list = [item for sublist in docs for item in sublist]\n\n# Split documents into smaller chunks for processing\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=250, chunk_overlap=0)\ndoc_splits = text_splitter.split_documents(docs_list)\n\n# Add documents to Chroma vector database\nvectorstore = Chroma.from_documents(documents=doc_splits, collection_name=\"rag-chroma\", embedding=embed)\nretriever = vectorstore.as_retriever()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T17:20:17.260801Z","iopub.execute_input":"2025-01-16T17:20:17.261106Z","iopub.status.idle":"2025-01-16T17:20:26.198333Z","shell.execute_reply.started":"2025-01-16T17:20:17.261077Z","shell.execute_reply":"2025-01-16T17:20:26.197028Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Grading Document Relevance\n\nThis section defines a data model for grading the relevance of retrieved documents to a user’s question. It initializes a language model (LLM) with function-calling capabilities to perform the grading. A prompt is constructed to instruct the LLM on how to assess the relevance of each document by assigning a binary score ('yes' or 'no'). A retrieval grader chain is then created and tested with a sample question to demonstrate its functionality.","metadata":{}},{"cell_type":"code","source":"from langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\nfrom pydantic import BaseModel, Field\n\n# Data model for grading document relevance\nclass GradeDocuments(BaseModel):\n    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n\n    binary_score: str = Field(description=\"Documents are relevant to the question, 'yes' or 'no'\")\n\n# LLM with function call for grading documents\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, api_key=my_api_key)\nstructured_llm_grader = llm.with_structured_output(GradeDocuments)\n\n# Prompt for grading document relevance\nsystem = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n    If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant. \\n\n    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\ngrade_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system),\n        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n    ]\n)\n\n# Chain for grading document relevance\nretrieval_grader = grade_prompt | structured_llm_grader\n\n# Test retrieval grader with a sample question\nquestion = \"agent memory\"\ndocs = retriever.invoke(question)\ndoc_txt = docs[1].page_content\n\nresult = retrieval_grader.invoke({\"question\": question, \"document\": doc_txt})\nprint(result)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T17:20:26.200603Z","iopub.execute_input":"2025-01-16T17:20:26.200973Z","iopub.status.idle":"2025-01-16T17:20:27.82562Z","shell.execute_reply.started":"2025-01-16T17:20:26.200937Z","shell.execute_reply":"2025-01-16T17:20:27.824449Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Setting Up the RAG Chain for Answer Generation\n\nThis block sets up the Retrieval-Augmented Generation (RAG) chain, which is responsible for generating answers based on the retrieved documents. It pulls a predefined RAG prompt from LangChain Hub, initializes an LLM for generating responses, and constructs a chain that processes the context and question to produce the final answer. A sample question is then run through the RAG chain to demonstrate its operation.","metadata":{}},{"cell_type":"code","source":"from langchain import hub\nfrom langchain_core.output_parsers import StrOutputParser\n\n# Pull the RAG prompt from the hub\nprompt = hub.pull(\"rlm/rag-prompt\")\n\n# LLM for generating answers\nllm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0, api_key=my_api_key)\n\n# Chain for generating answers using RAG\nrag_chain = prompt | llm | StrOutputParser()\n\n# Run the RAG chain with the sample question\ngeneration = rag_chain.invoke({\"context\": docs, \"question\": question})\nprint(generation)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T17:20:27.827374Z","iopub.execute_input":"2025-01-16T17:20:27.827808Z","iopub.status.idle":"2025-01-16T17:20:29.945026Z","shell.execute_reply.started":"2025-01-16T17:20:27.827766Z","shell.execute_reply":"2025-01-16T17:20:29.94398Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Show what the prompt looks like\nprompt = hub.pull(\"rlm/rag-prompt\").pretty_print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T17:20:29.945918Z","iopub.execute_input":"2025-01-16T17:20:29.946171Z","iopub.status.idle":"2025-01-16T17:20:30.155039Z","shell.execute_reply.started":"2025-01-16T17:20:29.946149Z","shell.execute_reply":"2025-01-16T17:20:30.153955Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Implementing the Question Rewriter\n\nThis section creates a mechanism to improve user questions for better search results. It defines an LLM that rephrases input questions to be more optimized for web searches by understanding the underlying semantic intent. A prompt is crafted to guide the LLM in transforming the questions, and a chain is established to process and rephrase the questions accordingly.","metadata":{}},{"cell_type":"code","source":"# Question Re-writer\n# LLM for re-writing questions\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, api_key=my_api_key)\n\n# Prompt for re-writing questions\nsystem = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n \n     for web search. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\nre_write_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system),\n        (\"human\", \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\"),\n    ]\n)\n\n# Chain for re-writing questions\nquestion_rewriter = re_write_prompt | llm | StrOutputParser()\nresult = question_rewriter.invoke({\"question\": question})\nprint(result)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T17:20:30.155954Z","iopub.execute_input":"2025-01-16T17:20:30.156341Z","iopub.status.idle":"2025-01-16T17:20:30.862394Z","shell.execute_reply.started":"2025-01-16T17:20:30.156312Z","shell.execute_reply":"2025-01-16T17:20:30.861419Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Initializing the Web Search Tool\n\nThis block sets up the DuckDuckGo search tool from LangChain Community tools. This tool is used to perform web searches when the initially retrieved documents are deemed irrelevant, ensuring that the system can fetch additional information from the web to answer the user's query effectively.","metadata":{}},{"cell_type":"code","source":"from langchain_community.tools import DuckDuckGoSearchRun\n\n# Initialize DuckDuckGo search tool\nweb_search_tool = DuckDuckGoSearchRun()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T17:20:30.863396Z","iopub.execute_input":"2025-01-16T17:20:30.86367Z","iopub.status.idle":"2025-01-16T17:20:30.915194Z","shell.execute_reply.started":"2025-01-16T17:20:30.863646Z","shell.execute_reply":"2025-01-16T17:20:30.914222Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Defining the Graph State and Workflow Nodes\n\nHere, the graph state and workflow nodes are defined to manage the flow of operations in the RAG system. A `GraphState` TypedDict outlines the necessary attributes such as the question, generated answer, web search flag, and retrieved documents. Functions for each node in the workflow—retrieving documents, generating answers, grading document relevance, transforming queries, and performing web searches—are implemented. Additionally, a decision function determines the next step based on the relevance of the retrieved documents.","metadata":{}},{"cell_type":"code","source":"from typing import List, Literal, Dict, Any\nfrom typing_extensions import TypedDict\nfrom langchain.schema import Document\n\nclass GraphState(TypedDict):\n    \"\"\"\n    Represents the state of our graph.\n\n    Attributes:\n        question: The current question.\n        generation: The LLM generation (answer).\n        web_search: Whether to perform a web search. (\"Yes\" or \"No\")\n        documents: List of retrieved documents.\n    \"\"\"\n    question: str\n    generation: str\n    web_search: Literal[\"Yes\", \"No\"]\n    documents: List[Document]\n\ndef retrieve_node(state: GraphState) -> GraphState:\n    \"\"\"\n    Retrieve documents relevant to the question.\n\n    Args:\n        state (GraphState): The current graph state.\n\n    Returns:\n        Dict[str, Any]: Updated state with retrieved documents.\n    \"\"\"\n    print(\"---RETRIEVE---\")\n    question = state[\"question\"]\n\n    # Retrieve documents using the retriever\n    documents = retriever.invoke(question)\n    return {\"documents\": documents, \"question\": question}\n\ndef generate_node(state: GraphState) -> GraphState:\n    \"\"\"\n    Generate an answer using the RAG chain.\n\n    Args:\n        state (GraphState): The current graph state.\n\n    Returns:\n        Dict[str, Any]: Updated state with the generated answer.\n    \"\"\"\n    print(\"---GENERATE---\")\n    question = state[\"question\"]\n    documents = state[\"documents\"]\n\n    # Generate answer using the RAG chain\n    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n\ndef grade_documents_node(state: GraphState) -> GraphState:\n    \"\"\"\n    Grade the relevance of retrieved documents to the question.\n\n    Args:\n        state (GraphState): The current graph state.\n\n    Returns:\n        Dict[str, Any]: Updated state with filtered relevant documents.\n    \"\"\"\n    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n    question = state[\"question\"]\n    documents = state[\"documents\"]\n\n    # Score each document for relevance\n    filtered_docs = []\n    web_search: Literal[\"Yes\", \"No\"] = \"No\"\n    for d in documents:\n        score = retrieval_grader.invoke({\"question\": question, \"document\": d.page_content})\n        grade = score.binary_score\n        if grade == \"yes\":\n            print(\"---GRADE: DOCUMENT RELEVANT---\")\n            filtered_docs.append(d)\n        else:\n            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n            web_search = \"Yes\"\n            continue\n    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search}\n\ndef transform_query_node(state: GraphState) -> GraphState:\n    \"\"\"\n    Transform the query to produce a better version for web search.\n\n    Args:\n        state (GraphState): The current graph state.\n\n    Returns:\n        Dict[str, Any]: Updated state with the re-phrased question.\n    \"\"\"\n    print(\"---TRANSFORM QUERY---\")\n    question = state[\"question\"]\n    documents = state[\"documents\"]\n\n    # Re-write the question for better search results\n    better_question = question_rewriter.invoke({\"question\": question})\n    return {\"documents\": documents, \"question\": better_question}\n\ndef web_search_node(state: GraphState) -> GraphState:\n    \"\"\"\n    Perform a web search based on the re-phrased question.\n\n    Args:\n        state (GraphState): The current graph state.\n\n    Returns:\n        Dict[str, Any]: Updated state with appended web search results.\n    \"\"\"\n    print(\"---WEB SEARCH---\")\n    question = state[\"question\"]\n    documents = state[\"documents\"]\n\n    # Perform web search using DuckDuckGo\n    search_results = web_search_tool.run(question)\n    web_results = Document(page_content=search_results)\n    documents.append(web_results)\n\n    return {\"documents\": documents, \"question\": question}\n\ndef decide_to_generate(state: GraphState) -> Literal[\"transform_query_node\", \"generate_node\"]:\n    \"\"\"\n    Decide whether to generate an answer or re-generate a question.\n\n    Args:\n        state (GraphState): The current graph state.\n\n    Returns:\n        Literal[\"transform_query_node\", \"generate_node\"]: Decision for the next node to call.\n    \"\"\"\n    print(\"---ASSESS GRADED DOCUMENTS---\")\n    web_search = state[\"web_search\"]\n\n    if web_search == \"Yes\":\n        # If documents are not relevant, re-generate the question\n        print(\"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\")\n        return \"transform_query_node\"\n    else:\n        # If documents are relevant, generate the answer\n        print(\"---DECISION: GENERATE---\")\n        return \"generate_node\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T17:20:30.917701Z","iopub.execute_input":"2025-01-16T17:20:30.918017Z","iopub.status.idle":"2025-01-16T17:20:30.93831Z","shell.execute_reply.started":"2025-01-16T17:20:30.917988Z","shell.execute_reply":"2025-01-16T17:20:30.937325Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Compiling and Visualizing the Workflow Graph\n\nThis block constructs the workflow graph by adding the defined nodes and establishing the connections (edges) between them. It uses LangGraph’s `StateGraph` to manage the workflow, specifying the start and end points. Additionally, it attempts to visualize the graph using Mermaid syntax, which provides a graphical representation of the workflow. Visualization is optional and may require additional dependencies.","metadata":{}},{"cell_type":"code","source":"from langgraph.graph import END, StateGraph, START\nfrom IPython.display import Image, display\n\n# Initialize the workflow graph\nworkflow = StateGraph(GraphState)\n\n# Define the nodes in the workflow\nworkflow.add_node(\"retrieve_node\", retrieve_node)                # Retrieve documents\nworkflow.add_node(\"grade_documents_node\", grade_documents_node)  # Grade documents\nworkflow.add_node(\"generate_node\", generate_node)                # Generate answer\nworkflow.add_node(\"transform_query_node\", transform_query_node)  # Transform query\nworkflow.add_node(\"web_search_node\", web_search_node)            # Web search\n\n# Build the graph edges\nworkflow.add_edge(START, \"retrieve_node\")\nworkflow.add_edge(\"retrieve_node\", \"grade_documents_node\")\nworkflow.add_conditional_edges(\"grade_documents_node\", decide_to_generate, [\"transform_query_node\", \"generate_node\"])\nworkflow.add_edge(\"transform_query_node\", \"web_search_node\")\nworkflow.add_edge(\"web_search_node\", \"generate_node\")\nworkflow.add_edge(\"generate_node\", END)\n\n# Compile the workflow\napp = workflow.compile()\n\n# Visualize the graph (optional, requires additional dependencies)\ntry:\n    display(Image(app.get_graph(xray=True).draw_mermaid_png()))\nexcept Exception:\n    pass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T17:20:30.939629Z","iopub.execute_input":"2025-01-16T17:20:30.939963Z","iopub.status.idle":"2025-01-16T17:20:32.059428Z","shell.execute_reply.started":"2025-01-16T17:20:30.939935Z","shell.execute_reply":"2025-01-16T17:20:32.058322Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Executing the Workflow with Examples\n\nThis section demonstrates how to execute the compiled workflow by providing an input question. It includes a helper function to convert non-serializable objects into JSON-serializable formats for easier output visualization. The workflow is run with a sample question, and the outputs from each node are printed in a structured JSON format.","metadata":{}},{"cell_type":"code","source":"import json\nimport warnings\n\n# Suppress LangSmith API key warning (if not using LangSmith)\nwarnings.filterwarnings(\"ignore\", category=UserWarning, message=\"API key must be provided when using hosted LangSmith API\")\n\n# Helper function to convert non-serializable objects to dictionaries\ndef convert_to_serializable(obj):\n    if hasattr(obj, \"dict\"):              # Check if the object has a .dict() method\n        return obj.dict()\n    elif isinstance(obj, (list, tuple)):  # Handle lists and tuples\n        return [convert_to_serializable(item) for item in obj]\n    elif isinstance(obj, dict):           # Handle dictionaries\n        return {key: convert_to_serializable(value) for key, value in obj.items()}\n    else:                                 # Return the object as-is if it's already serializable\n        return obj","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T17:20:32.060541Z","iopub.execute_input":"2025-01-16T17:20:32.060987Z","iopub.status.idle":"2025-01-16T17:20:32.067411Z","shell.execute_reply.started":"2025-01-16T17:20:32.060944Z","shell.execute_reply":"2025-01-16T17:20:32.066292Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example 1: Types of Agent Memory\ninputs = {\"question\": \"What are the types of agent memory?\"}\nfor output in app.stream(inputs):\n    for key, value in output.items():\n        print(f\"Output from node '{key}':\")\n        print(\"-\"*80)\n        # Convert non-serializable objects to dictionaries\n        serializable_value = convert_to_serializable(value)\n        # Print the serialized value as a JSON string\n        print(json.dumps(serializable_value, indent=2))\n    print(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T17:20:32.068428Z","iopub.execute_input":"2025-01-16T17:20:32.068725Z","iopub.status.idle":"2025-01-16T17:20:37.412024Z","shell.execute_reply.started":"2025-01-16T17:20:32.0687Z","shell.execute_reply":"2025-01-16T17:20:37.410981Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example 2: Prompt Engineering Techniques\ninputs = {\"question\": \"What are some prompt engineering techniques?\"}\nfor output in app.stream(inputs):\n    for key, value in output.items():\n        print(f\"Output from node '{key}':\")\n        print(\"-\"*80)\n        # Convert non-serializable objects to dictionaries\n        serializable_value = convert_to_serializable(value)\n        # Print the serialized value as a JSON string\n        print(json.dumps(serializable_value, indent=2))\n    print(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T17:20:37.413139Z","iopub.execute_input":"2025-01-16T17:20:37.413557Z","iopub.status.idle":"2025-01-16T17:20:41.072232Z","shell.execute_reply.started":"2025-01-16T17:20:37.413517Z","shell.execute_reply":"2025-01-16T17:20:41.070844Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example 3: Adversarial Attacks on LLMs\ninputs = {\"question\": \"What are adversarial attacks on large language models?\"}\nfor output in app.stream(inputs):\n    for key, value in output.items():\n        print(f\"Output from node '{key}':\")\n        print(\"-\"*80)\n        # Convert non-serializable objects to dictionaries\n        serializable_value = convert_to_serializable(value)\n        # Print the serialized value as a JSON string\n        print(json.dumps(serializable_value, indent=2))\n    print(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T17:20:41.073696Z","iopub.execute_input":"2025-01-16T17:20:41.074117Z","iopub.status.idle":"2025-01-16T17:20:45.069778Z","shell.execute_reply.started":"2025-01-16T17:20:41.074069Z","shell.execute_reply":"2025-01-16T17:20:45.06843Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Conclusion\n\nThe implementation presented showcases a foundational approach to integrating **Corrective RAG (CRAG)** principles within a retrieval-augmented generation framework using LangGraph and LangChain. By introducing a grading mechanism to assess the relevance of retrieved documents, the system ensures that only pertinent information contributes to the generation of responses, thereby enhancing the overall accuracy and reliability of the output.\n\nWhile this example omits the knowledge refinement phase for simplicity, it lays the groundwork for more sophisticated enhancements, such as partitioning documents into knowledge strips and implementing deeper self-reflection capabilities. Additionally, the incorporation of web search supplementation through DuckDuckGo demonstrates the system's ability to dynamically seek additional information when initial retrievals fall short, ensuring comprehensive and accurate answers to user queries.\n\nMoving forward, further refinements could include integrating advanced knowledge refinement techniques, expanding the range of data sources, and enhancing the grading criteria to capture more nuanced aspects of relevance. These enhancements would elevate the system's performance, making it a robust tool for a wide array of applications that demand precise and contextually appropriate language generation.\n","metadata":{}}]}