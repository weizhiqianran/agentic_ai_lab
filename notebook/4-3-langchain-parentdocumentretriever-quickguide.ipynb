{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10518203,"sourceType":"datasetVersion","datasetId":6510483}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **LangChain `ParentDocumentRetriever` Quick Reference**\n\n## Introduction\n\nThe `ParentDocumentRetriever` operates by first splitting documents into smaller chunks, which are then stored and indexed. During retrieval, it retrieves these small chunks based on a query and subsequently looks up their parent document IDs to return the larger original documents or predefined larger chunks. This approach strikes a balance between having sufficiently small segments for accurate embeddings and maintaining enough context for meaningful retrieval.\n\n### Key Features\n\n- **Chunking Strategy**: The retriever allows for both small chunk retrieval and parent document lookup, ensuring that the embeddings accurately reflect the content's meaning while preserving context.\n- **Dynamic Retrieval**: It can dynamically fetch parent documents based on the retrieved chunks, enhancing the relevance of the results returned to users.\n- **Metadata Handling**: It supports metadata fields, allowing users to retain relevant information associated with child documents during retrieval.\n\n### Use Cases\n\n1. **Contextual Retrieval**: Ideal for applications where understanding the context surrounding a specific piece of information is crucial, such as in question-answering systems.\n2. **Efficient Document Management**: Useful in scenarios where large documents need to be managed and accessed quickly without losing important contextual information.\n\nThe `ParentDocumentRetriever` thus serves as an effective tool within LangChain, enhancing how users can retrieve and interact with large sets of textual data while maintaining contextual integrity.","metadata":{}},{"cell_type":"markdown","source":"---\n\n## Preparation\n\n### Installing Required Libraries\nThis section installs the necessary Python libraries for working with LangChain, OpenAI embeddings, and Chroma vector store. These libraries include:\n- `langchain-openai`: Provides integration with OpenAI's embedding models.\n- `langchain_community`: Contains community-contributed modules and tools for LangChain.\n- `langchain_experimental`: Includes experimental features and utilities for LangChain.\n- `langchain-chroma`: Enables integration with the Chroma vector database.\n- `chromadb`: The core library for the Chroma vector database.","metadata":{}},{"cell_type":"code","source":"!pip install -qU langchain-openai\n!pip install -qU langchain_community\n!pip install -qU langchain_experimental\n!pip install -qU langchain-chroma>=0.1.2\n!pip install -qU chromadb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T18:24:07.949714Z","iopub.execute_input":"2025-01-19T18:24:07.950055Z","iopub.status.idle":"2025-01-19T18:25:09.448237Z","shell.execute_reply.started":"2025-01-19T18:24:07.949979Z","shell.execute_reply":"2025-01-19T18:25:09.447003Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Initializing OpenAI Embeddings\nThis section demonstrates how to securely fetch an OpenAI API key using Kaggle's `UserSecretsClient` and initialize the OpenAI embedding model. The `OpenAIEmbeddings` class is used to create an embedding model instance, which will be used to convert text into numerical embeddings.\n\nKey steps:\n1. **Fetch API Key**: The OpenAI API key is securely retrieved using Kaggle's `UserSecretsClient`.\n2. **Initialize Embeddings**: The `OpenAIEmbeddings` class is initialized with the `text-embedding-3-small` model and the fetched API key.\n\nThis setup ensures that the embedding model is ready for use in downstream tasks, such as caching embeddings or creating vector stores.","metadata":{}},{"cell_type":"code","source":"from langchain_openai import OpenAIEmbeddings\nfrom kaggle_secrets import UserSecretsClient\n\n# Fetch API key securely\nuser_secrets = UserSecretsClient()\nmy_api_key = user_secrets.get_secret(\"api-key-openai\")\n\n# Initialize OpenAI embeddings\nembed = OpenAIEmbeddings(model=\"text-embedding-3-small\", api_key=my_api_key)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T18:25:09.449408Z","iopub.execute_input":"2025-01-19T18:25:09.449762Z","iopub.status.idle":"2025-01-19T18:25:11.996193Z","shell.execute_reply.started":"2025-01-19T18:25:09.449731Z","shell.execute_reply":"2025-01-19T18:25:11.995405Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n## 1. Document Retrieval & Management\n\n### **Adding and Retrieving Documents**\n\nThis example demonstrates how to add documents to the `ParentDocumentRetriever` and subsequently retrieve them based on a query. It showcases the fundamental workflow of indexing and retrieving documents.","metadata":{}},{"cell_type":"code","source":"from langchain_chroma import Chroma\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain.storage import InMemoryStore\nfrom langchain.retrievers.parent_document_retriever import ParentDocumentRetriever\nfrom langchain.schema import Document\n\n# Initialize text splitters\nparent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, add_start_index=True)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400, add_start_index=True)\n\n# Initialize vectorstore and docstore with a unique collection name\nvectorstore = Chroma(embedding_function=embed, collection_name=\"doc_retrieval_add_retrieve\")\nstore = InMemoryStore()\n\n# Initialize the ParentDocumentRetriever\nretriever = ParentDocumentRetriever(\n    vectorstore=vectorstore,\n    docstore=store,\n    child_splitter=child_splitter,\n    parent_splitter=parent_splitter,\n)\n\n# Sample documents (create Document objects)\ndocuments = [\n    Document(page_content=\"Document 1 content goes here.\", metadata={\"source\": \"doc1\"}),\n    Document(page_content=\"Document 2 content goes here.\", metadata={\"source\": \"doc2\"}),\n]\n\n# Add documents to the retriever\nretriever.add_documents(documents)\n\n# Retrieve documents relevant to a query\nquery = \"content goes here\"\nrelevant_docs = retriever.invoke(query)\n\nprint(\"Retrieved Documents:\")\nfor doc in relevant_docs:\n    print(f\"Source: {doc.metadata['source']}, Content: {doc.page_content}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T18:28:44.976372Z","iopub.execute_input":"2025-01-19T18:28:44.976737Z","iopub.status.idle":"2025-01-19T18:28:50.033888Z","shell.execute_reply.started":"2025-01-19T18:28:44.976706Z","shell.execute_reply":"2025-01-19T18:28:50.033142Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Filtering Retrieved Documents by Metadata**\n\nThis example illustrates how to retrieve documents while filtering them based on specific metadata criteria. This is useful when you want to narrow down search results to documents from particular sources or with certain attributes.","metadata":{}},{"cell_type":"code","source":"from langchain_chroma import Chroma\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain.storage import InMemoryStore\nfrom langchain.retrievers.parent_document_retriever import ParentDocumentRetriever\nfrom langchain.schema import Document\n\n# Initialize text splitters\nparent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, add_start_index=True)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400, add_start_index=True)\n\n# Initialize vectorstore and docstore with a unique collection name\nvectorstore = Chroma(embedding_function=embed, collection_name=\"doc_retrieval_filter_metadata\")\nstore = InMemoryStore()\n\n# Initialize the ParentDocumentRetriever with child metadata fields\nretriever = ParentDocumentRetriever(\n    vectorstore=vectorstore,\n    docstore=store,\n    child_splitter=child_splitter,\n    parent_splitter=parent_splitter,\n    child_metadata_fields=[\"source\"],\n)\n\n# Sample documents (use Document objects)\ndocuments = [\n    Document(page_content=\"Document 1 content goes here.\", metadata={\"source\": \"internal\"}),\n    Document(page_content=\"Document 2 content goes here.\", metadata={\"source\": \"external\"}),\n]\n\n# Add documents to the retriever\nretriever.add_documents(documents)\n\n# Retrieve documents relevant to a query with metadata filter\nquery = \"content goes here\"\nmetadata_filter = {\"source\": \"internal\"}\nrelevant_docs = retriever.invoke(query, metadata=metadata_filter)\n\nprint(\"Retrieved Documents with 'internal' source:\")\nfor doc in relevant_docs:\n    print(f\"Source: {doc.metadata['source']}, Content: {doc.page_content}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T18:30:09.760804Z","iopub.execute_input":"2025-01-19T18:30:09.761146Z","iopub.status.idle":"2025-01-19T18:30:11.181997Z","shell.execute_reply.started":"2025-01-19T18:30:09.761117Z","shell.execute_reply":"2025-01-19T18:30:11.180922Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Updating Documents in the Retriever**\n    \nWhile the `ParentDocumentRetriever` does not provide a direct method to update documents, you can manage updates by removing existing documents and adding the updated versions. This example demonstrates how to perform such an update.","metadata":{}},{"cell_type":"code","source":"from langchain_chroma import Chroma\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain.storage import InMemoryStore\nfrom langchain.retrievers.parent_document_retriever import ParentDocumentRetriever\nfrom langchain.schema import Document  # Import the Document class\n\n# Initialize text splitters\nparent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, add_start_index=True)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400, add_start_index=True)\n\n# Initialize vectorstore and docstore with a unique collection name\nvectorstore = Chroma(embedding_function=embed, collection_name=\"doc_retrieval_update_docs\")\nstore = InMemoryStore()\n\n# Initialize the ParentDocumentRetriever\nretriever = ParentDocumentRetriever(\n    vectorstore=vectorstore,\n    docstore=store,\n    child_splitter=child_splitter,\n    parent_splitter=parent_splitter,\n)\n\n# Sample documents (use Document objects)\ndocuments = [\n    Document(page_content=\"Original content of Document 1.\", metadata={\"doc_id\": \"doc1\"}),\n    Document(page_content=\"Original content of Document 2.\", metadata={\"doc_id\": \"doc2\"}),\n]\n\n# Add documents to the retriever\nretriever.add_documents(documents)\n\n# Updated document (use Document object)\nupdated_document = Document(page_content=\"Updated content of Document 1.\", metadata={\"doc_id\": \"doc1\"})\n\n# Remove the old document (assuming a remove method exists)\n# Since there's no direct remove method, we'll reinitialize the retriever without the old document\n# In practice, implement a remove method or manage updates appropriately\n\n# For illustration, reinitialize the retriever with the same collection name\nretriever = ParentDocumentRetriever(\n    vectorstore=Chroma(embedding_function=embed, collection_name=\"doc_retrieval_update_docs\"),\n    docstore=InMemoryStore(),\n    child_splitter=child_splitter,\n    parent_splitter=parent_splitter,\n)\n\n# Add updated document\nretriever.add_documents([updated_document])\n\n# Retrieve updated document\nquery = \"updated content\"\nrelevant_docs = retriever.invoke(query)\n\nprint(\"Retrieved Updated Documents:\")\nfor doc in relevant_docs:\n    print(f\"Doc ID: {doc.metadata['doc_id']}, Content: {doc.page_content}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T18:33:21.323711Z","iopub.execute_input":"2025-01-19T18:33:21.324088Z","iopub.status.idle":"2025-01-19T18:33:23.094295Z","shell.execute_reply.started":"2025-01-19T18:33:21.324059Z","shell.execute_reply":"2025-01-19T18:33:23.093431Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n    \n## 2. Batch Processing\n    \n### **Batch Adding Multiple Documents**\n    \nThis example demonstrates how to add multiple documents to the `ParentDocumentRetriever` in a single batch operation. Batch processing can improve efficiency when dealing with large volumes of data.","metadata":{}},{"cell_type":"code","source":"from langchain_chroma import Chroma\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain.storage import InMemoryStore\nfrom langchain.retrievers.parent_document_retriever import ParentDocumentRetriever\nfrom langchain.schema import Document  # Import the Document class\n\n# Initialize text splitters\nparent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, add_start_index=True)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400, add_start_index=True)\n\n# Initialize vectorstore and docstore with a unique collection name\nvectorstore = Chroma(embedding_function=embed, collection_name=\"batch_processing_add\")\nstore = InMemoryStore()\n\n# Initialize the ParentDocumentRetriever\nretriever = ParentDocumentRetriever(\n    vectorstore=vectorstore,\n    docstore=store,\n    child_splitter=child_splitter,\n    parent_splitter=parent_splitter,\n)\n\n# Batch of documents (use Document objects)\ndocuments = [\n    Document(page_content=\"Content of Document 1.\", metadata={\"source\": \"doc1\"}),\n    Document(page_content=\"Content of Document 2.\", metadata={\"source\": \"doc2\"}),\n    Document(page_content=\"Content of Document 3.\", metadata={\"source\": \"doc3\"}),\n]\n\n# Batch add documents\nretriever.add_documents(documents)\n\n# Verify addition by retrieving a document\nquery = \"Content of Document 2.\"\nrelevant_docs = retriever.invoke(query)\n\nprint(\"Retrieved Document:\")\nfor doc in relevant_docs:\n    print(f\"Source: {doc.metadata['source']}, Content: {doc.page_content}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T18:33:43.588409Z","iopub.execute_input":"2025-01-19T18:33:43.588732Z","iopub.status.idle":"2025-01-19T18:33:44.673645Z","shell.execute_reply.started":"2025-01-19T18:33:43.588707Z","shell.execute_reply":"2025-01-19T18:33:44.67274Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Batch Retrieving Documents for Multiple Queries**\n    \nThis example showcases how to perform batch retrievals for multiple queries simultaneously. Batch retrieval can significantly speed up the process when handling multiple search requests.","metadata":{}},{"cell_type":"code","source":"from langchain_chroma import Chroma\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain.storage import InMemoryStore\nfrom langchain.retrievers.parent_document_retriever import ParentDocumentRetriever\nfrom langchain.schema import Document  # Import the Document class\n\n# Initialize text splitters\nparent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, add_start_index=True)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400, add_start_index=True)\n\n# Initialize vectorstore and docstore with a unique collection name\nvectorstore = Chroma(embedding_function=embed, collection_name=\"batch_processing_retrieve_queries\")\nstore = InMemoryStore()\n\n# Initialize the ParentDocumentRetriever\nretriever = ParentDocumentRetriever(\n    vectorstore=vectorstore,\n    docstore=store,\n    child_splitter=child_splitter,\n    parent_splitter=parent_splitter,\n)\n\n# Add sample documents (use Document objects)\ndocuments = [\n    Document(page_content=\"Python is a versatile programming language.\", metadata={\"doc_id\": \"doc1\"}),\n    Document(page_content=\"Java is widely used in enterprise applications.\", metadata={\"doc_id\": \"doc2\"}),\n    Document(page_content=\"JavaScript powers the web.\", metadata={\"doc_id\": \"doc3\"}),\n]\nretriever.add_documents(documents)\n\n# List of queries\nqueries = [\n    \"programming language\",\n    \"enterprise applications\",\n    \"web development\",\n]\n\n# Batch retrieve documents for all queries\nresults = retriever.batch(queries)\n\nfor i, docs in enumerate(results):\n    print(f\"Results for Query {i+1}:\")\n    for doc in docs:\n        print(f\"Doc ID: {doc.metadata['doc_id']}, Content: {doc.page_content}\")\n    print(\"---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T18:33:57.420564Z","iopub.execute_input":"2025-01-19T18:33:57.42088Z","iopub.status.idle":"2025-01-19T18:33:58.603824Z","shell.execute_reply.started":"2025-01-19T18:33:57.420857Z","shell.execute_reply":"2025-01-19T18:33:58.602827Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Batch Processing with Configuration**\n    \nThis example demonstrates how to use different configurations for each batch invocation. This flexibility allows for customized retrieval behaviors based on specific requirements.","metadata":{}},{"cell_type":"code","source":"from langchain_chroma import Chroma\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain.storage import InMemoryStore\nfrom langchain.retrievers.parent_document_retriever import ParentDocumentRetriever\nfrom langchain.schema import Document  # Import the Document class\n\n# Initialize text splitters\nparent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, add_start_index=True)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400, add_start_index=True)\n\n# Initialize vectorstore and docstore with a unique collection name\nvectorstore = Chroma(embedding_function=embed, collection_name=\"batch_processing_no_runnableconfig\")\nstore = InMemoryStore()\n\n# Initialize the ParentDocumentRetriever\nretriever = ParentDocumentRetriever(\n    vectorstore=vectorstore,\n    docstore=store,\n    child_splitter=child_splitter,\n    parent_splitter=parent_splitter,\n)\n\n# Add sample documents (use Document objects)\ndocuments = [\n    Document(page_content=\"Machine learning enables computers to learn from data.\", metadata={\"doc_id\": \"doc1\"}),\n    Document(page_content=\"Deep learning is a subset of machine learning.\", metadata={\"doc_id\": \"doc2\"}),\n    Document(page_content=\"Artificial intelligence encompasses machine learning and deep learning.\", metadata={\"doc_id\": \"doc3\"}),\n]\nretriever.add_documents(documents)\n\n# Define batch inputs\ninputs = [\"machine learning\", \"deep learning\"]\n\n# Batch retrieve documents\nresults = retriever.batch(inputs)\n\nfor i, docs in enumerate(results):\n    print(f\"Results for Query '{inputs[i]}':\")\n    for doc in docs:\n        print(f\"Doc ID: {doc.metadata['doc_id']}, Content: {doc.page_content}\")\n    print(\"---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T18:36:16.05039Z","iopub.execute_input":"2025-01-19T18:36:16.050707Z","iopub.status.idle":"2025-01-19T18:36:17.314054Z","shell.execute_reply.started":"2025-01-19T18:36:16.050682Z","shell.execute_reply":"2025-01-19T18:36:17.31312Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n    \n## 3. Streaming\n    \n### **Streaming Retrieval of Documents**\n    \nThis example demonstrates how to stream the retrieval of documents based on a query. Streaming allows processing results incrementally as they become available, which can be beneficial for real-time applications.","metadata":{}},{"cell_type":"code","source":"from langchain_chroma import Chroma\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain.storage import InMemoryStore\nfrom langchain.retrievers.parent_document_retriever import ParentDocumentRetriever\nfrom langchain.schema import Document\n\n# Initialize text splitters\nparent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, add_start_index=True)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400, add_start_index=True)\n\n# Initialize vectorstore and docstore with a unique collection name\nvectorstore = Chroma(embedding_function=embed, collection_name=\"streaming_retrieval\")\nstore = InMemoryStore()\n\n# Initialize the ParentDocumentRetriever\nretriever = ParentDocumentRetriever(\n    vectorstore=vectorstore,\n    docstore=store,\n    child_splitter=child_splitter,\n    parent_splitter=parent_splitter,\n)\n\n# Add sample documents (use Document objects)\ndocuments = [\n    Document(page_content=\"Stream processing allows handling data in real-time.\", metadata={\"doc_id\": \"doc1\"}),\n    Document(page_content=\"Batch processing handles large volumes of data at once.\", metadata={\"doc_id\": \"doc2\"}),\n    Document(page_content=\"Real-time analytics requires efficient streaming.\", metadata={\"doc_id\": \"doc3\"}),\n]\nretriever.add_documents(documents)\n\n# Stream retrieval of documents\nquery = \"real-time data processing\"\n\n# Process each chunk returned by the stream method\nfor chunk in retriever.stream(query):\n    for doc in chunk:  # Iterate through the documents in the chunk\n        print(f\"Retrieved Document: {doc.metadata['doc_id']}, Content: {doc.page_content}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T18:37:50.293876Z","iopub.execute_input":"2025-01-19T18:37:50.294284Z","iopub.status.idle":"2025-01-19T18:37:51.31613Z","shell.execute_reply.started":"2025-01-19T18:37:50.29425Z","shell.execute_reply":"2025-01-19T18:37:51.315143Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Streaming Events During Retrieval**\n    \nThis example showcases how to generate and handle a stream of events related to the retrieval process. Event streaming provides insights into the internal operations and progress of the retriever.","metadata":{}},{"cell_type":"code","source":"from langchain_chroma import Chroma\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain.storage import InMemoryStore\nfrom langchain.retrievers.parent_document_retriever import ParentDocumentRetriever\nfrom langchain.schema import Document\n\n# Initialize text splitters\nparent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, add_start_index=True)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400, add_start_index=True)\n\n# Initialize vectorstore and docstore with a unique collection name\nvectorstore = Chroma(embedding_function=embed, collection_name=\"streaming_events_retrieval\")\nstore = InMemoryStore()\n\n# Initialize the ParentDocumentRetriever\nretriever = ParentDocumentRetriever(\n    vectorstore=vectorstore,\n    docstore=store,\n    child_splitter=child_splitter,\n    parent_splitter=parent_splitter,\n)\n\n# Add sample documents (use Document objects)\ndocuments = [\n    Document(page_content=\"Event-driven architectures respond to events.\", metadata={\"doc_id\": \"doc1\"}),\n    Document(page_content=\"Streaming data enables real-time processing.\", metadata={\"doc_id\": \"doc2\"}),\n    Document(page_content=\"Asynchronous events improve system responsiveness.\", metadata={\"doc_id\": \"doc3\"}),\n]\nretriever.add_documents(documents)\n\n# Stream events during retrieval\nquery = \"real-time processing\"\n\n# Iterate over chunks returned by the stream method\nfor chunk in retriever.stream(query):  # Each chunk is a list of Document objects\n    for doc in chunk:  # Iterate through individual documents in the chunk\n        print(f\"Event: Retrieved Document ID {doc.metadata['doc_id']}, Content: {doc.page_content}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T18:38:58.144527Z","iopub.execute_input":"2025-01-19T18:38:58.144867Z","iopub.status.idle":"2025-01-19T18:38:59.151102Z","shell.execute_reply.started":"2025-01-19T18:38:58.144843Z","shell.execute_reply":"2025-01-19T18:38:59.149743Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Combining Streaming with Listeners**\n    \nWhile the `ParentDocumentRetriever` primarily supports synchronous streaming, you can enhance the retrieval process by integrating listeners that react to streamed data.","metadata":{}},{"cell_type":"code","source":"from langchain_chroma import Chroma\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain.storage import InMemoryStore\nfrom langchain.retrievers.parent_document_retriever import ParentDocumentRetriever\nfrom langchain.schema import Document\n\n# Define listener functions\ndef on_start(run_obj):\n    print(\"Retrieval process started.\")\n\ndef on_end(run_obj):\n    print(\"Retrieval process completed.\")\n\ndef on_error(run_obj):\n    print(\"An error occurred during retrieval.\")\n\n# Initialize text splitters\nparent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, add_start_index=True)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400, add_start_index=True)\n\n# Initialize vectorstore and docstore with a unique collection name\nvectorstore = Chroma(embedding_function=embed, collection_name=\"streaming_with_listeners\")\nstore = InMemoryStore()\n\n# Initialize the ParentDocumentRetriever with listeners\nretriever = ParentDocumentRetriever(\n    vectorstore=vectorstore,\n    docstore=store,\n    child_splitter=child_splitter,\n    parent_splitter=parent_splitter,\n).with_listeners(\n    on_start=on_start,\n    on_end=on_end,\n    on_error=on_error\n)\n\n# Add sample documents (use Document objects)\ndocuments = [\n    Document(page_content=\"Listener functions can react to retrieval events.\", metadata={\"doc_id\": \"doc1\"}),\n    Document(page_content=\"Event listeners enhance the functionality of retrievers.\", metadata={\"doc_id\": \"doc2\"}),\n    Document(page_content=\"Proper error handling ensures system robustness.\", metadata={\"doc_id\": \"doc3\"}),\n]\nretriever.add_documents(documents)\n\n# Stream retrieval with listeners\nquery = \"retrieval events\"\n\nfor chunk in retriever.stream(query):  # Process each chunk (list of Document objects)\n    for doc in chunk:  # Process each Document object within the chunk\n        print(f\"Retrieved Document: {doc.metadata['doc_id']}, Content: {doc.page_content}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T18:40:09.411012Z","iopub.execute_input":"2025-01-19T18:40:09.411407Z","iopub.status.idle":"2025-01-19T18:40:10.354791Z","shell.execute_reply.started":"2025-01-19T18:40:09.41138Z","shell.execute_reply":"2025-01-19T18:40:10.354011Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n    \n## 4. Event Handling\n    \n### **Binding Synchronous Lifecycle Listeners**\n    \nThis example demonstrates how to bind synchronous lifecycle listeners (`on_start` and `on_end`) to the `ParentDocumentRetriever`. These listeners execute custom functions at different stages of the retrieval process.","metadata":{}},{"cell_type":"code","source":"from langchain_chroma import Chroma\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain.storage import InMemoryStore\nfrom langchain.retrievers.parent_document_retriever import ParentDocumentRetriever\nfrom langchain.schema import Document\n\n# Define listener functions\ndef on_start(run_obj):\n    print(\"Retrieval started.\")\n\ndef on_end(run_obj):\n    print(\"Retrieval ended.\")\n\ndef on_error(run_obj):\n    print(\"An error occurred during retrieval.\")\n\n# Initialize text splitters\nparent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, add_start_index=True)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400, add_start_index=True)\n\n# Initialize vectorstore and docstore with a unique collection name\nvectorstore = Chroma(embedding_function=embed, collection_name=\"event_handling_bind_listeners\")\nstore = InMemoryStore()\n\n# Initialize the ParentDocumentRetriever with listeners\nretriever = ParentDocumentRetriever(\n    vectorstore=vectorstore,\n    docstore=store,\n    child_splitter=child_splitter,\n    parent_splitter=parent_splitter,\n).with_listeners(\n    on_start=on_start,\n    on_end=on_end,\n    on_error=on_error\n)\n\n# Add sample documents (use Document objects)\ndocuments = [\n    Document(page_content=\"Event listeners allow custom actions during retrieval.\", metadata={\"doc_id\": \"doc1\"}),\n    Document(page_content=\"They can be used to log retrieval activities.\", metadata={\"doc_id\": \"doc2\"}),\n]\nretriever.add_documents(documents)\n\n# Invoke retrieval to trigger listeners\nquery = \"event listeners\"\n\nretrieved_docs = retriever.invoke(query)\n\nfor doc in retrieved_docs:\n    print(f\"Retrieved Document: {doc.metadata['doc_id']}, Content: {doc.page_content}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T18:42:14.584796Z","iopub.execute_input":"2025-01-19T18:42:14.585186Z","iopub.status.idle":"2025-01-19T18:42:15.574628Z","shell.execute_reply.started":"2025-01-19T18:42:14.585156Z","shell.execute_reply":"2025-01-19T18:42:15.573751Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Dispatching Custom Events**\n    \nAlthough the `ParentDocumentRetriever` does not directly expose methods for dispatching custom events, you can integrate custom event dispatching within your application logic. This example illustrates how to simulate custom event handling during the retrieval process.","metadata":{}},{"cell_type":"code","source":"from langchain_chroma import Chroma\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain.storage import InMemoryStore\nfrom langchain.retrievers.parent_document_retriever import ParentDocumentRetriever\nfrom langchain.schema import Document\n\n# Define a custom event dispatcher\ndef dispatch_custom_event(event_name, data):\n    print(f\"Custom Event: {event_name}, Data: {data}\")\n\n# Define listener functions with custom event dispatching\ndef on_start(run_obj):\n    dispatch_custom_event(\"retrieval_started\", {\"query\": run_obj.input})\n\ndef on_end(run_obj):\n    dispatch_custom_event(\"retrieval_completed\", {\"num_documents\": len(run_obj.output)})\n\ndef on_error(run_obj):\n    dispatch_custom_event(\"retrieval_error\", {\"error\": str(run_obj.error)})\n\n# Initialize text splitters\nparent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, add_start_index=True)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400, add_start_index=True)\n\n# Initialize vectorstore and docstore with a unique collection name\nvectorstore = Chroma(embedding_function=embed, collection_name=\"event_handling_custom_events\")\nstore = InMemoryStore()\n\n# Initialize the ParentDocumentRetriever with custom event listeners\nretriever = ParentDocumentRetriever(\n    vectorstore=vectorstore,\n    docstore=store,\n    child_splitter=child_splitter,\n    parent_splitter=parent_splitter,\n).with_listeners(\n    on_start=on_start,\n    on_end=on_end,\n    on_error=on_error\n)\n\n# Add sample documents (use Document objects)\ndocuments = [\n    Document(page_content=\"Custom events provide flexibility in handling retrieval processes.\", metadata={\"doc_id\": \"doc1\"}),\n    Document(page_content=\"They can be tailored to specific application needs.\", metadata={\"doc_id\": \"doc2\"}),\n]\nretriever.add_documents(documents)\n\n# Invoke retrieval to trigger custom events\nquery = \"custom events\"\n\ntry:\n    retrieved_docs = retriever.invoke(query)\n    for doc in retrieved_docs:\n        print(f\"Retrieved Document: {doc.metadata['doc_id']}, Content: {doc.page_content}\")\nexcept Exception as e:\n    print(f\"Exception during retrieval: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T18:42:29.074034Z","iopub.execute_input":"2025-01-19T18:42:29.074422Z","iopub.status.idle":"2025-01-19T18:42:30.181346Z","shell.execute_reply.started":"2025-01-19T18:42:29.074394Z","shell.execute_reply":"2025-01-19T18:42:30.180361Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Using Custom Callbacks with Listeners**\n    \nThis example shows how to integrate custom callback functions with the retriever's lifecycle listeners to perform additional operations, such as logging or data transformation, during the retrieval process.","metadata":{}},{"cell_type":"code","source":"from langchain_chroma import Chroma\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain.storage import InMemoryStore\nfrom langchain.retrievers.parent_document_retriever import ParentDocumentRetriever\nfrom langchain.schema import Document\n\n# Define custom callback functions\ndef log_start(run_obj):\n    print(f\"[LOG] Retrieval started for query: '{run_obj.input}'\")\n\ndef log_end(run_obj):\n    print(f\"[LOG] Retrieval ended. Number of documents retrieved: {len(run_obj.output)}\")\n\ndef log_error(run_obj):\n    print(f\"[LOG] Retrieval failed with error: {run_obj.error}\")\n\n# Initialize text splitters\nparent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, add_start_index=True)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400, add_start_index=True)\n\n# Initialize vectorstore and docstore with a unique collection name\nvectorstore = Chroma(embedding_function=embed, collection_name=\"event_handling_custom_callbacks\")\nstore = InMemoryStore()\n\n# Initialize the ParentDocumentRetriever with custom callbacks\nretriever = ParentDocumentRetriever(\n    vectorstore=vectorstore,\n    docstore=store,\n    child_splitter=child_splitter,\n    parent_splitter=parent_splitter,\n).with_listeners(\n    on_start=log_start,\n    on_end=log_end,\n    on_error=log_error\n)\n\n# Add sample documents (use Document objects)\ndocuments = [\n    Document(page_content=\"Callbacks enhance the functionality of retrieval processes.\", metadata={\"doc_id\": \"doc1\"}),\n    Document(page_content=\"They allow for custom operations during retrieval.\", metadata={\"doc_id\": \"doc2\"}),\n]\nretriever.add_documents(documents)\n\n# Invoke retrieval to trigger custom callbacks\nquery = \"callbacks in retrieval\"\n\nretrieved_docs = retriever.invoke(query)\n\nfor doc in retrieved_docs:\n    print(f\"Retrieved Document: {doc.metadata['doc_id']}, Content: {doc.page_content}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T18:42:44.560704Z","iopub.execute_input":"2025-01-19T18:42:44.561033Z","iopub.status.idle":"2025-01-19T18:42:45.61606Z","shell.execute_reply.started":"2025-01-19T18:42:44.561009Z","shell.execute_reply":"2025-01-19T18:42:45.615309Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n    \n## 5. Error Handling\n    \n### **Implementing Retry Logic with `with_retry`**\n    \nThis example demonstrates how to add retry logic to the `ParentDocumentRetriever` using the `with_retry` method. The retriever will attempt to retry the retrieval operation upon encountering specified exceptions.","metadata":{}},{"cell_type":"code","source":"from langchain_chroma import Chroma\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain.storage import InMemoryStore\nfrom langchain.retrievers.parent_document_retriever import ParentDocumentRetriever\nfrom langchain.schema import Document\n\n# Initialize text splitters\nparent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, add_start_index=True)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400, add_start_index=True)\n\n# Initialize vectorstore and docstore with a unique collection name\nvectorstore = Chroma(embedding_function=embed, collection_name=\"error_handling_retry_logic\")\nstore = InMemoryStore()\n\n# Initialize the ParentDocumentRetriever\nretriever = ParentDocumentRetriever(\n    vectorstore=vectorstore,\n    docstore=store,\n    child_splitter=child_splitter,\n    parent_splitter=parent_splitter,\n)\n\n# Add sample documents (use Document objects)\ndocuments = [\n    Document(page_content=\"Reliable retrieval is crucial for applications.\", metadata={\"doc_id\": \"doc1\"}),\n]\nretriever.add_documents(documents)\n\n# Apply retry logic to the retriever\nretriever_with_retry = retriever.with_retry(\n    stop_after_attempt=3,\n    retry_if_exception_type=(ValueError,),\n    wait_exponential_jitter=True\n)\n\n# Invoke retrieval with retry logic\nquery = \"reliable retrieval\"\n\ntry:\n    retrieved_docs = retriever_with_retry.invoke(query)\n    for doc in retrieved_docs:\n        print(f\"Retrieved Document: {doc.metadata['doc_id']}, Content: {doc.page_content}\")\nexcept ValueError as e:\n    print(f\"Retrieval failed after retries: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T18:43:34.623823Z","iopub.execute_input":"2025-01-19T18:43:34.624231Z","iopub.status.idle":"2025-01-19T18:43:35.272325Z","shell.execute_reply.started":"2025-01-19T18:43:34.624198Z","shell.execute_reply":"2025-01-19T18:43:35.271495Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Handling Specific Exceptions with Retries**\n    \nThis example showcases how to configure the `with_retry` method to handle specific exception types. The retriever will only retry upon encountering the specified exceptions, allowing for more granular error management.","metadata":{}},{"cell_type":"code","source":"from langchain_chroma import Chroma\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain.storage import InMemoryStore\nfrom langchain.retrievers.parent_document_retriever import ParentDocumentRetriever\nfrom langchain.schema import Document\n\n# Initialize text splitters\nparent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, add_start_index=True)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400, add_start_index=True)\n\n# Initialize vectorstore and docstore with a unique collection name\nvectorstore = Chroma(embedding_function=embed, collection_name=\"error_handling_specific_retries\")\nstore = InMemoryStore()\n\n# Initialize the ParentDocumentRetriever\nretriever = ParentDocumentRetriever(\n    vectorstore=vectorstore,\n    docstore=store,\n    child_splitter=child_splitter,\n    parent_splitter=parent_splitter,\n)\n\n# Add sample documents (use Document objects)\ndocuments = [\n    Document(page_content=\"Selective error handling allows for precise control.\", metadata={\"doc_id\": \"doc1\"}),\n]\nretriever.add_documents(documents)\n\n# Apply selective retry logic\nretriever_with_retry = retriever.with_retry(\n    stop_after_attempt=2,\n    retry_if_exception_type=(ValueError,),  # Only retry on ValueError\n    wait_exponential_jitter=False\n)\n\n# Invoke retrieval with selective retry logic\nquery = \"selective error handling\"\n\ntry:\n    retrieved_docs = retriever_with_retry.invoke(query)\n    for doc in retrieved_docs:\n        print(f\"Retrieved Document: {doc.metadata['doc_id']}, Content: {doc.page_content}\")\nexcept Exception as e:\n    print(f\"Retrieval failed: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T18:45:23.041459Z","iopub.execute_input":"2025-01-19T18:45:23.041781Z","iopub.status.idle":"2025-01-19T18:45:23.914184Z","shell.execute_reply.started":"2025-01-19T18:45:23.041759Z","shell.execute_reply":"2025-01-19T18:45:23.913272Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Combining Retry with Fallbacks**\n    \nWhile the `ParentDocumentRetriever` categorizes fallbacks separately, combining retry logic with fallbacks can enhance robustness. This example demonstrates how to set up both retry mechanisms and fallback retrievers to ensure successful retrieval even in the face of multiple failures.","metadata":{}},{"cell_type":"code","source":"from langchain_chroma import Chroma\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain.storage import InMemoryStore\nfrom langchain.retrievers.parent_document_retriever import ParentDocumentRetriever\nfrom langchain.schema import Document\n\n# Initialize text splitters\nparent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, add_start_index=True)\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400, add_start_index=True)\n\n# Initialize vectorstores and docstores for primary and fallback with unique collection names\nprimary_vectorstore = Chroma(embedding_function=embed, collection_name=\"error_handling_retry_with_fallbacks_primary\")\nfallback_vectorstore = Chroma(embedding_function=embed, collection_name=\"error_handling_retry_with_fallbacks_fallback\")\nprimary_store = InMemoryStore()\nfallback_store = InMemoryStore()\n\n# Initialize the primary retriever\nprimary_retriever = ParentDocumentRetriever(\n    vectorstore=primary_vectorstore,\n    docstore=primary_store,\n    child_splitter=child_splitter,\n    parent_splitter=parent_splitter,\n)\n\n# Add sample documents to the primary retriever\nprimary_retriever.add_documents([\n    Document(page_content=\"Primary retriever document content.\", metadata={\"doc_id\": \"primary_doc\"}),\n])\n\n# Apply retry logic to the primary retriever\nprimary_with_retry = primary_retriever.with_retry(\n    stop_after_attempt=2,\n    retry_if_exception_type=(ValueError,),\n    wait_exponential_jitter=False\n)\n\n# Initialize the fallback retriever\nfallback_retriever = ParentDocumentRetriever(\n    vectorstore=fallback_vectorstore,\n    docstore=fallback_store,\n    child_splitter=child_splitter,\n    parent_splitter=parent_splitter,\n)\n\n# Add sample documents to the fallback retriever\nfallback_retriever.add_documents([\n    Document(page_content=\"Fallback retriever document content.\", metadata={\"doc_id\": \"fallback_doc\"}),\n])\n\n# Combine the primary retriever with fallback retrievers\ncombined_retriever = primary_with_retry.with_fallbacks(\n    fallbacks=[fallback_retriever],\n    exceptions_to_handle=(ValueError,)\n)\n\n# Invoke combined retriever\nquery = \"robust retrieval\"\n\nretrieved_docs = combined_retriever.invoke(query)\n\nfor doc in retrieved_docs:\n    print(f\"Retrieved Document: {doc.metadata['doc_id']}, Content: {doc.page_content}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T18:45:45.726455Z","iopub.execute_input":"2025-01-19T18:45:45.726811Z","iopub.status.idle":"2025-01-19T18:45:46.636805Z","shell.execute_reply.started":"2025-01-19T18:45:45.726767Z","shell.execute_reply":"2025-01-19T18:45:46.635954Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n## 6. Best Practices\n\n### **Using ParentDocumentRetriever for Full and Larger Chunk Retrieval**\n\n#### **Loading and Preparing Documents**\nThis section demonstrates loading documents from text files and preparing them for retrieval by using the `TextLoader`.","metadata":{}},{"cell_type":"code","source":"from langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain_chroma import Chroma\nfrom langchain_community.document_loaders import TextLoader\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\n# Load documents\nloaders = [\n    TextLoader(\"/kaggle/input/paul-graham-essay/paul_graham_essay.txt\"),\n    TextLoader(\"/kaggle/input/paul-graham-essay/state_of_the_union.txt\"),\n]\ndocs = []\nfor loader in loaders:\n    docs.extend(loader.load())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T18:57:50.691164Z","iopub.execute_input":"2025-01-19T18:57:50.691502Z","iopub.status.idle":"2025-01-19T18:57:50.712515Z","shell.execute_reply.started":"2025-01-19T18:57:50.691478Z","shell.execute_reply":"2025-01-19T18:57:50.711526Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **Retrieving Full Documents Using Small Chunks**\nIn this mode, documents are split into small chunks for indexing and retrieval. The `ParentDocumentRetriever` is configured to use only a child splitter.","metadata":{}},{"cell_type":"code","source":"# This text splitter is used to create the child documents\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n\n# The vectorstore to use to index the child chunks\nvectorstore = Chroma(collection_name=\"full_documents\", embedding_function=embed)\n\n# The storage layer for the parent documents\nstore = InMemoryStore()\nretriever = ParentDocumentRetriever(\n    vectorstore=vectorstore,\n    docstore=store,\n    child_splitter=child_splitter,\n)\n\n# Add documents to the retriever\nretriever.add_documents(docs, ids=None)\n\n# List keys in the document store\nprint(\"Number of keys:\", list(store.yield_keys()))\n\n# Perform similarity search in vectorstore\nsub_docs = vectorstore.similarity_search(\"justice breyer\")\nprint(\"Content length:\", len(sub_docs[0].page_content))\nprint(sub_docs[0].page_content)\n\n# Retrieve documents using the retriever\nretrieved_docs = retriever.invoke(\"justice breyer\")\nprint(\"Content length:\", len(retrieved_docs[0].page_content))\nprint(len(retrieved_docs[0].page_content))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T18:58:02.506146Z","iopub.execute_input":"2025-01-19T18:58:02.50643Z","iopub.status.idle":"2025-01-19T18:58:07.228663Z","shell.execute_reply.started":"2025-01-19T18:58:02.506411Z","shell.execute_reply":"2025-01-19T18:58:07.227742Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **Retrieving Larger Chunks with Parent Splitting**\nIn this mode, documents are first split into larger chunks (parent documents), which are further split into smaller chunks (child documents). This provides a balance between granularity and context.","metadata":{}},{"cell_type":"code","source":"# This text splitter is used to create the parent documents\nparent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n\n# This text splitter is used to create the child documents\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n\n# The vectorstore to use to index the child chunks\nvectorstore = Chroma(collection_name=\"split_parents\", embedding_function=embed)\n\n# The storage layer for the parent documents\nstore = InMemoryStore()\n\n# Initialize the retriever with parent and child splitters\nretriever = ParentDocumentRetriever(\n    vectorstore=vectorstore,\n    docstore=store,\n    child_splitter=child_splitter,\n    parent_splitter=parent_splitter,\n)\n\n# Add documents to the retriever\nretriever.add_documents(docs)\n\n# Check the number of keys in the document store\nprint(\"Number of keys:\", len(list(store.yield_keys())))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T18:59:25.191435Z","iopub.execute_input":"2025-01-19T18:59:25.191808Z","iopub.status.idle":"2025-01-19T18:59:29.962443Z","shell.execute_reply.started":"2025-01-19T18:59:25.191776Z","shell.execute_reply":"2025-01-19T18:59:29.961657Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Perform similarity search in vectorstore\nsub_docs = vectorstore.similarity_search(\"justice breyer\")\nprint(\"Content length:\", len(sub_docs[0].page_content))\nprint(sub_docs[0].page_content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T18:59:29.963476Z","iopub.execute_input":"2025-01-19T18:59:29.963784Z","iopub.status.idle":"2025-01-19T18:59:30.349412Z","shell.execute_reply.started":"2025-01-19T18:59:29.963753Z","shell.execute_reply":"2025-01-19T18:59:30.348513Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Retrieve documents using the retriever\nretrieved_docs = retriever.invoke(\"justice breyer\")\nprint(\"Content length:\", len(retrieved_docs[0].page_content))\nprint(retrieved_docs[0].page_content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T18:59:30.350994Z","iopub.execute_input":"2025-01-19T18:59:30.35132Z","iopub.status.idle":"2025-01-19T18:59:30.874957Z","shell.execute_reply.started":"2025-01-19T18:59:30.351295Z","shell.execute_reply":"2025-01-19T18:59:30.874263Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Conclusion\n\nThe `ParentDocumentRetriever` is a versatile tool that combines the strengths of granular similarity searches with the ability to retrieve documents at a broader contextual level. By enabling users to split documents into multiple hierarchical levels, it allows for a customizable and efficient retrieval process. Whether you are retrieving small snippets for precise searches or larger chunks for contextual analysis, the `ParentDocumentRetriever` offers an intuitive and scalable solution to document retrieval challenges. Its seamless integration with text splitting tools, vector stores like `Chroma`, and metadata-based storage ensures it can adapt to a wide range of use cases, providing both accuracy and context in retrieval tasks.","metadata":{}}]}