{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **How to Choose the Right Text Splitter for Your RAG Design**\n\n## **Introduction**\n\nIn the world of natural language processing (NLP) and document processing, splitting text into smaller, manageable chunks is a critical step for tasks like embedding generation, semantic search, summarization, and more. The choice of text-splitting method can significantly impact the quality of downstream applications. This article explores three powerful text-splitting classes from the `langchain-text-splitters` library: **`RecursiveCharacterTextSplitter`**, **`MarkdownHeaderTextSplitter`**, and **`SentenceTransformersTokenTextSplitter`**. Each of these tools is designed for specific use cases, offering unique features and capabilities. Through practical examples, we’ll demonstrate how to use these classes effectively and highlight their strengths. Additionally, a comparison table will help you choose the right tool for your needs.\n\n### **Comparison Table**\n\n| Feature/Class                     | RecursiveCharacterTextSplitter       | MarkdownHeaderTextSplitter           | SentenceTransformersTokenTextSplitter |\n|-----------------------------------|--------------------------------------|--------------------------------------|----------------------------------------|\n| **Primary Use Case**              | General-purpose text splitting       | Splitting Markdown by headers        | Token-based splitting for embeddings   |\n| **Splitting Mechanism**           | Recursive splitting by separators    | Header-based splitting               | Tokenization using Sentence Transformers |\n| **Customizable Separators**       | Yes                                  | No (headers are predefined)          | No (uses model tokenizer)              |\n| **Preserves Document Structure**  | No                                   | Yes (header hierarchy)               | No                                     |\n| **Token-Based Splitting**         | No                                   | No                                   | Yes                                    |\n| **Chunk Size Control**            | Yes (character-based)                | Yes (header-based)                   | Yes (token-based)                      |\n| **Overlap Between Chunks**        | Yes                                  | No                                   | Yes                                    |\n| **Best For**                      | General text processing              | Markdown documents                   | Embedding generation, NLP tasks        |","metadata":{}},{"cell_type":"markdown","source":"This code block installs two Python libraries, `langchain_community` and `langchain_experimental`, using `pip`. \n\n- **`langchain_community`**: Likely contains community-driven extensions or integrations for the LangChain framework.\n- **`langchain_experimental`**: Includes experimental features or tools for LangChain, which may be in active development or testing phases.\n\nThe `-qU` flags ensure that the installation is quiet (minimal output) and updates to the latest version if already installed.","metadata":{}},{"cell_type":"code","source":"!pip install -qU langchain_community\n!pip install -qU langchain_experimental","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T12:19:41.443529Z","iopub.execute_input":"2025-01-24T12:19:41.443877Z","iopub.status.idle":"2025-01-24T12:19:50.975163Z","shell.execute_reply.started":"2025-01-24T12:19:41.44385Z","shell.execute_reply":"2025-01-24T12:19:50.973708Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n## **1. RecursiveCharacterTextSplitter**\n\nThe `RecursiveCharacterTextSplitter` splits text recursively based on a list of separators (e.g., `\\n\\n`, `\\n`, ` `, etc.). It is useful for breaking down large texts into smaller chunks while preserving context.\n\n### **Example 1: Basic Usage**\n\nThis example demonstrates the basic usage of the `RecursiveCharacterTextSplitter`. It initializes the splitter with a specified chunk size and overlap, then splits a sample text into smaller chunks. Finally, it prints each resulting chunk to the console.","metadata":{}},{"cell_type":"code","source":"from langchain_text_splitters import RecursiveCharacterTextSplitter\n\n# Initialize the splitter\nsplitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n\n# Sample text\ntext = \"This is a sample text. It will be split into smaller chunks. The goal is to ensure that each chunk is manageable and retains context.\"\n\n# Split the text\nchunks = splitter.split_text(text)\n\n# Output the chunks with index\nfor index, chunk in enumerate(chunks):\n    print(f\"Chunk {index + 1}: {chunk}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T12:21:39.914177Z","iopub.execute_input":"2025-01-24T12:21:39.914572Z","iopub.status.idle":"2025-01-24T12:21:39.921076Z","shell.execute_reply.started":"2025-01-24T12:21:39.914535Z","shell.execute_reply":"2025-01-24T12:21:39.919645Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Example 2: Custom Separators**\n\nIn this example, the splitter is initialized with custom separators, including newline characters, spaces, and periods. This allows for more granular control over how the text is split, ensuring that splits occur at logical punctuation marks or line breaks.","metadata":{}},{"cell_type":"code","source":"# Initialize the splitter with custom separators\nsplitter = RecursiveCharacterTextSplitter(\n    separators=[\"\\n\", \" \", \".\"],  # Split by newline, space, and period\n    chunk_size=50,\n    chunk_overlap=10\n)\n\n# Sample text\ntext = \"This is a sample text.\\nIt will be split into smaller chunks.\\nThe goal is to ensure that each chunk is manageable.\"\n\n# Split the text\nchunks = splitter.split_text(text)\n\n# Output the chunks with index\nfor index, chunk in enumerate(chunks):\n    print(f\"Chunk {index + 1}: {chunk}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T12:21:55.290346Z","iopub.execute_input":"2025-01-24T12:21:55.290768Z","iopub.status.idle":"2025-01-24T12:21:55.296308Z","shell.execute_reply.started":"2025-01-24T12:21:55.290735Z","shell.execute_reply":"2025-01-24T12:21:55.295123Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Example 3: Splitting Documents**\n\nThis example showcases how to split multiple documents using the `RecursiveCharacterTextSplitter`. It imports the `Document` class, initializes the splitter, and processes a list of sample documents. Each split document's content is then printed.","metadata":{}},{"cell_type":"code","source":"from langchain_core.documents import Document\n\n# Initialize the splitter\nsplitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n\n# Sample documents\ndocuments = [\n    Document(page_content=\"This is the first document. It contains some text.\"),\n    Document(page_content=\"This is the second document. It contains more text.\")\n]\n\n# Split the documents\nsplit_docs = splitter.split_documents(documents)\n\n# Output the split documents with index\nfor index, doc in enumerate(split_docs):\n    print(f\"Doc {index + 1}: {doc.page_content}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T12:22:54.835359Z","iopub.execute_input":"2025-01-24T12:22:54.83577Z","iopub.status.idle":"2025-01-24T12:22:54.842395Z","shell.execute_reply.started":"2025-01-24T12:22:54.83574Z","shell.execute_reply":"2025-01-24T12:22:54.841345Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Example 4: Using with Metadata**\n\nIn this example, the splitter is used to create documents that include metadata. It initializes the splitter, prepares sample texts along with their corresponding metadata, and generates documents that pair each text chunk with its metadata. The resulting documents are then printed, displaying both content and metadata.","metadata":{}},{"cell_type":"code","source":"# Initialize the splitter\nsplitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n\n# Sample text with metadata\ntexts = [\"This is a sample text.\", \"It will be split into smaller chunks.\"]\nmetadatas = [{\"source\": \"doc1\"}, {\"source\": \"doc2\"}]\n\n# Create documents with metadata\ndocs = splitter.create_documents(texts, metadatas=metadatas)\n\n# Output the documents with index\nfor index, doc in enumerate(docs):\n    print(f\"Doc {index + 1}:\\n  page_content: {doc.page_content}\\n  metadata: {doc.metadata}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T12:24:59.158219Z","iopub.execute_input":"2025-01-24T12:24:59.158687Z","iopub.status.idle":"2025-01-24T12:24:59.16498Z","shell.execute_reply.started":"2025-01-24T12:24:59.158657Z","shell.execute_reply":"2025-01-24T12:24:59.163755Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n## **2. MarkdownHeaderTextSplitter**\n\nThe `MarkdownHeaderTextSplitter` splits Markdown documents based on specified headers, preserving the hierarchical structure of the document.\n\n### **Example 1: Basic Usage**\n\nThis example illustrates the basic usage of the `MarkdownHeaderTextSplitter`. It defines a hierarchy of headers to split on, initializes the splitter with these headers, and processes a sample Markdown text. Each resulting chunk corresponding to a header level is then printed.","metadata":{}},{"cell_type":"code","source":"from langchain_text_splitters import MarkdownHeaderTextSplitter\n\n# Define headers to split on\nheaders_to_split_on = [\n    (\"#\", \"Header 1\"),\n    (\"##\", \"Header 2\"),\n    (\"###\", \"Header 3\"),\n]\n\n# Initialize the splitter\nsplitter = MarkdownHeaderTextSplitter(headers_to_split_on)\n\n# Sample Markdown text\nmarkdown_text = \"\"\"\n# Level-1\nThis is some text under Header 1.\n\n## Level-2\nThis is some text under Header 2.\n\n### Level-3\nThis is some text under Header 3.\n\"\"\"\n\n# Split the text\nchunks = splitter.split_text(markdown_text)\n\n# Output the chunks with index\nfor index, chunk in enumerate(chunks):\n    print(f\"Chunk {index + 1}: {chunk}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T12:54:19.60602Z","iopub.execute_input":"2025-01-24T12:54:19.606372Z","iopub.status.idle":"2025-01-24T12:54:19.613685Z","shell.execute_reply.started":"2025-01-24T12:54:19.606345Z","shell.execute_reply":"2025-01-24T12:54:19.611856Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Example 2: Keeping Headers in Content**\n\nIn this example, the splitter is configured to retain the headers within the content of each chunk. By setting `strip_headers=False`, the original headers are preserved in the resulting text chunks.","metadata":{}},{"cell_type":"code","source":"# Initialize the splitter with headers kept in content\nsplitter = MarkdownHeaderTextSplitter(headers_to_split_on, strip_headers=False)\n\n# Split the text\nchunks = splitter.split_text(markdown_text)\n\n# Output the chunks\nfor chunk in chunks:\n    print(f\"{chunk.page_content}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T12:54:25.753967Z","iopub.execute_input":"2025-01-24T12:54:25.754328Z","iopub.status.idle":"2025-01-24T12:54:25.760153Z","shell.execute_reply.started":"2025-01-24T12:54:25.754301Z","shell.execute_reply":"2025-01-24T12:54:25.75879Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Example 3: Returning Each Line as a Separate Document**\n\nThis example demonstrates how to configure the splitter to treat each line of the Markdown text as a separate document. By setting `return_each_line=True`, the splitter processes and returns each line individually.","metadata":{}},{"cell_type":"code","source":"# Initialize the splitter to return each line as a separate document\nsplitter = MarkdownHeaderTextSplitter(headers_to_split_on, return_each_line=True)\n\n# Split the text\nchunks = splitter.split_text(markdown_text)\n\n# Output the chunks with index\nfor index, chunk in enumerate(chunks):\n    print(f\"Chunk {index + 1}: {chunk}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T12:54:29.115192Z","iopub.execute_input":"2025-01-24T12:54:29.115612Z","iopub.status.idle":"2025-01-24T12:54:29.121997Z","shell.execute_reply.started":"2025-01-24T12:54:29.115581Z","shell.execute_reply":"2025-01-24T12:54:29.121032Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Example 4: Combining with RecursiveCharacterTextSplitter**\n\nHere, the example shows how to combine `MarkdownHeaderTextSplitter` with `RecursiveCharacterTextSplitter` for more granular splitting. First, the Markdown text is split based on headers, and then each resulting chunk is further split into smaller character-based chunks. The final split chunks are then printed.","metadata":{}},{"cell_type":"code","source":"from langchain_text_splitters import RecursiveCharacterTextSplitter\n\n# Initialize the Markdown splitter\nmarkdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on)\n\n# Split the Markdown text\nmd_chunks = markdown_splitter.split_text(markdown_text)\n\n# Initialize the character splitter\nchar_splitter = RecursiveCharacterTextSplitter(chunk_size=50, chunk_overlap=10)\n\n# Further split the Markdown chunks\nfinal_chunks = char_splitter.split_documents(md_chunks)\n\n# Output the chunks with index\nfor index, chunk in enumerate(final_chunks):\n    print(f\"Chunk {index + 1}: {chunk.page_content}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T12:54:31.99914Z","iopub.execute_input":"2025-01-24T12:54:31.999574Z","iopub.status.idle":"2025-01-24T12:54:32.006695Z","shell.execute_reply.started":"2025-01-24T12:54:31.999541Z","shell.execute_reply":"2025-01-24T12:54:32.005725Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n## **3. SentenceTransformersTokenTextSplitter**\n\nThe `SentenceTransformersTokenTextSplitter` splits text into tokens using a Sentence Transformers model tokenizer, ensuring alignment with the model's token boundaries.\n\n### **Example 1: Basic Usage**\n\nThis example demonstrates the basic usage of the `SentenceTransformersTokenTextSplitter`. It initializes the splitter with specified token parameters, splits a sample text based on tokenization, and prints each resulting chunk.","metadata":{}},{"cell_type":"code","source":"from langchain_text_splitters import SentenceTransformersTokenTextSplitter\n\n# Initialize the splitter\nsplitter = SentenceTransformersTokenTextSplitter(chunk_overlap=20, tokens_per_chunk=100)\n\n# Longer sample text\ntext = \"\"\"\nNatural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language. \nIt focuses on how to program computers to process and analyze large amounts of natural language data. \nThe goal is to enable computers to understand, interpret, and generate human language in a way that is both meaningful and useful. \nNLP techniques are used in a wide range of applications, including machine translation, sentiment analysis, speech recognition, and text summarization. \nOne of the key challenges in NLP is dealing with the ambiguity and complexity of human language. \nFor example, the same word can have multiple meanings depending on the context, and sentences can be structured in many different ways. \nTo address these challenges, NLP researchers use a variety of techniques, including statistical models, machine learning algorithms, and deep learning architectures. \nIn recent years, advances in deep learning have led to significant improvements in NLP tasks, such as language modeling, text generation, and question answering. \nThese advancements have been driven by the development of large-scale pre-trained language models, such as BERT, GPT, and T5, which are trained on massive amounts of text data and can be fine-tuned for specific tasks.\n\"\"\"\n\n# Split the text\nchunks = splitter.split_text(text)\n\n# Output the chunks with index\nfor index, chunk in enumerate(chunks):\n    print(f\"Chunk {index + 1}:\\n{chunk}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T12:56:22.803208Z","iopub.execute_input":"2025-01-24T12:56:22.803678Z","iopub.status.idle":"2025-01-24T12:56:24.156966Z","shell.execute_reply.started":"2025-01-24T12:56:22.803645Z","shell.execute_reply":"2025-01-24T12:56:24.15536Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Example 2: Counting Tokens**\n\nIn this example, the splitter is used to count the number of tokens in a given text. This is useful for understanding how text is tokenized and ensuring that it fits within model constraints.","metadata":{}},{"cell_type":"code","source":"# Count tokens in the text\ntoken_count = splitter.count_tokens(text=text)\nprint(f\"Token count: {token_count}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T12:56:34.592935Z","iopub.execute_input":"2025-01-24T12:56:34.593303Z","iopub.status.idle":"2025-01-24T12:56:34.599817Z","shell.execute_reply.started":"2025-01-24T12:56:34.593274Z","shell.execute_reply":"2025-01-24T12:56:34.598641Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Example 3: Splitting Documents**\n\nThis example showcases how to split multiple documents using the `SentenceTransformersTokenTextSplitter`. It processes a list of sample documents, splits each based on tokenization, and prints the content of each resulting split document.","metadata":{}},{"cell_type":"code","source":"from langchain_core.documents import Document\n\n# Sample documents\ndocuments = [\n    Document(page_content=\"This is the first document.\"),\n    Document(page_content=\"This is the second document.\")\n]\n\n# Split the documents\nsplit_docs = splitter.split_documents(documents)\n\n# Output the split documents\nfor doc in split_docs:\n    print(doc.page_content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T12:56:43.074066Z","iopub.execute_input":"2025-01-24T12:56:43.074416Z","iopub.status.idle":"2025-01-24T12:56:43.082191Z","shell.execute_reply.started":"2025-01-24T12:56:43.07439Z","shell.execute_reply":"2025-01-24T12:56:43.081112Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Example 4: Using a Custom Model**\n\nHere, the splitter is initialized with a custom Sentence Transformers model. This allows for tokenization that aligns with the specific tokenizer of the chosen model, providing more accurate splits based on the model's token boundaries.","metadata":{}},{"cell_type":"code","source":"# Initialize the splitter with a custom Sentence Transformers model\nsplitter = SentenceTransformersTokenTextSplitter(\n    model_name=\"sentence-transformers/paraphrase-MiniLM-L6-v2\",\n    tokens_per_chunk=50,\n    chunk_overlap=10\n)\n\n# Split the text\nchunks = splitter.split_text(text)\n\n# Output the chunks with index\nfor index, chunk in enumerate(chunks):\n    print(f\"Chunk {index + 1}:\\n{chunk}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T12:57:27.798981Z","iopub.execute_input":"2025-01-24T12:57:27.799341Z","iopub.status.idle":"2025-01-24T12:57:28.956206Z","shell.execute_reply.started":"2025-01-24T12:57:27.799306Z","shell.execute_reply":"2025-01-24T12:57:28.955038Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n## **Conclusion**\n\nText splitting is a foundational step in many NLP and document processing workflows. The choice of the right tool depends on the nature of your data and the specific requirements of your task. \n\n- **`RecursiveCharacterTextSplitter`** is a versatile choice for general-purpose text splitting, offering flexibility in defining separators and chunk sizes.\n- **`MarkdownHeaderTextSplitter`** excels at preserving the hierarchical structure of Markdown documents, making it ideal for processing structured content.\n- **`SentenceTransformersTokenTextSplitter`** is tailored for token-based splitting, ensuring compatibility with Sentence Transformers models for tasks like embedding generation and semantic search.\n\nBy understanding the strengths and use cases of each class, you can make informed decisions and optimize your text-processing pipelines. Whether you’re working with plain text, Markdown, or tokenized data, these tools provide robust solutions for splitting text effectively.","metadata":{}}]}