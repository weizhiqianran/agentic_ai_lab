{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Agentic RAG with LangGraph\n\n## Introduction\n\nAgentic RAG (Retrieval-Augmented Generation) is a powerful approach that combines retrieval-based methods with generative models to enhance the quality and relevance of responses. In this example, we implement a **Retrieval Agent** using LangGraph, which allows an LLM (Large Language Model) to make decisions about whether to retrieve information from an indexed dataset. By giving the LLM access to a retriever tool, we enable it to dynamically decide when to retrieve additional context and when to generate responses directly.\n\nThis example demonstrates how to:\n1. Load and index blog posts into a vector store.\n2. Create a retriever tool for searching the indexed data.\n3. Define an agent state and workflow using LangGraph.\n4. Implement nodes for grading document relevance, rewriting queries, and generating responses.\n5. Visualize and execute the graph-based workflow.","metadata":{}},{"cell_type":"markdown","source":"## Step 0: Install Required Libraries\n\nInstall the necessary Python libraries for the project. This includes libraries for working with OpenAI, Anthropic, LangChain, LangGraph, and ChromaDB.","metadata":{}},{"cell_type":"code","source":"!pip install -qU langchain-openai\n!pip install -qU langchain-anthropic\n!pip install -qU langchain_community\n!pip install -qU langchain_experimental\n!pip install -qU langgraph\n!pip install -qU chromadb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-15T12:36:37.688324Z","iopub.execute_input":"2025-01-15T12:36:37.688745Z","iopub.status.idle":"2025-01-15T12:37:53.224556Z","shell.execute_reply.started":"2025-01-15T12:36:37.688705Z","shell.execute_reply":"2025-01-15T12:37:53.223436Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 1: Setup and Document Loading\n\nLoad blog posts from specified URLs, split them into smaller chunks, and store them in a vector database for retrieval.","metadata":{}},{"cell_type":"markdown","source":"## Step 2: Retriever Tool Creation\nThis step involves creating a retriever tool that enables the agent to search and retrieve relevant information from the indexed blog posts. The retriever tool is built using the vector store, which contains the processed and split document chunks. This tool is designed to help the agent efficiently query and fetch specific information from the stored blog posts.","metadata":{}},{"cell_type":"code","source":"from langchain_community.document_loaders import WebBaseLoader\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom kaggle_secrets import UserSecretsClient\n\n# Retrieve the LLM API Key\nuser_secrets = UserSecretsClient()\nmy_api_key = user_secrets.get_secret(\"my-openai-api-key\")\n\n# Use OpenAI embeddings for vectorization\nembedding = OpenAIEmbeddings(model=\"text-embedding-3-small\", api_key=my_api_key)\n\n# Define URLs to load blog posts\nurls = [\n    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n]\n\n# Load documents from the specified URLs\ndocs = [WebBaseLoader(url).load() for url in urls]\ndocs_list = [item for sublist in docs for item in sublist]  # Flatten the list of documents\n\n# Split documents into smaller chunks for processing\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n    chunk_size=100,   # Size of each chunk\n    chunk_overlap=50  # Overlap between chunks to maintain context\n)\ndoc_splits = text_splitter.split_documents(docs_list)\n\n# Add the split documents to a vector store for retrieval\nvectorstore = Chroma.from_documents(\n    documents=doc_splits,\n    collection_name=\"rag-chroma\",\n    embedding=embedding,           \n)\nretriever = vectorstore.as_retriever()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T12:46:02.871804Z","iopub.execute_input":"2025-01-15T12:46:02.872165Z","iopub.status.idle":"2025-01-15T12:46:13.689976Z","shell.execute_reply.started":"2025-01-15T12:46:02.87214Z","shell.execute_reply":"2025-01-15T12:46:13.688029Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This step involves creating a retriever tool that allows the agent to search and retrieve information from the stored blog posts. The tool is added to a list of tools available for the agent to use.","metadata":{}},{"cell_type":"code","source":"from langchain.tools.retriever import create_retriever_tool\n\n# Create a retriever tool to search and retrieve blog post information\nretriever_tool = create_retriever_tool(retriever, \"retrieve_blog_posts\",\n    \"Search and return information about Lilian Weng blog posts on LLM agents, prompt engineering, and adversarial attacks on LLMs.\",\n)\n\n# List of tools available for the agent\ntools = [retriever_tool]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T12:46:13.691856Z","iopub.execute_input":"2025-01-15T12:46:13.692202Z","iopub.status.idle":"2025-01-15T12:46:13.705099Z","shell.execute_reply.started":"2025-01-15T12:46:13.692168Z","shell.execute_reply":"2025-01-15T12:46:13.703759Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 3: Agent State Definition\n\nDefine the state of the agent, which consists of a sequence of messages. The state is passed between nodes in the graph.","metadata":{}},{"cell_type":"code","source":"from typing import Annotated, Sequence\nfrom typing_extensions import TypedDict\nfrom langchain_core.messages import BaseMessage\nfrom langgraph.graph.message import add_messages\n\n# Define the state of the agent, which is a sequence of messages\nclass AgentState(TypedDict):\n    messages: Annotated[Sequence[BaseMessage], add_messages]  # Messages are appended to the state","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T12:46:13.707475Z","iopub.execute_input":"2025-01-15T12:46:13.708054Z","iopub.status.idle":"2025-01-15T12:46:13.808074Z","shell.execute_reply.started":"2025-01-15T12:46:13.707999Z","shell.execute_reply":"2025-01-15T12:46:13.806669Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 4: Nodes and Edges Definition\n\nIn this step, we define the **nodes** and **edges** that make up the graph-based workflow. Each node represents a specific task or decision point, while the edges define the flow of logic between these nodes. The nodes include:\n\n1. **`grade_documents`**: Determines whether the retrieved documents are relevant to the user's question. If the documents are relevant, the workflow proceeds to generate a response. If not, the workflow rewrites the query to improve its clarity or relevance.\n2. **`agent_node`**: The core decision-making node. It decides whether to retrieve additional information using the retriever tool or to end the workflow if no further retrieval is needed.\n3. **`rewrite_node`**: Rewrites the user's question to better align with the underlying semantic intent. This is useful when the retrieved documents are not relevant, and the system needs to refine the query for better results.\n4. **`generate_node`**: Generates a final response to the user's question using the retrieved documents. This node is invoked only when the documents are deemed relevant.","metadata":{}},{"cell_type":"code","source":"from typing import Annotated, Literal, Sequence, Dict, Any\nfrom typing_extensions import TypedDict\nfrom langchain import hub\nfrom langchain_core.messages import BaseMessage, HumanMessage\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_openai import ChatOpenAI\nfrom pydantic import BaseModel, Field\nfrom langgraph.prebuilt import tools_condition\n\ndef grade_documents(state: AgentState) -> Literal[\"generate_node\", \"rewrite_node\"]:\n    \"\"\"\n    Grades the relevance of retrieved documents to the user's question.\n    Returns \"generate_node\" if the documents are relevant, otherwise \"rewrite_node\".\n\n    Args:\n        state (AgentState): The current state containing messages.\n\n    Returns:\n        Literal[\"generate_node\", \"rewrite_node\"]: Decision based on document relevance.\n    \"\"\"\n    print(\"---CHECK RELEVANCE---\")\n\n    # Define a Pydantic model for grading relevance\n    class Grade(BaseModel):\n        binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n\n    # Initialize the LLM for grading\n    model = ChatOpenAI(model_name=\"gpt-4o-mini\", api_key=my_api_key, temperature=0, streaming=True)\n    llm_with_tool = model.with_structured_output(Grade)\n\n    # Define the prompt for relevance grading\n    prompt = PromptTemplate(\n        template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n        Here is the retrieved document: \\n\\n {context} \\n\\n\n        Here is the user question: {question} \\n\n        If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\",\n        input_variables=[\"context\", \"question\"],\n    )\n\n    # Create a chain to process the grading\n    chain = prompt | llm_with_tool\n\n    # Extract the last message and question from the state\n    messages = state[\"messages\"]\n    last_message = messages[-1]\n    question = messages[0].content\n    docs = last_message.content\n\n    # Invoke the chain to grade the documents\n    scored_result = chain.invoke({\"question\": question, \"context\": docs})\n    score = scored_result.binary_score\n\n    # Return the decision based on the score\n    if score == \"yes\":\n        print(\"---DECISION: DOCS RELEVANT---\")\n        return \"generate_node\"\n    else:\n        print(\"---DECISION: DOCS NOT RELEVANT---\")\n        print(score)\n        return \"rewrite_node\"\n\ndef agent_node(state: AgentState) -> Dict[str, Sequence[BaseMessage]]:\n    \"\"\"\n    Invokes the agent to generate a response or decide to retrieve information.\n\n    Args:\n        state (AgentState): The current state containing messages.\n\n    Returns:\n        Dict[str, Sequence[BaseMessage]]: Updated state with the agent's response appended to messages.\n    \"\"\"\n    print(\"---CALL AGENT---\")\n    messages = state[\"messages\"]\n    model = ChatOpenAI(model_name=\"gpt-4o-mini\", api_key=my_api_key, temperature=0, streaming=True)\n    llm_with_tool = model.bind_tools(tools)    # Bind the available tools to the model\n    response = llm_with_tool.invoke(messages)  # Generate a response\n    return {\"messages\": [response]}\n\ndef rewrite_node(state: AgentState) -> Dict[str, Sequence[BaseMessage]]:\n    \"\"\"\n    Rewrites the user's question to improve clarity or relevance.\n\n    Args:\n        state (AgentState): The current state containing messages.\n\n    Returns:\n        Dict[str, Sequence[BaseMessage]]: Updated state with the rewritten question appended to messages.\n    \"\"\"\n    print(\"---TRANSFORM QUERY---\")\n    messages = state[\"messages\"]\n    question = messages[0].content\n\n    # Create a message to request a rewritten question\n    msg = [\n        HumanMessage(\n            content=f\"\"\" \\n \n    Look at the input and try to reason about the underlying semantic intent or meaning. \\n \n    Here is the initial question:\n    \\n -------------------------------------------------------- \\n\n    {question} \n    \\n -------------------------------------------------------- \\n\n    Formulate an improved question: \"\"\",\n        )\n    ]\n\n    # Invoke the LLM to rewrite the question\n    model = ChatOpenAI(model_name=\"gpt-4o-mini\", api_key=my_api_key, temperature=0, streaming=True)\n    response = model.invoke(msg)\n    return {\"messages\": [response]}\n\ndef generate_node(state: AgentState) -> Dict[str, Sequence[BaseMessage]]:\n    \"\"\"\n    Generates a response to the user's question using retrieved documents.\n\n    Args:\n        state (AgentState): The current state containing messages.\n\n    Returns:\n        Dict[str, Sequence[BaseMessage]]: Updated state with the generated response appended to messages.\n    \"\"\"\n    print(\"---GENERATE---\")\n    messages = state[\"messages\"]\n    question = messages[0].content\n    last_message = messages[-1]\n\n    docs = last_message.content  # Retrieved documents\n\n    # Pull the RAG prompt from the hub\n    prompt = hub.pull(\"rlm/rag-prompt\")\n\n    # Initialize the LLM for response generation\n    model = ChatOpenAI(model_name=\"gpt-4o-mini\", api_key=my_api_key, temperature=0, streaming=True)\n\n    # Create a RAG chain to generate the response\n    rag_chain = prompt | model | StrOutputParser()\n\n    # Invoke the chain to generate the response\n    response = rag_chain.invoke({\"context\": docs, \"question\": question})\n    return {\"messages\": [response]}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T13:51:10.396854Z","iopub.execute_input":"2025-01-15T13:51:10.397231Z","iopub.status.idle":"2025-01-15T13:51:10.412225Z","shell.execute_reply.started":"2025-01-15T13:51:10.397199Z","shell.execute_reply":"2025-01-15T13:51:10.410776Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Show what the prompt looks like\nprompt = hub.pull(\"rlm/rag-prompt\").pretty_print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T13:51:10.414111Z","iopub.execute_input":"2025-01-15T13:51:10.414456Z","iopub.status.idle":"2025-01-15T13:51:10.639963Z","shell.execute_reply.started":"2025-01-15T13:51:10.41441Z","shell.execute_reply":"2025-01-15T13:51:10.638752Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 5: Graph Construction\n\nConstruct the graph by defining nodes (agent, retrieve, rewrite, generate) and edges (conditional logic). The graph determines the flow of the workflow.","metadata":{}},{"cell_type":"code","source":"from langgraph.graph import END, StateGraph, START\nfrom langgraph.prebuilt import ToolNode\n\n# Node for retrieving documents\nretrieve_node = ToolNode([retriever_tool])\n\n# Define the workflow graph\nworkflow = StateGraph(AgentState)\n\n# Add nodes to the graph\nworkflow.add_node(\"agent_node\", agent_node)        # Agent node for decision-making\nworkflow.add_node(\"retrieve_node\", retrieve_node)  # Node for retrieving documents\nworkflow.add_node(\"rewrite_node\", rewrite_node)    # Node for rewriting the question\nworkflow.add_node(\"generate_node\", generate_node)  # Node for generating the final response\n\n# Define edges between nodes\nworkflow.add_edge(START, \"agent_node\")             # Start with the agent node\n\n# Use in the conditional_edge to route to the ToolNode if the last message has tool calls. \n# Otherwise, route to the end.\nworkflow.add_conditional_edges(\n    \"agent_node\",\n    tools_condition,               # Condition to decide whether to retrieve or end\n    {\n        \"tools\": \"retrieve_node\",  # If tools are needed, go to the retrieve node\n        END: END,                  # Otherwise, end the workflow\n    },\n)\nworkflow.add_conditional_edges(\"retrieve_node\", grade_documents, [\"generate_node\", \"rewrite_node\"])\nworkflow.add_edge(\"generate_node\", END)            # End after generating the response\nworkflow.add_edge(\"rewrite_node\", \"agent_node\")    # Go back to the agent after rewriting\n\n# Compile the graph\ngraph = workflow.compile()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T13:51:10.642214Z","iopub.execute_input":"2025-01-15T13:51:10.642703Z","iopub.status.idle":"2025-01-15T13:51:10.65193Z","shell.execute_reply.started":"2025-01-15T13:51:10.642669Z","shell.execute_reply":"2025-01-15T13:51:10.650794Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 6: Visualization\n\nVisualize the graph to understand its structure. This step is optional and requires additional dependencies.","metadata":{}},{"cell_type":"code","source":"from IPython.display import Image, display\n\n# Visualize the graph (optional, requires additional dependencies)\ntry:\n    display(Image(graph.get_graph(xray=True).draw_mermaid_png()))\nexcept Exception:\n    pass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T13:51:10.653167Z","iopub.execute_input":"2025-01-15T13:51:10.653636Z","iopub.status.idle":"2025-01-15T13:51:10.733225Z","shell.execute_reply.started":"2025-01-15T13:51:10.653575Z","shell.execute_reply":"2025-01-15T13:51:10.73205Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 7: Execution\n\nExecute the graph with a user question and print the outputs from each node. The code suppresses warnings related to LangSmith API keys (if not in use) and includes a helper function to convert non-serializable objects into dictionaries for JSON serialization. The example demonstrates querying the graph with a question about prompt engineering and streaming the outputs for display.","metadata":{}},{"cell_type":"code","source":"# Execute the graph with an input\nimport json\nimport warnings\n\n# Suppress LangSmith API key warning (if not using LangSmith)\nwarnings.filterwarnings(\"ignore\", category=UserWarning, message=\"API key must be provided when using hosted LangSmith API\")\n\n# Helper function to convert non-serializable objects to dictionaries\ndef convert_to_serializable(obj):\n    if hasattr(obj, \"dict\"):  # Check if the object has a .dict() method\n        return obj.dict()\n    elif isinstance(obj, (list, tuple)):  # Handle lists and tuples\n        return [convert_to_serializable(item) for item in obj]\n    elif isinstance(obj, dict):  # Handle dictionaries\n        return {key: convert_to_serializable(value) for key, value in obj.items()}\n    else:  # Return the object as-is if it's already serializable\n        return obj\n\n# Example 1: Question about Prompt Engineering\ninputs = {\n    \"messages\": [\n        (\"user\", \"What does Lilian Weng say about the types of agent memory?\"),\n    ]\n}\n\n# Stream the graph execution and print outputs\nfor output in graph.stream(inputs):\n    for key, value in output.items():\n        print(f\"Output from node '{key}':\")\n        print(\"-\"*80)\n        # Convert non-serializable objects to dictionaries\n        serializable_value = convert_to_serializable(value)\n        # Print the serialized value as a JSON string\n        print(json.dumps(serializable_value, indent=2))\n    print(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T13:51:10.734486Z","iopub.execute_input":"2025-01-15T13:51:10.734967Z","iopub.status.idle":"2025-01-15T13:51:18.454219Z","shell.execute_reply.started":"2025-01-15T13:51:10.734925Z","shell.execute_reply":"2025-01-15T13:51:18.452953Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example 2: Question about Adversarial Attacks on LLMs\ninputs = {\n    \"messages\": [\n        (\"user\", \"How can adversarial attacks be mitigated in large language models?\"),\n    ]\n}\n\n# Stream the graph execution and print outputs\nfor output in graph.stream(inputs):\n    for key, value in output.items():\n        print(f\"Output from node '{key}':\")\n        print(\"-\"*80)\n        # Convert non-serializable objects to dictionaries\n        serializable_value = convert_to_serializable(value)\n        # Print the serialized value as a JSON string\n        print(json.dumps(serializable_value, indent=2))\n    print(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T13:51:18.455196Z","iopub.execute_input":"2025-01-15T13:51:18.45548Z","iopub.status.idle":"2025-01-15T13:51:25.623762Z","shell.execute_reply.started":"2025-01-15T13:51:18.455454Z","shell.execute_reply":"2025-01-15T13:51:25.622272Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example 3: Question about Agent Planning\ninputs = {\n    \"messages\": [\n        (\"user\", \"What are the main components of agent planning as discussed by Lilian Weng?\"),\n    ]\n}\n\n# Stream the graph execution and print outputs\nfor output in graph.stream(inputs):\n    for key, value in output.items():\n        print(f\"Output from node '{key}':\")\n        print(\"-\"*80)\n        # Convert non-serializable objects to dictionaries\n        serializable_value = convert_to_serializable(value)\n        # Print the serialized value as a JSON string\n        print(json.dumps(serializable_value, indent=2))\n    print(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T13:51:25.624495Z","iopub.execute_input":"2025-01-15T13:51:25.624797Z","iopub.status.idle":"2025-01-15T13:51:29.610621Z","shell.execute_reply.started":"2025-01-15T13:51:25.624769Z","shell.execute_reply":"2025-01-15T13:51:29.609422Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Conclusion\n\nAgentic RAG with LangGraph provides a flexible and modular framework for building retrieval-augmented systems. By incorporating decision-making capabilities into the workflow, the system can dynamically adapt to user queries, ensuring that responses are both accurate and contextually relevant. This approach is particularly useful for applications like question-answering, where the ability to retrieve and process external information is critical. The modular design of LangGraph makes it easy to extend and customize the workflow for specific use cases, making it a valuable tool for building advanced AI systems.","metadata":{}}]}