{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c52a5e6a-cfd4-4979-8e5d-eccb69b46208",
    "_uuid": "75f02a6b-7778-4ce4-a3ca-4230f3c5acf7",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "# LangChain 中的消息设计：实用示例和用例\n",
    "\n",
    "## 介绍\n",
    "\n",
    "消息是 LangChain 中通信的支柱，能够与聊天模型进行无缝交互。无论您是处理基于文本的输入还是多模态输入，LangChain 统一的消息格式都确保了不同聊天模型提供商之间的兼容性。本指南探讨了在 LangChain 中使用消息的实用示例，从基本消息创建到高级功能，如修剪、过滤和合并消息。最后，您将对如何利用消息构建健壮的对话式 AI 应用程序有扎实的理解。\n",
    "\n",
    "### 消息的属性\n",
    "\n",
    "消息是聊天模型中通信的基本单元。它们表示聊天模型的输入和输出，以及与对话相关的任何其他上下文或元数据。LangChain 中的每条消息都具有：\n",
    "\n",
    "1. **角色**: 指定消息的发送者（例如，“用户”、“助手”、“系统”）。\n",
    "2. **内容**: 消息的实际内容，可以是文本或多模态数据（例如，图像、音频）。\n",
    "3. **其他元数据**: 可选信息，如消息 ID、名称、令牌使用情况或特定于模型的元数据。\n",
    "\n",
    "LangChain 的消息格式旨在在不同的聊天模型中保持一致，从而更容易在提供商之间切换或将多个模型集成到单个应用程序中。\n",
    "\n",
    "### 消息的关键组件\n",
    "\n",
    "#### 1. **角色**\n",
    "消息的角色定义了谁在发送它，并帮助聊天模型理解如何响应。主要角色是：\n",
    "\n",
    "- **系统**: 向模型提供指令或上下文（例如，“你是一个乐于助人的助手。”）。\n",
    "- **用户**: 表示与模型交互的用户的输入。\n",
    "- **助手**: 表示模型的响应，可以包括文本或调用工具的请求。\n",
    "- **工具**: 用于将工具调用的结果传递回模型。\n",
    "\n",
    "#### 2. **内容**\n",
    "消息的内容可以是：\n",
    "- **文本**: 最常见的内容类型。\n",
    "- **多模态数据**: 表示图像、音频或视频的字典列表（某些模型支持）。\n",
    "\n",
    "#### 3. **其他元数据**\n",
    "消息可以包含可选元数据，例如：\n",
    "- **ID**: 消息的唯一标识符。\n",
    "- **名称**: 用于区分具有相同角色的实体的名称属性。\n",
    "- **令牌使用情况**: 关于消息的令牌计数的信息。\n",
    "- **工具调用**: 模型发出的调用外部工具的请求。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "aec3feee-1e38-4ae0-8882-9cd97df301fd",
    "_uuid": "cd8568ff-6ddd-4443-98dd-33ff34556d5a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-01-18T11:43:26.329713Z",
     "iopub.status.busy": "2025-01-18T11:43:26.329419Z",
     "iopub.status.idle": "2025-01-18T11:44:04.141644Z",
     "shell.execute_reply": "2025-01-18T11:44:04.140342Z",
     "shell.execute_reply.started": "2025-01-18T11:43:26.329688Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -qU langchain-openai\n",
    "!pip install -qU langchain-anthropic\n",
    "!pip install -qU langchain_community\n",
    "!pip install -qU langchain_experimental"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "69e07e86-d498-4c07-a8a1-761e16d7a823",
    "_uuid": "d0452507-141c-4fb2-8ba7-3dc093229015",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "1. **检索 API 密钥**: \n",
    "   - `UserSecretsClient` 用于安全地检索存储在 Kaggle 环境中的 API 密钥。这确保了敏感凭据不会在代码中暴露。\n",
    "   - `get_secret` 方法使用密钥名称（例如，`\"my-anthropic-api-key\"` 或 `\"my-openai-api-key\"`）获取 API 密钥。\n",
    "\n",
    "2. **初始化 LLM**:\n",
    "   - **Anthropic**: `ChatAnthropic` 类用于初始化 Claude 模型（例如，`claude-3-5-sonnet-latest`）。 `temperature` 参数控制模型响应的随机性。\n",
    "   - **OpenAI**: `ChatOpenAI` 类用于初始化 GPT 模型（例如，`gpt-4o-mini`）。 `temperature` 参数设置为 `0` 以获得确定性响应。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a78099dc-dd78-4007-b733-24ef1d98143a",
    "_uuid": "ce286642-6fdf-44c2-af6f-19527e64fdd7",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "# from langchain_anthropic import ChatAnthropic\n",
    "# from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "# Retrieve LLM API Key\n",
    "# user_secrets = UserSecretsClient()\n",
    "\n",
    "# Initialize LLM (Anthropic, OpenAI)\n",
    "#model = ChatAnthropic(temperature=0, model=\"claude-3-5-sonnet-latest\", api_key=user_secrets.get_secret(\"my-anthropic-api-key\"))\n",
    "model = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\", api_key=user_secrets.get_secret(\"my-openai-api-key\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79153ef3-2210-4f27-bfec-ac64d46a7912",
    "_uuid": "5fa5b5af-acfc-4dc9-8534-0af8c5680c5c",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b7a86e9c-612c-4ad6-aa6b-d9585c423883",
    "_uuid": "9cc032bf-e168-40a8-bfe0-b678417d50c4",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "---\n",
    "\n",
    "## **1. 基本消息创建和使用**\n",
    "\n",
    "### 1.1: 创建和发送消息\n",
    "此示例演示如何在 LangChain 中创建和发送消息。 消息使用特定角色（`system`、`human`、`ai`）创建，并组合成对话。 然后打印对话以显示每条消息的角色和内容。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a34afd7c-76f9-42fd-9557-9dbf3022376e",
    "_uuid": "2b8fe9bf-6883-43d6-ba7d-3c07aaf3c488",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-01-18T12:13:35.725463Z",
     "iopub.status.busy": "2025-01-18T12:13:35.724999Z",
     "iopub.status.idle": "2025-01-18T12:13:35.734137Z",
     "shell.execute_reply": "2025-01-18T12:13:35.732683Z",
     "shell.execute_reply.started": "2025-01-18T12:13:35.725416Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "# Create a system message to set the context\n",
    "system_message = SystemMessage(content=\"You are a helpful assistant.\")\n",
    "\n",
    "# Create a user message\n",
    "user_message = HumanMessage(content=\"What is the capital of France?\")\n",
    "\n",
    "# Create an assistant message (model's response)\n",
    "assistant_message = AIMessage(content=\"The capital of France is Paris.\")\n",
    "\n",
    "# Combine messages into a conversation\n",
    "conversation = [system_message, user_message, assistant_message]\n",
    "\n",
    "# Print the conversation\n",
    "for msg in conversation:\n",
    "    print(f\"{msg.type}: {msg.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3ed168fb-28ec-4605-a7b7-92ebf04d01e7",
    "_uuid": "0bb2af03-3e3d-4eee-8d92-6b99196269cc",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "### 1.2: 自动将字符串转换为 HumanMessage\n",
    "调用聊天模型时，LangChain 会自动将字符串转换为 `HumanMessage`。 这简化了向模型发送用户输入的过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ee6b4f6f-37c7-407d-b2e9-80275ff3ac76",
    "_uuid": "bb4a8cac-cb95-4606-87f7-f5a6043a1edb",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-01-18T11:52:17.806168Z",
     "iopub.status.busy": "2025-01-18T11:52:17.805799Z",
     "iopub.status.idle": "2025-01-18T11:52:18.957058Z",
     "shell.execute_reply": "2025-01-18T11:52:18.95569Z",
     "shell.execute_reply.started": "2025-01-18T11:52:17.806139Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize the chat model\n",
    "# model = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\", api_key=user_secrets.get_secret(\"my-openai-api-key\"))\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, base_url=\"请输入地址\",\n",
    "                        openai_api_key=\"请输入密钥\")\n",
    "\n",
    "# Invoke the model with a string (automatically converted to HumanMessage)\n",
    "response = model.invoke(\"Tell me a joke about programming.\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f3b51be1-cfe6-4e46-ad2c-0e585cc237d5",
    "_uuid": "e3053574-05d3-4f4b-a104-9d49998de686",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "---\n",
    "\n",
    "## **2. 使用多模态消息**\n",
    "\n",
    "### 2.1: 发送多模态输入（文本 + 图像）\n",
    "某些模型支持多模态输入，例如文本和图像。 此示例显示如何创建多模态消息并将其发送到模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fae6c850-33ac-470b-9e05-7d32614e1ed8",
    "_uuid": "b2006384-7061-44d1-aeff-f3f37975a608",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-01-18T11:57:44.791039Z",
     "iopub.status.busy": "2025-01-18T11:57:44.790604Z",
     "iopub.status.idle": "2025-01-18T11:57:47.589808Z",
     "shell.execute_reply": "2025-01-18T11:57:47.588547Z",
     "shell.execute_reply.started": "2025-01-18T11:57:44.79101Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize the chat model\n",
    "# model = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\", api_key=user_secrets.get_secret(\"my-openai-api-key\"))\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, base_url=\"请输入地址\",\n",
    "                        openai_api_key=\"请输入密钥\")\n",
    "\n",
    "# Create a multimodal message with text and an image URL\n",
    "multimodal_message = HumanMessage(\n",
    "    content=[\n",
    "        {\"type\": \"text\", \"text\": \"What is in this image?\"},\n",
    "        {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://www.telegraph.co.uk/content/dam/news/2016/03/31/starling_trans_NvBQzQNjv4BqpJliwavx4coWFCaEkEsb3kvxIt-lGGWCWqwLa_RXJU8.jpg?imwidth=960\"}},\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Send the message to a multimodal model\n",
    "response = model.invoke([multimodal_message])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a7870a09-6a3b-4a9a-b362-c75daeb5f12a",
    "_uuid": "6a347708-8c89-423f-8bf3-4fd5ffd34c0d",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "### 2.2: 处理多模态输出\n",
    "如果模型返回多模态内容（例如，文本 + 音频），您可以按如下方式处理它："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f5a6be22-fc9e-4bc0-9b86-4483ff687b5b",
    "_uuid": "18b6596e-5fd6-4cc7-a4ac-35ac65513cbe",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-01-18T12:03:38.594629Z",
     "iopub.status.busy": "2025-01-18T12:03:38.594273Z",
     "iopub.status.idle": "2025-01-18T12:03:38.603726Z",
     "shell.execute_reply": "2025-01-18T12:03:38.601976Z",
     "shell.execute_reply.started": "2025-01-18T12:03:38.594604Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "# Simulate a multimodal response from the model\n",
    "multimodal_response = AIMessage(\n",
    "    content=[\n",
    "        {\"type\": \"text\", \"text\": \"Here is your answer.\"},\n",
    "        {\"type\": \"audio_url\", \"audio_url\": \"https://example.com/audio.mp3\"},\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Extract and process the content\n",
    "for content_block in multimodal_response.content:\n",
    "    if content_block[\"type\"] == \"text\":\n",
    "        print(\"Text:\", content_block[\"text\"])\n",
    "    elif content_block[\"type\"] == \"audio_url\":\n",
    "        print(\"Audio URL:\", content_block[\"audio_url\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "42adae32-0bba-4e1d-b8bd-bdd570cba611",
    "_uuid": "0517942f-24d6-4e52-afbd-080b675229d6",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "---\n",
    "\n",
    "## **3. 管理聊天记录**\n",
    "\n",
    "### 3.1: 根据令牌计数修剪聊天记录\n",
    "当聊天记录过长时，您可以修剪它以适应模型的令牌限制。 此示例显示如何在保留对话结构的同时修剪聊天记录。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9d6bb5e2-4cb0-4116-9a76-15eae7647d2e",
    "_uuid": "6330549c-f6f9-4810-8c7b-58c56fe31a39",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-01-18T11:59:22.538383Z",
     "iopub.status.busy": "2025-01-18T11:59:22.53804Z",
     "iopub.status.idle": "2025-01-18T11:59:22.546953Z",
     "shell.execute_reply": "2025-01-18T11:59:22.545848Z",
     "shell.execute_reply.started": "2025-01-18T11:59:22.538357Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, trim_messages\n",
    "\n",
    "# Example chat history\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"What is the capital of France?\"),\n",
    "    AIMessage(content=\"The capital of France is Paris.\"),\n",
    "    HumanMessage(content=\"What is the population of France?\"),\n",
    "    AIMessage(content=\"The population of France is approximately 67 million.\"),\n",
    "]\n",
    "\n",
    "# Trim the chat history to the last 50 tokens\n",
    "trimmed_messages = trim_messages(messages, token_counter=model, max_tokens=50, strategy=\"last\", include_system=True)\n",
    "\n",
    "# Print the trimmed messages\n",
    "for msg in trimmed_messages:\n",
    "    print(f\"{msg.type}: {msg.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cb2c8bdf-ce32-4ecc-963e-c41550f585a8",
    "_uuid": "40fd0aea-16af-45bf-a081-2afca97c99c4",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "### 3.2: 按类型过滤消息\n",
    "您可以过滤消息以仅包括特定类型（例如，仅用户消息）。 此示例演示如何按类型过滤消息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "323948d8-16f1-45cc-846a-24a5dae63f03",
    "_uuid": "43850ea3-5c04-4249-94d0-be342f2c9542",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-01-18T12:03:55.497156Z",
     "iopub.status.busy": "2025-01-18T12:03:55.496732Z",
     "iopub.status.idle": "2025-01-18T12:03:55.503741Z",
     "shell.execute_reply": "2025-01-18T12:03:55.502469Z",
     "shell.execute_reply.started": "2025-01-18T12:03:55.497129Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import filter_messages\n",
    "\n",
    "# Filter to include only HumanMessage (user messages)\n",
    "filtered_messages = filter_messages(messages, include_types=\"human\")\n",
    "\n",
    "# Print the filtered messages\n",
    "for msg in filtered_messages:\n",
    "    print(f\"{msg.type}: {msg.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3bf7fdcd-650d-4225-8d51-23544d8e0c87",
    "_uuid": "5ddcc702-55dd-4743-bd5e-73429085538d",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "---\n",
    "\n",
    "## **4. 高级功能**\n",
    "\n",
    "### 4.1: 合并连续消息\n",
    "某些模型不支持相同类型的连续消息。 使用 `merge_message_runs` 将它们合并为一条消息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f685f85a-0b15-4e80-bd3b-db70f44686b4",
    "_uuid": "0caad2f0-8849-49c2-8d1c-3599b7133e85",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-01-18T12:04:03.531373Z",
     "iopub.status.busy": "2025-01-18T12:04:03.531006Z",
     "iopub.status.idle": "2025-01-18T12:04:03.538403Z",
     "shell.execute_reply": "2025-01-18T12:04:03.536975Z",
     "shell.execute_reply.started": "2025-01-18T12:04:03.531345Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import merge_message_runs\n",
    "\n",
    "# Example messages with consecutive HumanMessages\n",
    "messages = [\n",
    "    HumanMessage(content=\"What is the capital of France?\"),\n",
    "    HumanMessage(content=\"And what is the population?\"),\n",
    "]\n",
    "\n",
    "# Merge consecutive messages\n",
    "merged_messages = merge_message_runs(messages)\n",
    "\n",
    "# Print the merged messages\n",
    "for msg in merged_messages:\n",
    "    print(f\"{msg.type}: {msg.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cf5c3496-d7b2-456f-9194-adaa0a9cd1ec",
    "_uuid": "72815003-c4fd-4bb1-82c3-139d24805227",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "### 4.2: 流式响应\n",
    "使用 `AIMessageChunk` 实时流式传输来自模型的响应。 此示例显示如何流式传输模型的响应。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a57d82a2-3e37-4b6f-8c54-718e6d8a8051",
    "_uuid": "9e783328-5b10-4894-8a5b-db807e77fdb5",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-01-18T12:05:06.831384Z",
     "iopub.status.busy": "2025-01-18T12:05:06.829948Z",
     "iopub.status.idle": "2025-01-18T12:05:09.112828Z",
     "shell.execute_reply": "2025-01-18T12:05:09.111118Z",
     "shell.execute_reply.started": "2025-01-18T12:05:06.831324Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize the chat model\n",
    "# model = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\", api_key=user_secrets.get_secret(\"my-openai-api-key\"))\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, base_url=\"请输入地址\",\n",
    "                        openai_api_key=\"请输入密钥\")\n",
    "\n",
    "# Stream the model's response\n",
    "for chunk in model.stream([HumanMessage(content=\"Tell me a joke about cats that has at least 5 sentences.\")]):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "091e098a-4d32-4f8f-9b19-f41517cb6b2c",
    "_uuid": "5954da74-502f-4505-b928-4ec54dd493c2",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "---\n",
    "\n",
    "## **5. 工具调用和函数消息**\n",
    "\n",
    "### 5.1: 使用 ToolMessage 进行工具调用\n",
    "当模型请求调用工具时，使用 `ToolMessage` 将结果传递回去。 此示例演示如何处理工具调用和响应。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2bc8076c-c5e4-4568-a98c-fbfbad62a4de",
    "_uuid": "4b8b9993-323e-4595-9e9b-db39fae55adc",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-01-18T12:05:52.90524Z",
     "iopub.status.busy": "2025-01-18T12:05:52.904798Z",
     "iopub.status.idle": "2025-01-18T12:05:53.265609Z",
     "shell.execute_reply": "2025-01-18T12:05:53.264469Z",
     "shell.execute_reply.started": "2025-01-18T12:05:52.905212Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, ToolMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize the chat model\n",
    "# model = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\", api_key=user_secrets.get_secret(\"my-openai-api-key\"))\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, base_url=\"请输入地址\",\n",
    "                        openai_api_key=\"请输入密钥\")\n",
    "\n",
    "# Simulate a tool call request from the model\n",
    "tool_call_message = AIMessage(\n",
    "    content=\"\",\n",
    "    tool_calls=[\n",
    "        {\n",
    "            \"id\": \"call_123\",  # Unique ID for the tool call\n",
    "            \"name\": \"get_weather\",  # Name of the tool\n",
    "            \"args\": {\"location\": \"Paris\"},  # Arguments for the tool\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Simulate the tool's response\n",
    "tool_response = ToolMessage(\n",
    "    content=\"The weather in Paris is sunny.\",\n",
    "    tool_call_id=tool_call_message.tool_calls[0][\"id\"],  # Use the same ID as the tool call\n",
    ")\n",
    "\n",
    "# Pass the tool response back to the model\n",
    "response = model.invoke([tool_call_message, tool_response])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a5191d06-c4b4-4a38-95cb-03a91bd04e13",
    "_uuid": "ceca0c0c-41a0-4d2f-94ec-5320af8d902d",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "### 5.2: 传统 FunctionMessage (OpenAI)\n",
    "对于 OpenAI 的传统函数调用 API，请使用 `FunctionMessage`。 此示例显示如何处理传统函数调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a5e88156-08f8-49b5-a5dc-78bf1f5ebb0e",
    "_uuid": "e7f3dfc6-e965-4754-b12f-b950100f8de5",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-01-18T12:06:06.38231Z",
     "iopub.status.busy": "2025-01-18T12:06:06.381898Z",
     "iopub.status.idle": "2025-01-18T12:06:06.768753Z",
     "shell.execute_reply": "2025-01-18T12:06:06.766992Z",
     "shell.execute_reply.started": "2025-01-18T12:06:06.38228Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import FunctionMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize the chat model\n",
    "# model = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\", api_key=user_secrets.get_secret(\"my-openai-api-key\"))\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, base_url=\"请输入地址\",\n",
    "                        openai_api_key=\"请输入密钥\")\n",
    "\n",
    "# Simulate a function response\n",
    "function_response = FunctionMessage(\n",
    "    name=\"get_weather\",\n",
    "    content='{\"temperature\": 22, \"condition\": \"sunny\"}',\n",
    ")\n",
    "\n",
    "# Pass the function response back to the model\n",
    "response = model.invoke([function_response])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "be353960-06f7-44b4-ad31-4e051223e60c",
    "_uuid": "f104f876-b584-4163-bf2f-ff7eb4e1ca01",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "---\n",
    "\n",
    "## **6. 与聊天模型集成**\n",
    "\n",
    "### 6.1: 将消息与 ChatOpenAI 结合使用\n",
    "以下是如何将消息与 OpenAI 的聊天模型结合使用。 此示例演示如何创建对话并获取模型的响应。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2c788e84-c411-480e-b1f0-f6a3d289cca6",
    "_uuid": "f6011f98-da0d-472e-a23d-e881ffe61828",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-01-18T12:06:22.98498Z",
     "iopub.status.busy": "2025-01-18T12:06:22.984595Z",
     "iopub.status.idle": "2025-01-18T12:06:23.463649Z",
     "shell.execute_reply": "2025-01-18T12:06:23.462368Z",
     "shell.execute_reply.started": "2025-01-18T12:06:22.984951Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize the chat model\n",
    "# model = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\", api_key=user_secrets.get_secret(\"my-openai-api-key\"))\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, base_url=\"请输入地址\",\n",
    "                        openai_api_key=\"请输入密钥\")\n",
    "\n",
    "# Create a conversation\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"What is the capital of Germany?\"),\n",
    "]\n",
    "\n",
    "# Get the model's response\n",
    "response = model.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c7efef4e-c87d-4a5f-bbf1-faed9ee7b216",
    "_uuid": "a5a2e9f4-6917-45b1-a284-7c403b8f36a3",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "### 6.2: 将消息与 Anthropic 的 Claude 结合使用\n",
    "以下是如何将消息与 Anthropic 的 Claude 模型结合使用。 此示例演示如何发送用户消息并获取模型的响应。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1f4a3caa-dab5-41cf-9d37-e45688dea995",
    "_uuid": "379ff7b6-8849-4832-a042-1aa8ef09e655",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-01-18T12:09:26.035579Z",
     "iopub.status.busy": "2025-01-18T12:09:26.035153Z",
     "iopub.status.idle": "2025-01-18T12:09:28.588361Z",
     "shell.execute_reply": "2025-01-18T12:09:28.587051Z",
     "shell.execute_reply.started": "2025-01-18T12:09:26.035549Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Initialize the chat model\n",
    "model = ChatAnthropic(temperature=0, model=\"claude-3-5-sonnet-latest\", api_key=user_secrets.get_secret(\"my-anthropic-api-key\"))\n",
    "\n",
    "# Create a user message\n",
    "user_message = HumanMessage(content=\"Tell me a fun fact about space.\")\n",
    "\n",
    "# Get the model's response\n",
    "response = model.invoke([user_message])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e6b9ccd0-1597-4e34-89cc-e99a5d860ac8",
    "_uuid": "35bae3ca-a2be-4174-bae7-9c16740fae0e",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "---\n",
    "\n",
    "## **7. 将消息与其他组件链接**\n",
    "\n",
    "### 7.1: 将消息与提示链接\n",
    "在链中将消息与提示结合起来。 此示例显示如何使用提示模板和聊天模型创建链。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "915152f6-bbfe-483a-9932-0640dffda92b",
    "_uuid": "fd275c59-6c46-477a-a6e2-492b1c2c94b0",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-01-18T12:10:42.480627Z",
     "iopub.status.busy": "2025-01-18T12:10:42.480172Z",
     "iopub.status.idle": "2025-01-18T12:10:43.126807Z",
     "shell.execute_reply": "2025-01-18T12:10:43.125765Z",
     "shell.execute_reply.started": "2025-01-18T12:10:42.480592Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize the chat model\n",
    "model = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\", api_key=user_secrets.get_secret(\"my-openai-api-key\"))\n",
    "\n",
    "# Define a prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"human\", \"{query}\"),\n",
    "])\n",
    "\n",
    "# Create a chain\n",
    "chain = prompt | model\n",
    "\n",
    "# Invoke the chain\n",
    "response = chain.invoke({\"query\": \"What is the capital of Italy?\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "18bfee96-cc6d-46d6-9766-297bed28fdce",
    "_uuid": "11b2c893-4670-41f5-8a06-d6812b2a94bc",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "### 7.2: 与消息修剪链接\n",
    "将消息修剪与聊天模型结合起来。 此示例演示如何在将消息传递给模型之前对其进行修剪。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9cccaf65-861c-44ef-858b-d4490b99594b",
    "_uuid": "bbf64a54-6b2a-4a06-ab9d-dd72f57635c0",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-01-18T12:10:56.193233Z",
     "iopub.status.busy": "2025-01-18T12:10:56.192794Z",
     "iopub.status.idle": "2025-01-18T12:10:56.966864Z",
     "shell.execute_reply": "2025-01-18T12:10:56.965678Z",
     "shell.execute_reply.started": "2025-01-18T12:10:56.193197Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import trim_messages\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize the chat model\n",
    "model = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\", api_key=user_secrets.get_secret(\"my-openai-api-key\"))\n",
    "\n",
    "# Create a trimmer\n",
    "trimmer = trim_messages(\n",
    "    token_counter=model,\n",
    "    max_tokens=50,\n",
    "    strategy=\"last\",\n",
    "    include_system=True,\n",
    ")\n",
    "\n",
    "# Create a chain\n",
    "chain = trimmer | model\n",
    "\n",
    "# Invoke the chain\n",
    "response = chain.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "949d720a-3f28-44cd-9b50-a78fc37d4673",
    "_uuid": "bd516738-d975-4866-b3b8-64cdbbbc97c3",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## 结论\n",
    "\n",
    "LangChain 的消息设计为使用聊天模型提供了一个强大而灵活的框架。 从创建简单的对话到处理复杂的多模态输入和管理聊天记录，本指南中的示例演示了 LangChain 消息系统的多功能性。 通过掌握这些技术，您可以构建更高效、更有效的对话式 AI 应用程序，从而确保跨不同模型和用例的兼容性。"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
