{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# LangChain 中的消息设计：实用示例和用例\n\n## 介绍\n\n消息是 LangChain 中通信的支柱，能够与聊天模型进行无缝交互。无论您是处理基于文本的输入还是多模态输入，LangChain 统一的消息格式都确保了不同聊天模型提供商之间的兼容性。本指南探讨了在 LangChain 中使用消息的实用示例，从基本消息创建到高级功能，如修剪、过滤和合并消息。最后，您将对如何利用消息构建健壮的对话式 AI 应用程序有扎实的理解。\n\n### 消息的属性\n\n消息是聊天模型中通信的基本单元。它们表示聊天模型的输入和输出，以及与对话相关的任何其他上下文或元数据。LangChain 中的每条消息都具有：\n\n1. **角色**: 指定消息的发送者（例如，“用户”、“助手”、“系统”）。\n2. **内容**: 消息的实际内容，可以是文本或多模态数据（例如，图像、音频）。\n3. **其他元数据**: 可选信息，如消息 ID、名称、令牌使用情况或特定于模型的元数据。\n\nLangChain 的消息格式旨在在不同的聊天模型中保持一致，从而更容易在提供商之间切换或将多个模型集成到单个应用程序中。\n\n### 消息的关键组件\n\n#### 1. **角色**\n消息的角色定义了谁在发送它，并帮助聊天模型理解如何响应。主要角色是：\n\n- **系统**: 向模型提供指令或上下文（例如，“你是一个乐于助人的助手。”）。\n- **用户**: 表示与模型交互的用户的输入。\n- **助手**: 表示模型的响应，可以包括文本或调用工具的请求。\n- **工具**: 用于将工具调用的结果传递回模型。\n\n#### 2. **内容**\n消息的内容可以是：\n- **文本**: 最常见的内容类型。\n- **多模态数据**: 表示图像、音频或视频的字典列表（某些模型支持）。\n\n#### 3. **其他元数据**\n消息可以包含可选元数据，例如：\n- **ID**: 消息的唯一标识符。\n- **名称**: 用于区分具有相同角色的实体的名称属性。\n- **令牌使用情况**: 关于消息的令牌计数的信息。\n- **工具调用**: 模型发出的调用外部工具的请求。","metadata":{"_uuid":"75f02a6b-7778-4ce4-a3ca-4230f3c5acf7","_cell_guid":"c52a5e6a-cfd4-4979-8e5d-eccb69b46208","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"!pip install -qU langchain-openai\n!pip install -qU langchain-anthropic\n!pip install -qU langchain_community\n!pip install -qU langchain_experimental","metadata":{"_uuid":"cd8568ff-6ddd-4443-98dd-33ff34556d5a","_cell_guid":"aec3feee-1e38-4ae0-8882-9cd97df301fd","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-01-18T11:43:26.329419Z","iopub.execute_input":"2025-01-18T11:43:26.329713Z","iopub.status.idle":"2025-01-18T11:44:04.141644Z","shell.execute_reply.started":"2025-01-18T11:43:26.329688Z","shell.execute_reply":"2025-01-18T11:44:04.140342Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"1. **检索 API 密钥**: \n   - `UserSecretsClient` 用于安全地检索存储在 Kaggle 环境中的 API 密钥。这确保了敏感凭据不会在代码中暴露。\n   - `get_secret` 方法使用密钥名称（例如，`\"my-anthropic-api-key\"` 或 `\"my-openai-api-key\"`）获取 API 密钥。\n\n2. **初始化 LLM**:\n   - **Anthropic**: `ChatAnthropic` 类用于初始化 Claude 模型（例如，`claude-3-5-sonnet-latest`）。 `temperature` 参数控制模型响应的随机性。\n   - **OpenAI**: `ChatOpenAI` 类用于初始化 GPT 模型（例如，`gpt-4o-mini`）。 `temperature` 参数设置为 `0` 以获得确定性响应。","metadata":{"_uuid":"d0452507-141c-4fb2-8ba7-3dc093229015","_cell_guid":"69e07e86-d498-4c07-a8a1-761e16d7a823","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from langchain_openai import ChatOpenAI\n# from langchain_anthropic import ChatAnthropic\n# from kaggle_secrets import UserSecretsClient\n\n# Retrieve LLM API Key\n# user_secrets = UserSecretsClient()\n\n# Initialize LLM (Anthropic, OpenAI)\n#model = ChatAnthropic(temperature=0, model=\"claude-3-5-sonnet-latest\", api_key=user_secrets.get_secret(\"my-anthropic-api-key\"))\nmodel = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\", api_key=user_secrets.get_secret(\"my-openai-api-key\"))","metadata":{"_uuid":"ce286642-6fdf-44c2-af6f-19527e64fdd7","_cell_guid":"a78099dc-dd78-4007-b733-24ef1d98143a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"_uuid":"5fa5b5af-acfc-4dc9-8534-0af8c5680c5c","_cell_guid":"79153ef3-2210-4f27-bfec-ac64d46a7912","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n## **1. 基本消息创建和使用**\n\n### 1.1: 创建和发送消息\n此示例演示如何在 LangChain 中创建和发送消息。 消息使用特定角色（`system`、`human`、`ai`）创建，并组合成对话。 然后打印对话以显示每条消息的角色和内容。","metadata":{"_uuid":"9cc032bf-e168-40a8-bfe0-b678417d50c4","_cell_guid":"b7a86e9c-612c-4ad6-aa6b-d9585c423883","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n\n# Create a system message to set the context\nsystem_message = SystemMessage(content=\"You are a helpful assistant.\")\n\n# Create a user message\nuser_message = HumanMessage(content=\"What is the capital of France?\")\n\n# Create an assistant message (model's response)\nassistant_message = AIMessage(content=\"The capital of France is Paris.\")\n\n# Combine messages into a conversation\nconversation = [system_message, user_message, assistant_message]\n\n# Print the conversation\nfor msg in conversation:\n    print(f\"{msg.type}: {msg.content}\")","metadata":{"_uuid":"2b8fe9bf-6883-43d6-ba7d-3c07aaf3c488","_cell_guid":"a34afd7c-76f9-42fd-9557-9dbf3022376e","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-01-18T12:13:35.724999Z","iopub.execute_input":"2025-01-18T12:13:35.725463Z","iopub.status.idle":"2025-01-18T12:13:35.734137Z","shell.execute_reply.started":"2025-01-18T12:13:35.725416Z","shell.execute_reply":"2025-01-18T12:13:35.732683Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 1.2: 自动将字符串转换为 HumanMessage\n调用聊天模型时，LangChain 会自动将字符串转换为 `HumanMessage`。 这简化了向模型发送用户输入的过程。","metadata":{"_uuid":"0bb2af03-3e3d-4eee-8d92-6b99196269cc","_cell_guid":"3ed168fb-28ec-4605-a7b7-92ebf04d01e7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from langchain_openai import ChatOpenAI\n\n# Initialize the chat model\n# model = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\", api_key=user_secrets.get_secret(\"my-openai-api-key\"))\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, base_url=\"http://20.243.34.136:2999/v1\",\n                        openai_api_key=\"sk-j8r3Pxztstd3wBjF8fEe44E63f69486bAdC2C4562bD1E1F3\")\n\n# Invoke the model with a string (automatically converted to HumanMessage)\nresponse = model.invoke(\"Tell me a joke about programming.\")\nprint(response.content)","metadata":{"_uuid":"bb4a8cac-cb95-4606-87f7-f5a6043a1edb","_cell_guid":"ee6b4f6f-37c7-407d-b2e9-80275ff3ac76","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-01-18T11:52:17.805799Z","iopub.execute_input":"2025-01-18T11:52:17.806168Z","iopub.status.idle":"2025-01-18T11:52:18.957058Z","shell.execute_reply.started":"2025-01-18T11:52:17.806139Z","shell.execute_reply":"2025-01-18T11:52:18.95569Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n## **2. 使用多模态消息**\n\n### 2.1: 发送多模态输入（文本 + 图像）\n某些模型支持多模态输入，例如文本和图像。 此示例显示如何创建多模态消息并将其发送到模型。","metadata":{"_uuid":"e3053574-05d3-4f4b-a104-9d49998de686","_cell_guid":"f3b51be1-cfe6-4e46-ad2c-0e585cc237d5","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from langchain_core.messages import HumanMessage\nfrom langchain_openai import ChatOpenAI\n\n# Initialize the chat model\n# model = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\", api_key=user_secrets.get_secret(\"my-openai-api-key\"))\nmodel = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, base_url=\"http://20.243.34.136:2999/v1\",\n                        openai_api_key=\"sk-j8r3Pxztstd3wBjF8fEe44E63f69486bAdC2C4562bD1E1F3\")\n\n# Create a multimodal message with text and an image URL\nmultimodal_message = HumanMessage(\n    content=[\n        {\"type\": \"text\", \"text\": \"What is in this image?\"},\n        {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://www.telegraph.co.uk/content/dam/news/2016/03/31/starling_trans_NvBQzQNjv4BqpJliwavx4coWFCaEkEsb3kvxIt-lGGWCWqwLa_RXJU8.jpg?imwidth=960\"}},\n    ]\n)\n\n# Send the message to a multimodal model\nresponse = model.invoke([multimodal_message])\nprint(response.content)","metadata":{"_uuid":"b2006384-7061-44d1-aeff-f3f37975a608","_cell_guid":"fae6c850-33ac-470b-9e05-7d32614e1ed8","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-01-18T11:57:44.790604Z","iopub.execute_input":"2025-01-18T11:57:44.791039Z","iopub.status.idle":"2025-01-18T11:57:47.589808Z","shell.execute_reply.started":"2025-01-18T11:57:44.79101Z","shell.execute_reply":"2025-01-18T11:57:47.588547Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.2: 处理多模态输出\n如果模型返回多模态内容（例如，文本 + 音频），您可以按如下方式处理它：","metadata":{"_uuid":"6a347708-8c89-423f-8bf3-4fd5ffd34c0d","_cell_guid":"a7870a09-6a3b-4a9a-b362-c75daeb5f12a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from langchain_core.messages import AIMessage\n\n# Simulate a multimodal response from the model\nmultimodal_response = AIMessage(\n    content=[\n        {\"type\": \"text\", \"text\": \"Here is your answer.\"},\n        {\"type\": \"audio_url\", \"audio_url\": \"https://example.com/audio.mp3\"},\n    ]\n)\n\n# Extract and process the content\nfor content_block in multimodal_response.content:\n    if content_block[\"type\"] == \"text\":\n        print(\"Text:\", content_block[\"text\"])\n    elif content_block[\"type\"] == \"audio_url\":\n        print(\"Audio URL:\", content_block[\"audio_url\"])","metadata":{"_uuid":"18b6596e-5fd6-4cc7-a4ac-35ac65513cbe","_cell_guid":"f5a6be22-fc9e-4bc0-9b86-4483ff687b5b","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-01-18T12:03:38.594273Z","iopub.execute_input":"2025-01-18T12:03:38.594629Z","iopub.status.idle":"2025-01-18T12:03:38.603726Z","shell.execute_reply.started":"2025-01-18T12:03:38.594604Z","shell.execute_reply":"2025-01-18T12:03:38.601976Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n## **3. 管理聊天记录**\n\n### 3.1: 根据令牌计数修剪聊天记录\n当聊天记录过长时，您可以修剪它以适应模型的令牌限制。 此示例显示如何在保留对话结构的同时修剪聊天记录。","metadata":{"_uuid":"0517942f-24d6-4e52-afbd-080b675229d6","_cell_guid":"42adae32-0bba-4e1d-b8bd-bdd570cba611","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, trim_messages\n\n# Example chat history\nmessages = [\n    SystemMessage(content=\"You are a helpful assistant.\"),\n    HumanMessage(content=\"What is the capital of France?\"),\n    AIMessage(content=\"The capital of France is Paris.\"),\n    HumanMessage(content=\"What is the population of France?\"),\n    AIMessage(content=\"The population of France is approximately 67 million.\"),\n]\n\n# Trim the chat history to the last 50 tokens\ntrimmed_messages = trim_messages(messages, token_counter=model, max_tokens=50, strategy=\"last\", include_system=True)\n\n# Print the trimmed messages\nfor msg in trimmed_messages:\n    print(f\"{msg.type}: {msg.content}\")","metadata":{"_uuid":"6330549c-f6f9-4810-8c7b-58c56fe31a39","_cell_guid":"9d6bb5e2-4cb0-4116-9a76-15eae7647d2e","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-01-18T11:59:22.53804Z","iopub.execute_input":"2025-01-18T11:59:22.538383Z","iopub.status.idle":"2025-01-18T11:59:22.546953Z","shell.execute_reply.started":"2025-01-18T11:59:22.538357Z","shell.execute_reply":"2025-01-18T11:59:22.545848Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3.2: 按类型过滤消息\n您可以过滤消息以仅包括特定类型（例如，仅用户消息）。 此示例演示如何按类型过滤消息。","metadata":{"_uuid":"40fd0aea-16af-45bf-a081-2afca97c99c4","_cell_guid":"cb2c8bdf-ce32-4ecc-963e-c41550f585a8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from langchain_core.messages import filter_messages\n\n# Filter to include only HumanMessage (user messages)\nfiltered_messages = filter_messages(messages, include_types=\"human\")\n\n# Print the filtered messages\nfor msg in filtered_messages:\n    print(f\"{msg.type}: {msg.content}\")","metadata":{"_uuid":"43850ea3-5c04-4249-94d0-be342f2c9542","_cell_guid":"323948d8-16f1-45cc-846a-24a5dae63f03","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-01-18T12:03:55.496732Z","iopub.execute_input":"2025-01-18T12:03:55.497156Z","iopub.status.idle":"2025-01-18T12:03:55.503741Z","shell.execute_reply.started":"2025-01-18T12:03:55.497129Z","shell.execute_reply":"2025-01-18T12:03:55.502469Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n## **4. 高级功能**\n\n### 4.1: 合并连续消息\n某些模型不支持相同类型的连续消息。 使用 `merge_message_runs` 将它们合并为一条消息。","metadata":{"_uuid":"5ddcc702-55dd-4743-bd5e-73429085538d","_cell_guid":"3bf7fdcd-650d-4225-8d51-23544d8e0c87","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from langchain_core.messages import merge_message_runs\n\n# Example messages with consecutive HumanMessages\nmessages = [\n    HumanMessage(content=\"What is the capital of France?\"),\n    HumanMessage(content=\"And what is the population?\"),\n]\n\n# Merge consecutive messages\nmerged_messages = merge_message_runs(messages)\n\n# Print the merged messages\nfor msg in merged_messages:\n    print(f\"{msg.type}: {msg.content}\")","metadata":{"_uuid":"0caad2f0-8849-49c2-8d1c-3599b7133e85","_cell_guid":"f685f85a-0b15-4e80-bd3b-db70f44686b4","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-01-18T12:04:03.531006Z","iopub.execute_input":"2025-01-18T12:04:03.531373Z","iopub.status.idle":"2025-01-18T12:04:03.538403Z","shell.execute_reply.started":"2025-01-18T12:04:03.531345Z","shell.execute_reply":"2025-01-18T12:04:03.536975Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4.2: 流式响应\n使用 `AIMessageChunk` 实时流式传输来自模型的响应。 此示例显示如何流式传输模型的响应。","metadata":{"_uuid":"72815003-c4fd-4bb1-82c3-139d24805227","_cell_guid":"cf5c3496-d7b2-456f-9194-adaa0a9cd1ec","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from langchain_core.messages import HumanMessage\nfrom langchain_openai import ChatOpenAI\n\n# Initialize the chat model\n# model = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\", api_key=user_secrets.get_secret(\"my-openai-api-key\"))\nmodel = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, base_url=\"http://20.243.34.136:2999/v1\",\n                        openai_api_key=\"sk-j8r3Pxztstd3wBjF8fEe44E63f69486bAdC2C4562bD1E1F3\")\n\n# Stream the model's response\nfor chunk in model.stream([HumanMessage(content=\"Tell me a joke about cats that has at least 5 sentences.\")]):\n    print(chunk.content, end=\"\", flush=True)","metadata":{"_uuid":"9e783328-5b10-4894-8a5b-db807e77fdb5","_cell_guid":"a57d82a2-3e37-4b6f-8c54-718e6d8a8051","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-01-18T12:05:06.829948Z","iopub.execute_input":"2025-01-18T12:05:06.831384Z","iopub.status.idle":"2025-01-18T12:05:09.112828Z","shell.execute_reply.started":"2025-01-18T12:05:06.831324Z","shell.execute_reply":"2025-01-18T12:05:09.111118Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n## **5. 工具调用和函数消息**\n\n### 5.1: 使用 ToolMessage 进行工具调用\n当模型请求调用工具时，使用 `ToolMessage` 将结果传递回去。 此示例演示如何处理工具调用和响应。","metadata":{"_uuid":"5954da74-502f-4505-b928-4ec54dd493c2","_cell_guid":"091e098a-4d32-4f8f-9b19-f41517cb6b2c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from langchain_core.messages import AIMessage, ToolMessage\nfrom langchain_openai import ChatOpenAI\n\n# Initialize the chat model\n# model = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\", api_key=user_secrets.get_secret(\"my-openai-api-key\"))\nmodel = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, base_url=\"http://20.243.34.136:2999/v1\",\n                        openai_api_key=\"sk-j8r3Pxztstd3wBjF8fEe44E63f69486bAdC2C4562bD1E1F3\")\n\n# Simulate a tool call request from the model\ntool_call_message = AIMessage(\n    content=\"\",\n    tool_calls=[\n        {\n            \"id\": \"call_123\",  # Unique ID for the tool call\n            \"name\": \"get_weather\",  # Name of the tool\n            \"args\": {\"location\": \"Paris\"},  # Arguments for the tool\n        }\n    ],\n)\n\n# Simulate the tool's response\ntool_response = ToolMessage(\n    content=\"The weather in Paris is sunny.\",\n    tool_call_id=tool_call_message.tool_calls[0][\"id\"],  # Use the same ID as the tool call\n)\n\n# Pass the tool response back to the model\nresponse = model.invoke([tool_call_message, tool_response])\nprint(response.content)","metadata":{"_uuid":"4b8b9993-323e-4595-9e9b-db39fae55adc","_cell_guid":"2bc8076c-c5e4-4568-a98c-fbfbad62a4de","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-01-18T12:05:52.904798Z","iopub.execute_input":"2025-01-18T12:05:52.90524Z","iopub.status.idle":"2025-01-18T12:05:53.265609Z","shell.execute_reply.started":"2025-01-18T12:05:52.905212Z","shell.execute_reply":"2025-01-18T12:05:53.264469Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 5.2: 传统 FunctionMessage (OpenAI)\n对于 OpenAI 的传统函数调用 API，请使用 `FunctionMessage`。 此示例显示如何处理传统函数调用。","metadata":{"_uuid":"ceca0c0c-41a0-4d2f-94ec-5320af8d902d","_cell_guid":"a5191d06-c4b4-4a38-95cb-03a91bd04e13","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from langchain_core.messages import FunctionMessage\nfrom langchain_openai import ChatOpenAI\n\n# Initialize the chat model\n# model = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\", api_key=user_secrets.get_secret(\"my-openai-api-key\"))\nmodel = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, base_url=\"http://20.243.34.136:2999/v1\",\n                        openai_api_key=\"sk-j8r3Pxztstd3wBjF8fEe44E63f69486bAdC2C4562bD1E1F3\")\n\n# Simulate a function response\nfunction_response = FunctionMessage(\n    name=\"get_weather\",\n    content='{\"temperature\": 22, \"condition\": \"sunny\"}',\n)\n\n# Pass the function response back to the model\nresponse = model.invoke([function_response])\nprint(response.content)","metadata":{"_uuid":"e7f3dfc6-e965-4754-b12f-b950100f8de5","_cell_guid":"a5e88156-08f8-49b5-a5dc-78bf1f5ebb0e","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-01-18T12:06:06.381898Z","iopub.execute_input":"2025-01-18T12:06:06.38231Z","iopub.status.idle":"2025-01-18T12:06:06.768753Z","shell.execute_reply.started":"2025-01-18T12:06:06.38228Z","shell.execute_reply":"2025-01-18T12:06:06.766992Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n## **6. 与聊天模型集成**\n\n### 6.1: 将消息与 ChatOpenAI 结合使用\n以下是如何将消息与 OpenAI 的聊天模型结合使用。 此示例演示如何创建对话并获取模型的响应。","metadata":{"_uuid":"f104f876-b584-4163-bf2f-ff7eb4e1ca01","_cell_guid":"be353960-06f7-44b4-ad31-4e051223e60c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from langchain_core.messages import HumanMessage, SystemMessage\nfrom langchain_openai import ChatOpenAI\n\n# Initialize the chat model\n# model = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\", api_key=user_secrets.get_secret(\"my-openai-api-key\"))\nmodel = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, base_url=\"http://20.243.34.136:2999/v1\",\n                        openai_api_key=\"sk-j8r3Pxztstd3wBjF8fEe44E63f69486bAdC2C4562bD1E1F3\")\n\n# Create a conversation\nmessages = [\n    SystemMessage(content=\"You are a helpful assistant.\"),\n    HumanMessage(content=\"What is the capital of Germany?\"),\n]\n\n# Get the model's response\nresponse = model.invoke(messages)\nprint(response.content)","metadata":{"_uuid":"f6011f98-da0d-472e-a23d-e881ffe61828","_cell_guid":"2c788e84-c411-480e-b1f0-f6a3d289cca6","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-01-18T12:06:22.984595Z","iopub.execute_input":"2025-01-18T12:06:22.98498Z","iopub.status.idle":"2025-01-18T12:06:23.463649Z","shell.execute_reply.started":"2025-01-18T12:06:22.984951Z","shell.execute_reply":"2025-01-18T12:06:23.462368Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 6.2: 将消息与 Anthropic 的 Claude 结合使用\n以下是如何将消息与 Anthropic 的 Claude 模型结合使用。 此示例演示如何发送用户消息并获取模型的响应。","metadata":{"_uuid":"a5a2e9f4-6917-45b1-a284-7c403b8f36a3","_cell_guid":"c7efef4e-c87d-4a5f-bbf1-faed9ee7b216","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from langchain_anthropic import ChatAnthropic\nfrom langchain_core.messages import HumanMessage\n\n# Initialize the chat model\nmodel = ChatAnthropic(temperature=0, model=\"claude-3-5-sonnet-latest\", api_key=user_secrets.get_secret(\"my-anthropic-api-key\"))\n\n# Create a user message\nuser_message = HumanMessage(content=\"Tell me a fun fact about space.\")\n\n# Get the model's response\nresponse = model.invoke([user_message])\nprint(response.content)","metadata":{"_uuid":"379ff7b6-8849-4832-a042-1aa8ef09e655","_cell_guid":"1f4a3caa-dab5-41cf-9d37-e45688dea995","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-01-18T12:09:26.035153Z","iopub.execute_input":"2025-01-18T12:09:26.035579Z","iopub.status.idle":"2025-01-18T12:09:28.588361Z","shell.execute_reply.started":"2025-01-18T12:09:26.035549Z","shell.execute_reply":"2025-01-18T12:09:28.587051Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n## **7. 将消息与其他组件链接**\n\n### 7.1: 将消息与提示链接\n在链中将消息与提示结合起来。 此示例显示如何使用提示模板和聊天模型创建链。","metadata":{"_uuid":"35bae3ca-a2be-4174-bae7-9c16740fae0e","_cell_guid":"e6b9ccd0-1597-4e34-89cc-e99a5d860ac8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\n\n# Initialize the chat model\nmodel = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\", api_key=user_secrets.get_secret(\"my-openai-api-key\"))\n\n# Define a prompt template\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant.\"),\n    (\"human\", \"{query}\"),\n])\n\n# Create a chain\nchain = prompt | model\n\n# Invoke the chain\nresponse = chain.invoke({\"query\": \"What is the capital of Italy?\"})\nprint(response.content)","metadata":{"_uuid":"fd275c59-6c46-477a-a6e2-492b1c2c94b0","_cell_guid":"915152f6-bbfe-483a-9932-0640dffda92b","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-01-18T12:10:42.480172Z","iopub.execute_input":"2025-01-18T12:10:42.480627Z","iopub.status.idle":"2025-01-18T12:10:43.126807Z","shell.execute_reply.started":"2025-01-18T12:10:42.480592Z","shell.execute_reply":"2025-01-18T12:10:43.125765Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 7.2: 与消息修剪链接\n将消息修剪与聊天模型结合起来。 此示例演示如何在将消息传递给模型之前对其进行修剪。","metadata":{"_uuid":"11b2c893-4670-41f5-8a06-d6812b2a94bc","_cell_guid":"18bfee96-cc6d-46d6-9766-297bed28fdce","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from langchain_core.messages import trim_messages\nfrom langchain_openai import ChatOpenAI\n\n# Initialize the chat model\nmodel = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\", api_key=user_secrets.get_secret(\"my-openai-api-key\"))\n\n# Create a trimmer\ntrimmer = trim_messages(\n    token_counter=model,\n    max_tokens=50,\n    strategy=\"last\",\n    include_system=True,\n)\n\n# Create a chain\nchain = trimmer | model\n\n# Invoke the chain\nresponse = chain.invoke(messages)\nprint(response.content)","metadata":{"_uuid":"bbf64a54-6b2a-4a06-ab9d-dd72f57635c0","_cell_guid":"9cccaf65-861c-44ef-858b-d4490b99594b","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-01-18T12:10:56.192794Z","iopub.execute_input":"2025-01-18T12:10:56.193233Z","iopub.status.idle":"2025-01-18T12:10:56.966864Z","shell.execute_reply.started":"2025-01-18T12:10:56.193197Z","shell.execute_reply":"2025-01-18T12:10:56.965678Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 结论\n\nLangChain 的消息设计为使用聊天模型提供了一个强大而灵活的框架。 从创建简单的对话到处理复杂的多模态输入和管理聊天记录，本指南中的示例演示了 LangChain 消息系统的多功能性。 通过掌握这些技术，您可以构建更高效、更有效的对话式 AI 应用程序，从而确保跨不同模型和用例的兼容性。","metadata":{"_uuid":"bd516738-d975-4866-b3b8-64cdbbbc97c3","_cell_guid":"949d720a-3f28-44cd-9b50-a78fc37d4673","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}}]}