{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# AI Coder Workflow for Custom Library with LangGraph\n\n## Introduction\n\nThis notebook demonstrates how to build an **AI-powered code generation workflow** tailored for your custom library using **LangGraph**, **LangChain**, and **Pydantic**. The workflow is designed to process user queries, generate reliable Python code solutions, and iteratively improve the outputs through error handling and reflection. By leveraging LangGraph, the system connects different stages of code generation into a dynamic and reusable workflow. This makes it especially useful for automating development tasks related to custom libraries, ensuring accurate and executable solutions.\n\nHere’s an outline of the process:\n\n1. **Importing Required Libraries**: Setting up the necessary libraries and dependencies.\n2. **Constants and Configuration**: Defining constants such as model names, maximum iterations, and documentation URLs.\n3. **Loading Custom Library Documentation**: Implementing a function to fetch and consolidate library documentation from a specified URL. This ensures that all necessary information is accessible for the workflow, using a recursive loader to retrieve multiple pages or sections of the documentation.\n4. **Defining Data Models**: Creating data models using Pydantic to structure the workflow's state and code solutions.\n5. **Initializing the Language Model**: Setting up the language model (LLM) for code generation.\n6. **Creating Prompt Templates**: Designing prompts to guide the LLM in generating structured code solutions.\n7. **Defining Workflow Nodes**: Implementing functions that handle code generation, code checking, and reflection on errors.\n8. **Building and Compiling the Workflow**: Assembling the workflow graph and compiling it for execution.\n9. **Executing Example Queries**: Demonstrating the workflow with sample user questions and displaying the generated code solutions.\n10. **Conclusion**: Summarizing the workflow and its capabilities.","metadata":{}},{"cell_type":"markdown","source":"## Installation of Required Packages\n\nFirst, we need to install the necessary packages required for our hierarchical agent system. These packages include various components of LangChain and LangGraph.","metadata":{}},{"cell_type":"code","source":"!pip install -qU langchain-openai\n!pip install -qU langchain-anthropic\n!pip install -qU langchain_community\n!pip install -qU langchain_experimental\n!pip install -qU langgraph","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-15T07:09:42.034493Z","iopub.execute_input":"2025-01-15T07:09:42.034854Z","iopub.status.idle":"2025-01-15T07:10:19.494019Z","shell.execute_reply.started":"2025-01-15T07:09:42.034826Z","shell.execute_reply":"2025-01-15T07:10:19.492701Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Importing Required Libraries\n\nFirst, we import all the necessary libraries and modules required for our workflow. This includes libraries for web scraping, language model interaction, data modeling, and workflow management.","metadata":{}},{"cell_type":"code","source":"from bs4 import BeautifulSoup as Soup\nfrom langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\nfrom langchain_anthropic import ChatAnthropic\nfrom pydantic import BaseModel, Field\nfrom typing import List, Optional\nfrom langgraph.graph import END, StateGraph, START\nfrom langchain_core.messages import SystemMessage, HumanMessage\nfrom kaggle_secrets import UserSecretsClient","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T07:19:26.884453Z","iopub.execute_input":"2025-01-15T07:19:26.88485Z","iopub.status.idle":"2025-01-15T07:19:30.699324Z","shell.execute_reply.started":"2025-01-15T07:19:26.884815Z","shell.execute_reply":"2025-01-15T07:19:30.698115Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Constants and Configuration\n\nThis section defines key constants used throughout the workflow, such as the model name, the maximum number of iterations for code generation attempts, and the URL for your private library documentation (e.g., LCEL documentation).","metadata":{}},{"cell_type":"code","source":"# Maximum number of iterations for code generation attempts\nMAX_ITERATIONS = 3\n\n# URL for your private library (LCEL documentation is used here)\nLCEL_DOCS_URL = \"https://python.langchain.com/docs/concepts/lcel/\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T07:19:57.91987Z","iopub.execute_input":"2025-01-15T07:19:57.92033Z","iopub.status.idle":"2025-01-15T07:19:57.925561Z","shell.execute_reply.started":"2025-01-15T07:19:57.920289Z","shell.execute_reply":"2025-01-15T07:19:57.924079Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Loading Custom Library Documentation\n\nThis section provides a function to load and combine custom library documentation (demonstrated here using LCEL documentation) from a specified URL. A recursive URL loader is utilized to fetch and parse the content, ensuring the inclusion of all relevant documentation pages.","metadata":{}},{"cell_type":"code","source":"# Load LCEL Documentation\ndef load_lcel_docs(url: str) -> str:\n    \"\"\"\n    Load and concatenate LCEL documentation from the given URL.\n\n    Args:\n        url (str): The URL to load the documentation from.\n\n    Returns:\n        str: Concatenated content of all the documentation pages.\n    \"\"\"\n    loader = RecursiveUrlLoader(\n        url=url, max_depth=20, extractor=lambda x: Soup(x, \"html.parser\").text\n    )\n    docs = loader.load()\n    # Sort documents by source in reverse order to ensure consistent ordering\n    sorted_docs = sorted(docs, key=lambda x: x.metadata[\"source\"], reverse=True)\n    # Join all document contents with a separator\n    return \"\\n\\n\\n --- \\n\\n\\n\".join(doc.page_content for doc in sorted_docs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T07:19:57.927119Z","iopub.execute_input":"2025-01-15T07:19:57.927512Z","iopub.status.idle":"2025-01-15T07:19:57.948435Z","shell.execute_reply.started":"2025-01-15T07:19:57.927482Z","shell.execute_reply":"2025-01-15T07:19:57.947127Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Defining Data Models\n\nUsing Pydantic, we define data models to structure the state of the workflow and the code solutions generated. This ensures type safety and clarity in the data being handled.\n\n### CodeSolution Model\n\nRepresents the structure of the code solutions, including a description (`prefix`), import statements (`imports`), and the main code block (`code`).","metadata":{}},{"cell_type":"code","source":"# Data Model for Code Solutions\nclass CodeSolution(BaseModel):\n    \"\"\"\n    Schema for code solutions to questions about LCEL.\n\n    Attributes:\n        prefix (str): Description of the problem and approach.\n        imports (str): Code block containing import statements.\n        code (str): Code block excluding import statements.\n    \"\"\"\n    prefix: str = Field(description=\"Description of the problem and approach\")\n    imports: str = Field(description=\"Code block import statements\")\n    code: str = Field(description=\"Code block not including import statements\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T07:19:57.950129Z","iopub.execute_input":"2025-01-15T07:19:57.95051Z","iopub.status.idle":"2025-01-15T07:19:57.977653Z","shell.execute_reply.started":"2025-01-15T07:19:57.950472Z","shell.execute_reply":"2025-01-15T07:19:57.976525Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### GraphState Model\n\nRepresents the state of the workflow graph, tracking errors, messages, generated code solutions, and the number of iterations.","metadata":{}},{"cell_type":"code","source":"# Graph State Definition using Pydantic\nclass GraphState(BaseModel):\n    \"\"\"\n    Represents the state of the graph.\n\n    Attributes:\n        error (str): Indicates if an error occurred ('yes' or 'no').\n        messages (List): List of messages (user questions, error messages, etc.).\n        generation (Optional[CodeSolution]): Generated code solution.\n        iterations (int): Number of attempts made.\n    \"\"\"\n    error: str = Field(default=\"no\", description=\"'yes' or 'no' to indicate if an error occurred\")\n    messages: List = Field(default_factory=list, description=\"List of messages (user questions, error messages, etc.)\")\n    generation: Optional[CodeSolution] = Field(default=None, description=\"Generated code solution\")\n    iterations: int = Field(default=0, description=\"Number of attempts made\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T07:19:57.979718Z","iopub.execute_input":"2025-01-15T07:19:57.98013Z","iopub.status.idle":"2025-01-15T07:19:58.014608Z","shell.execute_reply.started":"2025-01-15T07:19:57.98009Z","shell.execute_reply":"2025-01-15T07:19:58.013216Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Initializing the Language Model\n\nWe initialize the Language Learning Model (LLM) that will be used for generating code solutions. In this case, we're using the `deepseek-chat` model from DeepSeek, but alternatives like Anthropic's Claude or OpenAI's GPT-4 can also be configured.","metadata":{}},{"cell_type":"code","source":"user_secrets = UserSecretsClient()\n\n# Initialize LLM\n# Anthropic\n#llm = ChatAnthropic(temperature=0, model=\"claude-3-5-sonnet-latest\", api_key=user_secrets.get_secret(\"my-anthropic-api-key\"))\n\n# OpenAI\n# llm = ChatOpenAI(temperature=0, model=\"gpt-4o\", api_key=user_secrets.get_secret(\"my-openai-api-key\"))\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, base_url=\"http://20.243.34.136:2999/v1\",\n                        openai_api_key=\"sk-j8r3Pxztstd3wBjF8fEe44E63f69486bAdC2C4562bD1E1F3\")\n\n# DeepSeek-V3\n#llm = ChatOpenAI(temperature=0, model=\"deepseek-chat\", api_key=user_secrets.get_secret(\"my-deepseek-api-key\"),\n#                 base_url=\"https://api.deepseek.com/v1\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T07:24:50.476433Z","iopub.execute_input":"2025-01-15T07:24:50.476781Z","iopub.status.idle":"2025-01-15T07:24:50.824136Z","shell.execute_reply.started":"2025-01-15T07:24:50.476754Z","shell.execute_reply":"2025-01-15T07:24:50.823109Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Creating Prompt Templates\n\nWe create a prompt template that guides the LLM in generating structured code solutions. The prompt includes system messages and placeholders for context and user messages.","metadata":{}},{"cell_type":"code","source":"# Prompt Template for Code Generation\ncode_gen_prompt = ChatPromptTemplate.from_messages(\n    [\n        SystemMessage(\n            \"\"\"You are a coding assistant with expertise in LCEL, LangChain expression language.\nHere is a full set of LCEL documentation:\n------------------------------------------\n{context}\n------------------------------------------\nAnswer the user question based on the above provided documentation. Ensure any code you provide can be executed\nwith all required imports and variables defined. Structure your answer with a description of the code solution.\nThen list the imports. And finally list the functioning code block. Here is the user question:\"\"\"\n        ),\n        (\"placeholder\", \"{messages}\"),\n    ]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T07:24:50.825453Z","iopub.execute_input":"2025-01-15T07:24:50.825951Z","iopub.status.idle":"2025-01-15T07:24:50.831063Z","shell.execute_reply.started":"2025-01-15T07:24:50.82589Z","shell.execute_reply":"2025-01-15T07:24:50.829882Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Defining Workflow Nodes\n\nWorkflow nodes are functions that represent different stages in the workflow. Here, we define three primary nodes: `gen_code_node`, `check_code_node`, and `reflect_code_node`.\n\n### Code Generation Node\n\nGenerates a code solution based on the current state, invoking the LLM with the appropriate context and messages.","metadata":{}},{"cell_type":"code","source":"# Nodes\ndef gen_code_node(state: GraphState) -> GraphState:\n    \"\"\"\n    Generate a code solution based on the current state.\n\n    Args:\n        state (GraphState): The current state of the graph.\n\n    Returns:\n        GraphState: Updated state with the generated code solution.\n    \"\"\"\n    print(\"---GENERATING CODE SOLUTION---\")\n    messages = state.messages\n    iterations = state.iterations\n    error = state.error\n\n    # Add retry message if there was an error\n    if error == \"yes\":\n        messages.append(HumanMessage(\"Now, try again. Invoke the code tool to structure the output with a prefix, imports, and code block.\"))\n\n    # Generate code solution using the code generation chain\n    code_solution = code_gen_chain.invoke({\"context\": concatenated_content, \"messages\": messages})\n    # Append the generated solution to the messages\n    messages.append(HumanMessage(f\"{code_solution.prefix} \\n Imports: {code_solution.imports} \\n Code: {code_solution.code}\"))\n\n    # Increment iteration count and return updated state\n    return GraphState(\n        error=\"no\",\n        messages=messages,\n        generation=code_solution,\n        iterations=iterations + 1,\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T07:24:50.833911Z","iopub.execute_input":"2025-01-15T07:24:50.834405Z","iopub.status.idle":"2025-01-15T07:24:50.856311Z","shell.execute_reply.started":"2025-01-15T07:24:50.834367Z","shell.execute_reply":"2025-01-15T07:24:50.855147Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Code Checking Node\n\nChecks the generated code for errors by attempting to execute the import statements and the main code block. Updates the state based on the success or failure of these executions.","metadata":{}},{"cell_type":"code","source":"def check_code_node(state: GraphState) -> GraphState:\n    \"\"\"\n    Check the generated code for errors.\n\n    Args:\n        state (GraphState): The current state of the graph.\n\n    Returns:\n        GraphState: Updated state indicating whether the code passed or failed the checks.\n    \"\"\"\n    print(\"---CHECKING CODE---\")\n    messages = state.messages\n    code_solution = state.generation\n    iterations = state.iterations\n\n    # Check imports by attempting to execute them\n    try:\n        exec(code_solution.imports)\n    except Exception as e:\n        print(\"---CODE IMPORT CHECK: FAILED---\")\n        messages.append(HumanMessage(f\"Your solution failed the import test: {e}\"))\n        return GraphState(\n            error=\"yes\",\n            messages=messages,\n            generation=code_solution,\n            iterations=iterations,\n        )\n\n    # Check execution by attempting to run the full code (imports + code)\n    try:\n        exec(code_solution.imports + \"\\n\" + code_solution.code)\n    except Exception as e:\n        print(\"---CODE BLOCK CHECK: FAILED---\")\n        messages.append(HumanMessage(f\"Your solution failed the code execution test: {e}\"))\n        return GraphState(\n            error=\"yes\",\n            messages=messages,\n            generation=code_solution,\n            iterations=iterations,\n        )\n\n    # If no errors, return the state with error set to 'no'\n    print(\"---NO CODE TEST FAILURES---\")\n    return GraphState(\n        error=\"no\",\n        messages=messages,\n        generation=code_solution,\n        iterations=iterations,\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T07:24:50.858339Z","iopub.execute_input":"2025-01-15T07:24:50.858755Z","iopub.status.idle":"2025-01-15T07:24:50.876925Z","shell.execute_reply.started":"2025-01-15T07:24:50.85872Z","shell.execute_reply":"2025-01-15T07:24:50.875633Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Reflection Node\n\nReflects on any errors encountered during code generation or execution, providing insights for improvement.","metadata":{}},{"cell_type":"code","source":"def reflect_code_node(state: GraphState) -> GraphState:\n    \"\"\"\n    Reflect on errors and provide insights for improvement.\n\n    Args:\n        state (GraphState): The current state of the graph.\n\n    Returns:\n        GraphState: Updated state with reflections on the error.\n    \"\"\"\n    print(\"---REFLECTING ON ERRORS---\")\n    messages = state.messages\n    code_solution = state.generation\n\n    # Generate reflections using the code generation chain\n    reflections = code_gen_chain.invoke({\"context\": concatenated_content, \"messages\": messages})\n    messages.append(HumanMessage(f\"Here are reflections on the error: {reflections}\"))\n\n    return GraphState(\n        error=\"yes\",\n        messages=messages,\n        generation=code_solution,\n        iterations=state.iterations,\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T07:24:50.878009Z","iopub.execute_input":"2025-01-15T07:24:50.878309Z","iopub.status.idle":"2025-01-15T07:24:50.903192Z","shell.execute_reply.started":"2025-01-15T07:24:50.878283Z","shell.execute_reply":"2025-01-15T07:24:50.901955Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Building and Compiling the Workflow\n\nWe construct the workflow graph by adding the defined nodes and specifying the transitions between them based on the state of the workflow. The workflow starts with code generation, followed by code checking, and optionally reflects on errors before retrying.","metadata":{}},{"cell_type":"code","source":"# Chain for Code Generation\ncode_gen_chain = code_gen_prompt | llm.with_structured_output(CodeSolution)\n\n# Edges\ndef decide_to_finish(state: GraphState) -> str:\n    \"\"\"\n    Determine whether to finish or retry based on the state.\n\n    Args:\n        state (GraphState): The current state of the graph.\n\n    Returns:\n        str: Decision to either finish or retry the code generation process.\n    \"\"\"\n    error = state.error\n    iterations = state.iterations\n\n    # If no error or max iterations reached, finish\n    if error == \"no\" or iterations == MAX_ITERATIONS:\n        print(\"---DECISION: FINISH---\")\n        return \"end\"\n    else:\n        # Otherwise, decide to retry or reflect based on the error\n        print(\"---DECISION: RE-TRY SOLUTION---\")\n        return \"reflect_code_node\" if error == \"yes\" else \"gen_code_node\"\n\n# Build and Compile the Workflow\nworkflow = StateGraph(GraphState)\nworkflow.add_node(\"gen_code_node\", gen_code_node)  # Add code generation node\nworkflow.add_node(\"check_code_node\", check_code_node)  # Add code checking node\nworkflow.add_node(\"reflect_code_node\", reflect_code_node)  # Add reflection node\n\nworkflow.add_edge(START, \"gen_code_node\")  # Start with code generation\nworkflow.add_edge(\"gen_code_node\", \"check_code_node\")  # Check the generated code\nworkflow.add_conditional_edges(\n    \"check_code_node\",\n    decide_to_finish,\n    {\"end\": END, \"reflect_code_node\": \"reflect_code_node\", \"gen_code_node\": \"gen_code_node\"},\n)\nworkflow.add_edge(\"reflect_code_node\", \"gen_code_node\")  # Retry after reflection\napp = workflow.compile()  # Compile the workflow","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T07:24:50.904472Z","iopub.execute_input":"2025-01-15T07:24:50.904854Z","iopub.status.idle":"2025-01-15T07:24:50.933024Z","shell.execute_reply.started":"2025-01-15T07:24:50.904821Z","shell.execute_reply":"2025-01-15T07:24:50.931686Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Optional: Displaying the Workflow Graph\n\nIf desired, the workflow graph can be visualized. This requires additional dependencies and is optional.","metadata":{}},{"cell_type":"code","source":"# Optional: Display the workflow graph (requires extra dependencies)\nfrom IPython.display import Image, display\n\ntry:\n    display(Image(app.get_graph().draw_mermaid_png()))\nexcept Exception:\n    # This requires some extra dependencies and is optional\n    pass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T07:24:50.933961Z","iopub.execute_input":"2025-01-15T07:24:50.934412Z","iopub.status.idle":"2025-01-15T07:24:51.005391Z","shell.execute_reply.started":"2025-01-15T07:24:50.934377Z","shell.execute_reply":"2025-01-15T07:24:51.004261Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Loading LCEL Documentation Content\n\nWe load the LCEL documentation content using the previously defined `load_lcel_docs` function. This content serves as the context for the LLM when generating code solutions.","metadata":{}},{"cell_type":"code","source":"# Load LCEL Documentation\nconcatenated_content = load_lcel_docs(LCEL_DOCS_URL)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T07:24:51.006514Z","iopub.execute_input":"2025-01-15T07:24:51.006887Z","iopub.status.idle":"2025-01-15T07:24:51.64853Z","shell.execute_reply.started":"2025-01-15T07:24:51.006853Z","shell.execute_reply":"2025-01-15T07:24:51.647045Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Function to Extract and Print Code\n\nA utility function is defined to extract the generated Python code from the workflow's solution and print it in a readable format.","metadata":{}},{"cell_type":"code","source":"# Function to Extract and Print Code\ndef extract_and_print_code(solution: dict) -> None:\n    \"\"\"\n    Extracts the Python code from the solution and prints it.\n\n    Args:\n        solution (dict): The solution dictionary returned by the workflow.\n    \"\"\"\n    # Convert the solution dictionary to a GraphState object\n    graph_state = GraphState(**solution)\n\n    if graph_state.generation is None:\n        print(\"No code generation found in the solution.\")\n        return\n\n    # Extract the code from the CodeSolution object\n    code_solution = graph_state.generation\n    print()\n    print(\"# \" + \"-\"*80)\n    print(\"# Extracted Python Code\")\n    print(\"# \" + \"-\"*80)\n    print(code_solution.code)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T07:24:51.680493Z","iopub.execute_input":"2025-01-15T07:24:51.680875Z","iopub.status.idle":"2025-01-15T07:24:51.687128Z","shell.execute_reply.started":"2025-01-15T07:24:51.680844Z","shell.execute_reply":"2025-01-15T07:24:51.68589Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Executing Example Queries\n\nWe demonstrate the workflow's capabilities by executing several example user queries. For each query, we initialize the workflow's state, invoke the workflow, and display the generated code solution.","metadata":{}},{"cell_type":"code","source":"# Example Usage 1\nquestion = \"How can I directly pass a string to a runnable and use it to construct the input needed for my prompt?\"\n\ninitial_state = GraphState(\n    messages=[HumanMessage(question)],\n    iterations=0,\n    error=\"no\",\n    generation=None,\n)\nsolution = app.invoke(initial_state)\n\n# Output the Solution\nextract_and_print_code(solution)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T07:24:51.688406Z","iopub.execute_input":"2025-01-15T07:24:51.688724Z","iopub.status.idle":"2025-01-15T07:25:49.523975Z","shell.execute_reply.started":"2025-01-15T07:24:51.688698Z","shell.execute_reply":"2025-01-15T07:25:49.52252Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example Usage 2\nquestion = \"\"\"How can I create a simple LCEL chain that takes a string as input, converts it to uppercase,\nand then appends the text ' - Processed' to the result? Use the pipe operator to chain the steps.\"\"\"\n\ninitial_state = GraphState(\n    messages=[HumanMessage(question)],\n    iterations=0,\n    error=\"no\",\n    generation=None,\n)\nsolution = app.invoke(initial_state)\n\n# Output the Solution\nextract_and_print_code(solution)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example Usage 3\nquestion = \"\"\"How can I create an LCEL chain that routes the input to one of two runnables based on a condition?\nFor example, if the input is a number greater than 10, route it to a runnable that multiplies it by 2;\notherwise, route it to a runnable that adds 5.\"\"\"\n\ninitial_state = GraphState(\n    messages=[HumanMessage(question)],\n    iterations=0,\n    error=\"no\",\n    generation=None,\n)\nsolution = app.invoke(initial_state)\n\n# Output the Solution\nextract_and_print_code(solution)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example Usage 4\nquestion = \"\"\"How can I create an LCEL chain that takes a dictionary with name and age keys,\nformats a string like 'Name: {name}, Age: {age}', and then converts the result to uppercase?\nUse the pipe operator to chain the steps.\"\"\"\n\ninitial_state = GraphState(\n    messages=[HumanMessage(question)],\n    iterations=0,\n    error=\"no\",\n    generation=None,\n)\nsolution = app.invoke(initial_state)\n\n# Output the Solution\nextract_and_print_code(solution)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Conclusion\n\nIn this notebook, we’ve built a robust **AI Coder Workflow** for your custom library using **LangGraph**. This workflow automates the process of interpreting user queries, generating Python code, and refining solutions through structured prompts, error handling, and iterative enhancements. By integrating LangGraph’s modular workflow design with comprehensive library documentation, this system ensures that the generated code is both accurate and executable.\n\nThis approach not only streamlines development tasks but also provides a flexible foundation for scaling automation in coding processes. With this workflow, developers can confidently address complex coding requirements while maintaining the reliability and precision needed for custom libraries.","metadata":{}}]}