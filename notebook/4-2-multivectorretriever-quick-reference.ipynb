{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **LangChain `MultiVectorRetriever` Quick Reference**\n\n## **Introduction**\n\nThe `MultiVectorRetriever` class allows for the retrieval of documents based on a set of multiple embeddings. This is beneficial for applications such as:\n\n- **Chunking**: Splitting a document into smaller segments and embedding each segment, which can help in capturing semantic meaning while maintaining context.\n- **Summarization**: Creating and embedding summaries of documents alongside the original content, facilitating quicker searches and retrievals.\n- **Hypothetical Questions**: Embedding hypothetical questions relevant to the document, enhancing the retriever's ability to match queries with appropriate documents.\n\n### Key Features\n\n- **Parallel Invocation**: The `MultiVectorRetriever` can run its retrieval process in parallel using asynchronous methods, improving performance when dealing with large datasets.\n- **Custom Search Types**: Users can specify different search types, such as similarity searches or Max Marginal Relevance (MMR), to tailor the retrieval process according to their needs.\n- **Flexible Storage Options**: The retriever supports various storage backends for both the parent documents and their embeddings, allowing for greater flexibility in implementation.\n\n### Use Cases\n\n1. **Enhanced Document Retrieval**: By using multiple vectors, the retriever can return more relevant results based on varied representations of a document.\n2. **Efficient Information Retrieval**: Ideal for applications requiring quick access to large volumes of information, such as chatbots or search engines that need to understand user queries deeply.","metadata":{}},{"cell_type":"markdown","source":"---\n\n## Preparation\n\n### Installing Required Libraries\nThis section installs the necessary Python libraries for working with LangChain, OpenAI embeddings, and Chroma vector store. These libraries include:\n- `langchain-openai`: Provides integration with OpenAI's embedding models.\n- `langchain_community`: Contains community-contributed modules and tools for LangChain.\n- `langchain_experimental`: Includes experimental features and utilities for LangChain.\n- `langchain-chroma`: Enables integration with the Chroma vector database.\n- `chromadb`: The core library for the Chroma vector database.","metadata":{}},{"cell_type":"code","source":"!pip install -qU langchain-openai\n!pip install -qU langchain_community\n!pip install -qU langchain_experimental\n!pip install -qU langchain-chroma>=0.1.2\n!pip install -qU chromadb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T18:06:54.243611Z","iopub.execute_input":"2025-01-19T18:06:54.244008Z","iopub.status.idle":"2025-01-19T18:08:09.397249Z","shell.execute_reply.started":"2025-01-19T18:06:54.243952Z","shell.execute_reply":"2025-01-19T18:08:09.395913Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Initializing OpenAI Embeddings\nThis section demonstrates how to securely fetch an OpenAI API key using Kaggle's `UserSecretsClient` and initialize the OpenAI embedding model. The `OpenAIEmbeddings` class is used to create an embedding model instance, which will be used to convert text into numerical embeddings.\n\nKey steps:\n1. **Fetch API Key**: The OpenAI API key is securely retrieved using Kaggle's `UserSecretsClient`.\n2. **Initialize Embeddings**: The `OpenAIEmbeddings` class is initialized with the `text-embedding-3-small` model and the fetched API key.\n\nThis setup ensures that the embedding model is ready for use in downstream tasks, such as caching embeddings or creating vector stores.","metadata":{}},{"cell_type":"code","source":"from langchain_openai import OpenAIEmbeddings\nfrom kaggle_secrets import UserSecretsClient\n\n# Fetch API key securely\nuser_secrets = UserSecretsClient()\nmy_api_key = user_secrets.get_secret(\"api-key-openai\")\n\n# Initialize OpenAI embeddings\nembed = OpenAIEmbeddings(model=\"text-embedding-3-small\", api_key=my_api_key)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T18:08:09.398567Z","iopub.execute_input":"2025-01-19T18:08:09.398856Z","iopub.status.idle":"2025-01-19T18:08:12.527925Z","shell.execute_reply.started":"2025-01-19T18:08:09.398831Z","shell.execute_reply":"2025-01-19T18:08:12.526715Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n## **1. Document Retrieval**\n\n### **Basic Document Retrieval**\nThis example demonstrates how to retrieve relevant documents using a query. It initializes a `Chroma` vector store and an `InMemoryByteStore` for storing parent documents. Documents are added to both the vector store and byte store, and a query is used to retrieve relevant documents.","metadata":{}},{"cell_type":"code","source":"from langchain_chroma import Chroma\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain.retrievers.multi_vector import MultiVectorRetriever\nfrom langchain.schema import Document\nfrom langchain.storage import InMemoryByteStore\nfrom langchain_core.load.dump import dumps  # For serializing documents\n\n# Initialize vector store, byte store, and embeddings\nvectorstore = Chroma(embedding_function=embed)\nbyte_store = InMemoryByteStore()  # Required for MultiVectorRetriever\nretriever = MultiVectorRetriever(vectorstore=vectorstore, byte_store=byte_store)\n\n# Add documents to the vector store and byte store\ndocuments = [\n    Document(page_content=\"LangChain is a framework for building LLM applications.\", metadata={\"doc_id\": \"1\"})\n]\n\n# Add documents to the vector store\nvectorstore.add_documents(documents)\n\n# Add parent documents to the byte store (properly serialized)\nfor doc in documents:\n    serialized_doc = dumps(doc)  # Serialize the Document object\n    byte_store.mset([(doc.metadata[\"doc_id\"], serialized_doc.encode(\"utf-8\"))])\n\n# Retrieve relevant documents\nquery = \"What is LangChain?\"\nresults = retriever.invoke(query)\nprint(results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T18:08:12.529042Z","iopub.execute_input":"2025-01-19T18:08:12.529631Z","iopub.status.idle":"2025-01-19T18:08:16.760166Z","shell.execute_reply.started":"2025-01-19T18:08:12.529599Z","shell.execute_reply":"2025-01-19T18:08:16.75908Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Retrieval with Metadata Filtering**\nThis example shows how to retrieve documents while filtering by metadata. Documents with specific metadata (e.g., `language: Python`) are added to the vector store and byte store. The retriever is then used to fetch documents that match the metadata filter.","metadata":{}},{"cell_type":"code","source":"# Add documents with metadata\ndocuments = [\n    Document(page_content=\"LangChain supports Python.\", metadata={\"doc_id\": \"2\", \"language\": \"Python\"}),\n    Document(page_content=\"LangChain also supports JavaScript.\", metadata={\"doc_id\": \"3\", \"language\": \"JavaScript\"}),\n]\n\n# Add documents to the vector store\nvectorstore.add_documents(documents)\n\n# Add parent documents to the byte store (properly serialized)\nfor doc in documents:\n    serialized_doc = dumps(doc)  # Serialize the Document object\n    byte_store.mset([(doc.metadata[\"doc_id\"], serialized_doc.encode(\"utf-8\"))])\n\n# Retrieve documents with metadata filtering\nquery = \"What languages does LangChain support?\"\nresults = retriever.invoke(query, search_kwargs={\"filter\": {\"language\": \"Python\"}})\nprint(results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T18:08:16.761362Z","iopub.execute_input":"2025-01-19T18:08:16.761731Z","iopub.status.idle":"2025-01-19T18:08:18.516102Z","shell.execute_reply.started":"2025-01-19T18:08:16.761701Z","shell.execute_reply":"2025-01-19T18:08:18.514901Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n## **2. Batch Processing**\n\n### **Batch Retrieval**\nThis example demonstrates how to retrieve documents for multiple queries in a batch. Documents are added to the vector store and byte store, and a list of queries is processed in a single batch. The results for all queries are returned together.","metadata":{}},{"cell_type":"code","source":"# Add documents to the vector store\ndocuments = [\n    Document(page_content=\"LangChain is a framework for LLM applications.\", metadata={\"doc_id\": \"4\"}),\n    Document(page_content=\"OpenAI provides powerful language models.\", metadata={\"doc_id\": \"5\"}),\n]\n\n# Add documents to the vector store\nvectorstore.add_documents(documents)\n\n# Add parent documents to the byte store (properly serialized)\nfor doc in documents:\n    serialized_doc = dumps(doc)  # Serialize the Document object\n    byte_store.mset([(doc.metadata[\"doc_id\"], serialized_doc.encode(\"utf-8\"))])\n\n# Batch retrieval\nqueries = [\"What is LangChain?\", \"What does OpenAI provide?\"]\nresults = retriever.batch(queries)\nprint(results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T18:08:18.517538Z","iopub.execute_input":"2025-01-19T18:08:18.51796Z","iopub.status.idle":"2025-01-19T18:08:19.332692Z","shell.execute_reply.started":"2025-01-19T18:08:18.517921Z","shell.execute_reply":"2025-01-19T18:08:19.331587Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Batch Retrieval with Custom Config**\nThis example shows how to use a custom configuration for batch retrieval. The `max_concurrency` parameter is used to control the number of parallel retrieval operations. This is useful for optimizing performance when processing a large number of queries.","metadata":{}},{"cell_type":"code","source":"# Batch retrieval with custom config\nqueries = [\"What is LangChain?\", \"What does OpenAI provide?\"]\nresults = retriever.batch(queries, config={\"max_concurrency\": 2})\nprint(results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T18:08:19.33376Z","iopub.execute_input":"2025-01-19T18:08:19.334174Z","iopub.status.idle":"2025-01-19T18:08:19.906489Z","shell.execute_reply.started":"2025-01-19T18:08:19.334141Z","shell.execute_reply":"2025-01-19T18:08:19.905224Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n## **3. Streaming**\n\n### **Streaming Retrieval Results**\nThis example demonstrates how to stream retrieval results in real-time. Documents are added to the vector store and byte store, and the retriever streams results as they are retrieved. This is useful for handling large datasets or real-time applications.","metadata":{}},{"cell_type":"code","source":"# Add documents to the vector store\ndocuments = [\n    Document(page_content=\"LangChain is a framework for LLM applications.\", metadata={\"doc_id\": \"6\"}),\n    Document(page_content=\"OpenAI provides powerful language models.\", metadata={\"doc_id\": \"7\"}),\n]\n\n# Add documents to the vector store\nvectorstore.add_documents(documents)\n\n# Add parent documents to the byte store (properly serialized)\nfor doc in documents:\n    serialized_doc = dumps(doc)  # Serialize the Document object\n    byte_store.mset([(doc.metadata[\"doc_id\"], serialized_doc.encode(\"utf-8\"))])\n\n# Stream retrieval results\nquery = \"What is LangChain?\"\nfor result in retriever.stream(query):\n    print(result)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T18:08:19.909617Z","iopub.execute_input":"2025-01-19T18:08:19.90992Z","iopub.status.idle":"2025-01-19T18:08:20.88177Z","shell.execute_reply.started":"2025-01-19T18:08:19.90989Z","shell.execute_reply":"2025-01-19T18:08:20.88084Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Streaming with Metadata**\nThis example shows how to stream results while including metadata. The `include_metadata` parameter is used to ensure that metadata is included in the streaming output. This is useful when additional context is needed for each retrieved document.","metadata":{}},{"cell_type":"code","source":"# Stream retrieval results with metadata\nquery = \"What does OpenAI provide?\"\nfor result in retriever.stream(query, search_kwargs={\"include_metadata\": True}):\n    print(result)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T18:08:20.883152Z","iopub.execute_input":"2025-01-19T18:08:20.883532Z","iopub.status.idle":"2025-01-19T18:08:21.361004Z","shell.execute_reply.started":"2025-01-19T18:08:20.883502Z","shell.execute_reply":"2025-01-19T18:08:21.359694Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n## **4. Configuration and Customization**\n\n### **Binding Arguments to the Retriever**\nThis example demonstrates how to bind additional arguments to the retriever. The `search_kwargs` parameter is used to customize the retrieval process, such as limiting the number of results (`k`). This allows for flexible configuration of the retriever.","metadata":{}},{"cell_type":"code","source":"from langchain_chroma import Chroma\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain.retrievers.multi_vector import MultiVectorRetriever\nfrom langchain.schema import Document\nfrom langchain.storage import InMemoryByteStore\nfrom langchain_core.load.dump import dumps  # For serializing documents\n\n# Initialize vector store, byte store\nvectorstore = Chroma(embedding_function=embed)\nbyte_store = InMemoryByteStore()\nretriever = MultiVectorRetriever(vectorstore=vectorstore, byte_store=byte_store)\n\n# Add documents to the vector store and byte store\ndocuments = [\n    Document(page_content=\"LangChain is a framework for building LLM applications.\", metadata={\"doc_id\": \"1\"})\n]\n\n# Add documents to the vector store\nvectorstore.add_documents(documents)\n\n# Add parent documents to the byte store (properly serialized)\nfor doc in documents:\n    serialized_doc = dumps(doc)  # Serialize the Document object\n    byte_store.mset([(doc.metadata[\"doc_id\"], serialized_doc.encode(\"utf-8\"))])\n\n# Bind additional arguments\ncustom_retriever = retriever.bind(search_kwargs={\"k\": 3})  # Retrieve top 3 results\n\n# Retrieve relevant documents\nquery = \"What is LangChain?\"\nresults = custom_retriever.invoke(query)\nprint(results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T18:08:21.362134Z","iopub.execute_input":"2025-01-19T18:08:21.362521Z","iopub.status.idle":"2025-01-19T18:08:21.849159Z","shell.execute_reply.started":"2025-01-19T18:08:21.362481Z","shell.execute_reply":"2025-01-19T18:08:21.848077Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Configurable Alternatives**\nThis example shows how to configure alternative retrievers at runtime. A `ConfigurableField` is used to define a default retriever and an alternative retriever. The retriever can be switched at runtime using a configuration parameter. This is useful for testing different retrieval strategies.","metadata":{}},{"cell_type":"code","source":"from langchain_core.runnables.utils import ConfigurableField\n\n# Create a configurable retriever\nconfigurable_retriever = retriever.configurable_alternatives(\n    ConfigurableField(id=\"retriever\"),\n    default_key=\"default\",\n    alternative_retriever=MultiVectorRetriever(vectorstore=Chroma(embedding_function=embed), byte_store=InMemoryByteStore())\n)\n\n# Use the default retriever\nprint(\"Using Default Retriever:\")\nresults = configurable_retriever.invoke(\"What is LangChain?\")\nprint(results)\n\n# Use the alternative retriever\nprint(\"\\nUsing Alternative Retriever:\")\nresults = configurable_retriever.with_config(configurable={\"retriever\": \"alternative_retriever\"}).invoke(\"What is LangChain?\")\nprint(results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T18:09:28.520636Z","iopub.execute_input":"2025-01-19T18:09:28.521092Z","iopub.status.idle":"2025-01-19T18:09:29.53326Z","shell.execute_reply.started":"2025-01-19T18:09:28.521061Z","shell.execute_reply":"2025-01-19T18:09:29.532262Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n## **5. Event Handling and Error Handling**\n\n### **Adding Lifecycle Listeners**\nThis example demonstrates how to add synchronous lifecycle listeners to the retriever. The `on_start` and `on_end` listeners are used to track the start and end of retrieval operations. This is useful for logging or monitoring the retrieval process.","metadata":{}},{"cell_type":"code","source":"from langchain_chroma import Chroma\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain.retrievers.multi_vector import MultiVectorRetriever\nfrom langchain.schema import Document\nfrom langchain.storage import InMemoryByteStore\nfrom langchain_core.load.dump import dumps\n\n# Initialize vector store, byte store, and embeddings\nvectorstore = Chroma(embedding_function=embed)\nbyte_store = InMemoryByteStore()\nretriever = MultiVectorRetriever(vectorstore=vectorstore, byte_store=byte_store)\n\n# Add documents to the vector store and byte store\ndocuments = [\n    Document(page_content=\"LangChain is a framework for building LLM applications.\", metadata={\"doc_id\": \"1\"})\n]\n\n# Add documents to the vector store\nvectorstore.add_documents(documents)\n\n# Add parent documents to the byte store (properly serialized)\nfor doc in documents:\n    serialized_doc = dumps(doc)  # Serialize the Document object\n    byte_store.mset([(doc.metadata[\"doc_id\"], serialized_doc.encode(\"utf-8\"))])\n\n# Define lifecycle listeners\ndef on_start(run):\n    print(f\"Retrieval started with input: {run.input}\")\n\ndef on_end(run):\n    print(f\"Retrieval ended with output: {run.output}\")\n\n# Add listeners to the retriever\nlistener_retriever = retriever.with_listeners(on_start=on_start, on_end=on_end)\n\n# Invoke the retriever with listeners\nresults = listener_retriever.invoke(\"What is LangChain?\")\nprint(results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T18:09:29.534643Z","iopub.execute_input":"2025-01-19T18:09:29.53507Z","iopub.status.idle":"2025-01-19T18:09:30.167022Z","shell.execute_reply.started":"2025-01-19T18:09:29.535006Z","shell.execute_reply":"2025-01-19T18:09:30.165895Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Retry on Failure**\nThis example shows how to add retry logic to handle failures. The `with_retry` method is used to specify the number of retry attempts and the types of exceptions to handle. This ensures that transient failures do not disrupt the retrieval process.","metadata":{}},{"cell_type":"code","source":"# Add retry logic\nretry_retriever = retriever.with_retry(stop_after_attempt=3, retry_if_exception_type=(Exception,))\n\n# Invoke the retriever with retry logic\nresults = retry_retriever.invoke(\"What is LangChain?\")\nprint(results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T18:09:30.169806Z","iopub.execute_input":"2025-01-19T18:09:30.170173Z","iopub.status.idle":"2025-01-19T18:09:30.467248Z","shell.execute_reply.started":"2025-01-19T18:09:30.170139Z","shell.execute_reply":"2025-01-19T18:09:30.466223Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Fallback Retriever**\nThis example demonstrates how to add a fallback retriever in case of failure. A fallback retriever is defined and added to the primary retriever. If the primary retriever fails, the fallback retriever is used as a backup. This provides redundancy and improves reliability.","metadata":{}},{"cell_type":"code","source":"from langchain.retrievers import MultiVectorRetriever\n\n# Create a fallback retriever\nfallback_retriever = MultiVectorRetriever(vectorstore=Chroma(embedding_function=embed), byte_store=InMemoryByteStore())\n\n# Add fallback to the retriever\nfallback_enabled_retriever = retriever.with_fallbacks([fallback_retriever])\nresults = fallback_enabled_retriever.invoke(\"What is LangChain?\")\nprint(results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T18:09:30.468365Z","iopub.execute_input":"2025-01-19T18:09:30.468807Z","iopub.status.idle":"2025-01-19T18:09:30.698714Z","shell.execute_reply.started":"2025-01-19T18:09:30.468777Z","shell.execute_reply":"2025-01-19T18:09:30.697697Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n## **6. Best Practices**\n\n### **Example 1: Associating Summaries with a Document for Retrieval**\n\n1. **Summarization Chain**:\n   - A chain is created to summarize documents using an LLM (`ChatOpenAI`).\n   - The chain takes a document's content, generates a summary, and outputs it as a string.\n2. **Batch Summarization**:\n   - The summarization chain is applied to a batch of documents (`docs`) with a concurrency limit of 5.\n3. **Vector Store and Document Store**:\n   - A `Chroma` vector store is initialized to store the summaries.\n   - An `InMemoryByteStore` is used to store the original documents.\n4. **Retriever Initialization**:\n   - A `MultiVectorRetriever` is initialized to link summaries (stored in the vector store) with the original documents (stored in the document store).\n5. **Querying**:\n   - The retriever is queried with a search term (`\"justice breyer\"`), and it returns the relevant parent documents.","metadata":{}},{"cell_type":"code","source":"from langchain_core.documents import Document\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain.storage import InMemoryByteStore\nfrom langchain.retrievers import MultiVectorRetriever\nfrom langchain_openai import OpenAIEmbeddings, ChatOpenAI\nfrom langchain_chroma import Chroma\nimport uuid\n\n# Initialize OpenAI embeddings and LLM\nembed = OpenAIEmbeddings(model=\"text-embedding-3-small\", api_key=my_api_key)\nmodel = ChatOpenAI(model=\"gpt-4o-mini\", api_key=my_api_key)\n\n# Define a list of documents to summarize and retrieve\ndocs = [\n    Document(page_content=\"LangChain is a framework for building LLM applications.\", metadata={\"title\": \"LangChain Overview\"}),\n    Document(page_content=\"OpenAI provides powerful language models like GPT-4.\", metadata={\"title\": \"OpenAI Models\"}),\n    Document(page_content=\"Chroma is a vector store for embedding-based retrieval.\", metadata={\"title\": \"Chroma Vector Store\"}),\n]\n\n# Define summarization chain\nchain = (\n    {\"doc\": lambda x: x.page_content}\n    | ChatPromptTemplate.from_template(\"Summarize the following document:\\n\\n{doc}\")\n    | model\n    | StrOutputParser()\n)\n\n# Generate summaries for documents\nsummaries = chain.batch(docs, {\"max_concurrency\": 5})\n\n# Initialize vector store and document store\nvectorstore = Chroma(collection_name=\"summaries\", embedding_function=embed)\nstore = InMemoryByteStore()\nid_key = \"doc_id\"\n\n# Initialize retriever\nretriever = MultiVectorRetriever(\n    vectorstore=vectorstore,\n    byte_store=store,\n    id_key=id_key,\n)\n\n# Generate unique IDs for documents\ndoc_ids = [str(uuid.uuid4()) for _ in docs]\n\n# Create summary documents\nsummary_docs = [\n    Document(page_content=s, metadata={id_key: doc_ids[i]})\n    for i, s in enumerate(summaries)\n]\n\n# Add summaries to vector store and original documents to document store\nretriever.vectorstore.add_documents(summary_docs)\nretriever.docstore.mset(list(zip(doc_ids, docs)))\n\n# Query the retriever\nretrieved_docs = retriever.invoke(\"LangChain\")\nprint(retrieved_docs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T18:09:30.699698Z","iopub.execute_input":"2025-01-19T18:09:30.699956Z","iopub.status.idle":"2025-01-19T18:09:33.004353Z","shell.execute_reply.started":"2025-01-19T18:09:30.699934Z","shell.execute_reply":"2025-01-19T18:09:33.003301Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Example 2: Hypothetical Queries for Improved Retrieval**\n\n1. **Hypothetical Questions Chain**:\n   - A chain is created to generate hypothetical questions for a document using an LLM (`ChatOpenAI`).\n   - The chain uses a structured output (`HypotheticalQuestions`) to ensure the output is a list of questions.\n2. **Batch Question Generation**:\n   - The chain is applied to a batch of documents (`docs`) with a concurrency limit of 5.\n3. **Vector Store and Document Store**:\n   - A `Chroma` vector store is initialized to store the hypothetical questions.\n   - An `InMemoryByteStore` is used to store the original documents.\n4. **Retriever Initialization**:\n   - A `MultiVectorRetriever` is initialized to link hypothetical questions (stored in the vector store) with the original documents (stored in the document store).\n5. **Querying**:\n   - The retriever is queried with a search term (`\"justice breyer\"`), and it returns the relevant parent documents.","metadata":{}},{"cell_type":"code","source":"from typing import List\nfrom pydantic import BaseModel, Field\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain.storage import InMemoryByteStore\nfrom langchain.retrievers import MultiVectorRetriever\nfrom langchain_chroma import Chroma\nfrom langchain_openai import OpenAIEmbeddings, ChatOpenAI\nimport uuid\n\n# Define Pydantic model for hypothetical questions\nclass HypotheticalQuestions(BaseModel):\n    \"\"\"Generate hypothetical questions.\"\"\"\n    questions: List[str] = Field(..., description=\"List of questions\")\n\n# Initialize OpenAI embeddings and LLM\nembed = OpenAIEmbeddings(model=\"text-embedding-3-small\", api_key=my_api_key)\nmodel = ChatOpenAI(model=\"gpt-4o-mini\", api_key=my_api_key)\n\n# Define chain to generate hypothetical questions\nchain = (\n    {\"doc\": lambda x: x.page_content}\n    | ChatPromptTemplate.from_template(\n        \"Generate a list of exactly 3 hypothetical questions that the below document could be used to answer:\\n\\n{doc}\"\n    )\n    | model.with_structured_output(HypotheticalQuestions)\n    | (lambda x: x.questions)\n)\n\n# Generate hypothetical questions for documents\nhypothetical_questions = chain.batch(docs, {\"max_concurrency\": 5})\n\n# Initialize vector store and document store\nvectorstore = Chroma(collection_name=\"hypo-questions\", embedding_function=embed)\nstore = InMemoryByteStore()\nid_key = \"doc_id\"\n\n# Initialize retriever\nretriever = MultiVectorRetriever(\n    vectorstore=vectorstore,\n    byte_store=store,\n    id_key=id_key,\n)\n\n# Generate unique IDs for documents\ndoc_ids = [str(uuid.uuid4()) for _ in docs]\n\n# Create question documents\nquestion_docs = []\nfor i, question_list in enumerate(hypothetical_questions):\n    question_docs.extend(\n        [Document(page_content=s, metadata={id_key: doc_ids[i]}) for s in question_list]\n    )\n\n# Add questions to vector store and original documents to document store\nretriever.vectorstore.add_documents(question_docs)\nretriever.docstore.mset(list(zip(doc_ids, docs)))\n\n# Query the retriever\nretrieved_docs = retriever.invoke(\"justice breyer\")\nprint(retrieved_docs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T18:09:33.005433Z","iopub.execute_input":"2025-01-19T18:09:33.005781Z","iopub.status.idle":"2025-01-19T18:09:35.748787Z","shell.execute_reply.started":"2025-01-19T18:09:33.005755Z","shell.execute_reply":"2025-01-19T18:09:35.747596Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Conclusion**\n\nThe `MultiVectorRetriever` is a versatile and robust solution for advanced document retrieval tasks. By leveraging multiple representations of documents—such as summaries, chunks, or hypothetical questions—it significantly improves the accuracy and relevance of search results. Its integration with vector stores and document stores allows for efficient indexing and retrieval, while its customizable nature makes it adaptable to a wide range of use cases. Whether you're building a retrieval-augmented generation (RAG) system, a semantic search engine, or a document summarization tool, the `MultiVectorRetriever` provides the flexibility and power needed to deliver high-quality results. With its ability to handle complex retrieval scenarios, it stands as a key component in modern natural language processing workflows.","metadata":{}}]}