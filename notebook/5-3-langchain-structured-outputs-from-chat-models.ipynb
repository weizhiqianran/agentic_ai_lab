{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **LangChain Structured Outputs from Chat Models**\n\n## Introduction\n\nAs large language models (LLMs) like ChatGPT and Claude become increasingly integrated into diverse applications, the need for precise and reliable data exchange grows. Structured outputs offer a solution by enabling chat models to produce responses in specific, predictable formats. This article explores the concept of structured outputs, delves into LangChain's `.with_structured_output()` method, and provides practical examples to illustrate its implementation and benefits.\n\n### What Are Structured Outputs?\n\nStructured outputs refer to responses generated by chat models that adhere to predefined data formats, such as JSON, Pydantic models, or TypedDicts. Unlike unstructured text, structured outputs ensure consistency, facilitating seamless integration with other systems, APIs, or workflows that require specific data formats.\n\n### Why Structured Outputs Matter\n\n- **Predictability**: Downstream systems can reliably parse and utilize the data without ambiguity.\n- **Efficiency**: Automates data formatting and validation, reducing the need for manual intervention.\n- **Robustness**: Minimizes errors by enforcing data integrity through validation mechanisms.\n\n---\n\n## The `.with_structured_output()` Method\n\nLangChain, a powerful framework for building applications with LLMs, offers the `.with_structured_output()` method to enhance interactions by enabling structured responses from chat models. This method is pivotal for developers aiming to integrate LLMs into systems that demand specific data formats.\n\n### Key Features\n\n1. **Declarative Data Models**:\n    - Allows defining desired output structures using Pydantic models or JSON schemas.\n    - **Example**:\n      ```python\n      from pydantic import BaseModel, Field, EmailStr\n      \n      class Person(BaseModel):\n          name: str = Field(description=\"The person's full name\")\n          age: int = Field(description=\"The person's age in years\")\n      ```\n\n2. **Custom Output Parsing**:\n    - Transforms raw LLM outputs to match the defined schema.\n    - Capable of handling complex data transformations, such as converting comma-separated text into lists.\n\n3. **Error Handling and Validation**:\n    - Automatically validates outputs against the specified schema.\n    - Implements error correction loops to rectify invalid outputs using fallback mechanisms.\n\n4. **Integration with OpenAI Function Calling**:\n    - Utilizes OpenAI's function-calling APIs to ensure outputs are in structured formats like JSON.\n    - **Example**:\n      ```python\n      from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n      chain = (\n          prompt\n          | model.bind(function_call={\"name\": \"joke\"}, functions=functions)\n          | JsonOutputFunctionsParser()\n      )\n      ```\n\n5. **Parallel and Modular Processing**:\n    - Integrates seamlessly with other chains or components for multi-step processing.\n    - **Example**:\n      ```python\n      retrieval_chain = (\n          {\"context\": retriever, \"question\": RunnablePassthrough()}\n          | prompt\n          | model\n          | StrOutputParser()\n      )\n      ```\n\n---\n\n## Comparison of Methods\n\n| **Feature**                         | **Pydantic**                                                   | **TypedDict**                                                 | **JSON Schema**                                              |\n|-------------------------------------|----------------------------------------------------------------|----------------------------------------------------------------|--------------------------------------------------------------|\n| **Type Enforcement**                | Strong type enforcement with validation                        | Limited type enforcement                                      | Schema-based validation                                     |\n| **Ease of Use**                     | Requires defining classes; more verbose                        | Simpler for dictionary-like structures                        | Requires understanding of JSON Schema syntax                |\n| **Flexibility**                     | Highly flexible with complex data models                       | Suitable for simpler data structures                           | Excellent for interoperability across different systems     |\n| **Integration**                     | Seamless with Python applications and frameworks                | Easy to integrate with Python codebases                        | Language-agnostic, ideal for API integrations                |\n| **Validation Capabilities**         | Extensive validation options                                   | Basic type annotations                                         | Comprehensive validation rules                               |\n| **Use Cases**                       | Enterprise applications, data pipelines, APIs                   | Lightweight applications, quick prototypes                      | API specifications, cross-language data exchange             |\n| **Example Tools/Frameworks**        | FastAPI, Django, LangChain                                     | LangChain                                                     | OpenAPI, LangChain                                          |\n\n### Choosing the Right Method\n\n- **Use Pydantic** when you need robust data validation and are working within Python-centric ecosystems.\n- **Use TypedDict** for simpler scenarios where lightweight type annotations suffice.\n- **Use JSON Schema** when interoperability and cross-language support are paramount.","metadata":{}},{"cell_type":"code","source":"!pip install -qU langchain-openai\n!pip install -qU langchain-anthropic\n!pip install -qU langchain_community\n!pip install -qU langchain_experimental","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-18T15:23:51.390898Z","iopub.execute_input":"2025-01-18T15:23:51.391252Z","iopub.status.idle":"2025-01-18T15:24:21.308566Z","shell.execute_reply.started":"2025-01-18T15:23:51.391214Z","shell.execute_reply":"2025-01-18T15:24:21.307129Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain_openai import ChatOpenAI\nfrom langchain_anthropic import ChatAnthropic\nfrom kaggle_secrets import UserSecretsClient\nfrom langchain_core.output_parsers import StrOutputParser\n\n# Retrieve LLM API Key\nuser_secrets = UserSecretsClient()\n\n# Initialize the language model\n#model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, api_key=user_secrets.get_secret(\"my-openai-api-key\"))\n#model = ChatAnthropic(model=\"claude-3-5-sonnet-latest\", temperature=0, api_key=user_secrets.get_secret(\"my-anthropic-api-key\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T15:28:28.333108Z","iopub.execute_input":"2025-01-18T15:28:28.333658Z","iopub.status.idle":"2025-01-18T15:28:28.81996Z","shell.execute_reply.started":"2025-01-18T15:28:28.333583Z","shell.execute_reply":"2025-01-18T15:28:28.818886Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n## 1. Pydantic\n\n### 1.1 Using Standard Fields with Pydantic\nPydantic is a data validation and settings management library that uses Python type annotations. It is widely used for defining data models with type enforcement.","metadata":{}},{"cell_type":"code","source":"from typing import Optional\nfrom pydantic import BaseModel, Field\nfrom langchain_openai import ChatOpenAI\n\n# Define a Pydantic class for the joke schema\nclass Joke(BaseModel):\n    \"\"\"Joke to tell user.\"\"\"\n    setup: str = Field(description=\"The setup of the joke\")\n    punchline: str = Field(description=\"The punchline to the joke\")\n    rating: Optional[int] = Field(default=None, description=\"How funny the joke is, from 1 to 10\")\n\n# Initialize the ChatOpenAI model\n# model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, api_key=user_secrets.get_secret(\"my-openai-api-key\"))\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, base_url=\"http://20.243.34.136:2999/v1\",\n                        openai_api_key=\"sk-j8r3Pxztstd3wBjF8fEe44E63f69486bAdC2C4562bD1E1F3\")\n\n# Configure the model to return structured output using the Pydantic class\nstructured_llm = model.with_structured_output(Joke)\n\n# Generate a joke about cats\nresult = structured_llm.invoke(\"Tell me a joke about cats\")\n\n# Access fields using dot notation\nprint(\"Setup:\", result.setup)\nprint(\"Punchline:\", result.punchline)\nprint(\"Rating:\", result.rating)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T15:39:30.930853Z","iopub.execute_input":"2025-01-18T15:39:30.931366Z","iopub.status.idle":"2025-01-18T15:39:32.166573Z","shell.execute_reply.started":"2025-01-18T15:39:30.931324Z","shell.execute_reply":"2025-01-18T15:39:32.165263Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 1.2 Using Literal Field with Pydantic Models\nPydantic is a data validation and settings management library that uses Python type annotations. It provides a way to define data structures with type checking.","metadata":{}},{"cell_type":"code","source":"from langchain_openai import ChatOpenAI\nfrom pydantic import BaseModel\nfrom typing import Literal\n\n# Define a Pydantic model for structured output\nclass WeatherResponse(BaseModel):\n    location: str\n    temperature: float\n    unit: Literal[\"Celsius\", \"Fahrenheit\"]\n\n# Initialize the ChatOpenAI model\n# model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, api_key=user_secrets.get_secret(\"my-openai-api-key\"))\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, base_url=\"http://20.243.34.136:2999/v1\",\n                        openai_api_key=\"sk-j8r3Pxztstd3wBjF8fEe44E63f69486bAdC2C4562bD1E1F3\")\n\n# Configure the model to output structured data using the Pydantic model\nstructured_model = model.with_structured_output(WeatherResponse)\n\n# Generate a structured response by invoking the RunnableSequence\nresponse = structured_model.invoke(\"What's the weather in Paris?\")\nprint(f\"Location   : {response.location}\")\nprint(f\"Temperature: {response.temperature}\")\nprint(f\"Unit       : {response.unit}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T15:39:50.750779Z","iopub.execute_input":"2025-01-18T15:39:50.75113Z","iopub.status.idle":"2025-01-18T15:39:59.5273Z","shell.execute_reply.started":"2025-01-18T15:39:50.751102Z","shell.execute_reply":"2025-01-18T15:39:59.526123Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 1.3 Using Pydantic with Union\nYou can configure a model to handle multiple types of structured outputs by using a Union type in the schema. This allows the model to choose between different output formats based on the input or context.","metadata":{}},{"cell_type":"code","source":"from typing import Union, Optional\nfrom pydantic import BaseModel, Field\nfrom langchain_anthropic import ChatAnthropic\n\n# Define Pydantic classes for different response types\nclass Joke(BaseModel):\n    \"\"\"Joke to tell user.\"\"\"\n    setup: str = Field(description=\"The setup of the joke\")\n    punchline: str = Field(description=\"The punchline to the joke\")\n    rating: Optional[int] = Field(default=None, description=\"How funny the joke is, from 1 to 10\")\n\nclass Fact(BaseModel):\n    \"\"\"Fact to tell user.\"\"\"\n    topic: str = Field(description=\"The topic of the fact\")\n    fact: str = Field(description=\"The fact itself\")\n    source: Optional[str] = Field(default=None, description=\"The source of the fact\")\n\nclass FinalResponse(BaseModel):\n    \"\"\"Final response that can be either a joke or a fact.\"\"\"\n    response: Union[Joke, Fact]\n\n# Initialize the ChatAnthropic model\n# model = ChatAnthropic(model=\"claude-3-5-sonnet-latest\", temperature=0, api_key=user_secrets.get_secret(\"my-anthropic-api-key\"))\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, base_url=\"http://20.243.34.136:2999/v1\",\n                        openai_api_key=\"sk-j8r3Pxztstd3wBjF8fEe44E63f69486bAdC2C4562bD1E1F3\")\n\n# Configure the model to return structured output using the Union schema\nstructured_llm = model.with_structured_output(FinalResponse)\n\n# Generate a joke\nresult = structured_llm.invoke(\"Tell me a joke about cats\")\nprint(result)\n\n# Access fields of the nested response\nif isinstance(result.response, Joke):\n    print(\"Setup:\", result.response.setup)\n    print(\"Punchline:\", result.response.punchline)\n    print(\"Rating:\", result.response.rating)\n\n# Generate a fact\nresult = structured_llm.invoke(\"Tell me a fact about the moon\")\nprint(result)\n\nif isinstance(result.response, Fact):\n    print(\"Topic:\", result.response.topic)\n    print(\"Fact:\", result.response.fact)\n    print(\"Source:\", result.response.source)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T15:40:27.971956Z","iopub.execute_input":"2025-01-18T15:40:27.972358Z","iopub.status.idle":"2025-01-18T15:40:32.160217Z","shell.execute_reply.started":"2025-01-18T15:40:27.972328Z","shell.execute_reply":"2025-01-18T15:40:32.159133Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n## 2. TypedDict\n\n### 2.1 Using Standard Fields with TypedDict\nTypedDict allows for type annotations of dictionaries, making it suitable for defining structured outputs without the overhead of full-fledged models.","metadata":{}},{"cell_type":"code","source":"from typing import Optional\nfrom typing_extensions import Annotated, TypedDict\nfrom langchain_openai import ChatOpenAI\n\n# Define a TypedDict for the joke schema\nclass Joke(TypedDict):\n    \"\"\"Joke to tell user.\"\"\"\n    setup: Annotated[str, ..., \"The setup of the joke\"]\n    punchline: Annotated[str, ..., \"The punchline of the joke\"]\n    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]\n\n# Initialize the ChatOpenAI model\n# model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, api_key=user_secrets.get_secret(\"my-openai-api-key\"))\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, base_url=\"http://20.243.34.136:2999/v1\",\n                        openai_api_key=\"sk-j8r3Pxztstd3wBjF8fEe44E63f69486bAdC2C4562bD1E1F3\")\n\n# Configure the model to return structured output using the TypedDict\nstructured_llm = model.with_structured_output(Joke)\n\n# Generate a joke about cats\nresult = structured_llm.invoke(\"Tell me a joke about cats\")\nprint(result)\n\n# Access fields using key-based access\nprint(\"Setup:\", result[\"setup\"])\nprint(\"Punchline:\", result[\"punchline\"])\nprint(\"Rating:\", result[\"rating\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T15:40:54.115695Z","iopub.execute_input":"2025-01-18T15:40:54.116045Z","iopub.status.idle":"2025-01-18T15:40:55.057176Z","shell.execute_reply.started":"2025-01-18T15:40:54.116019Z","shell.execute_reply":"2025-01-18T15:40:55.056055Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.2 Using Literal Field with TypedDict\nTypedDict allows for the definition of dictionary-like data structures with type annotations, enhancing type safety and clarity.","metadata":{}},{"cell_type":"code","source":"from langchain_anthropic import ChatAnthropic\nfrom typing import TypedDict, Literal\n\n# Define a TypedDict for structured output\nclass ActionResponse(TypedDict):\n    action: Literal[\"create\", \"update\", \"delete\"]\n    target: str\n    details: str\n\n# Initialize the ChatAnthropic model\n# model = ChatAnthropic(model=\"claude-3-5-sonnet-latest\", temperature=0, api_key=user_secrets.get_secret(\"my-anthropic-api-key\"))\nmodel = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, base_url=\"http://20.243.34.136:2999/v1\",\n                        openai_api_key=\"sk-j8r3Pxztstd3wBjF8fEe44E63f69486bAdC2C4562bD1E1F3\")\n\n# Configure the model to output structured data using the TypedDict\nstructured_model = model.with_structured_output(ActionResponse)\n\n# Generate a structured response by invoking the RunnableSequence\nresponse = structured_model.invoke(\"Create a new user with the name John Doe.\")\nprint(f\"Action : {response['action']}\")\nprint(f\"Target : {response['target']}\")\nprint(f\"Details: {response['details']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T15:42:21.99699Z","iopub.execute_input":"2025-01-18T15:42:21.997332Z","iopub.status.idle":"2025-01-18T15:42:24.410355Z","shell.execute_reply.started":"2025-01-18T15:42:21.997309Z","shell.execute_reply":"2025-01-18T15:42:24.409327Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.3 Using TypedDict with Union\nSimilarly, you can use TypedDict with Union to handle multiple response types without the need for full-fledged models.","metadata":{}},{"cell_type":"code","source":"from typing import Union, Optional\nfrom typing_extensions import Annotated, TypedDict\nfrom langchain_openai import ChatOpenAI\n\n# Define TypedDict classes for different response types\nclass Joke(TypedDict):\n    \"\"\"Joke to tell user.\"\"\"\n    setup: Annotated[str, ..., \"The setup of the joke\"]\n    punchline: Annotated[str, ..., \"The punchline of the joke\"]\n    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]\n\nclass Fact(TypedDict):\n    \"\"\"Fact to tell user.\"\"\"\n    topic: Annotated[str, ..., \"The topic of the fact\"]\n    fact: Annotated[str, ..., \"The fact itself\"]\n    source: Annotated[Optional[str], None, \"The source of the fact\"]\n\nclass FinalResponse(TypedDict):\n    \"\"\"Final response that can be either a joke or a fact.\"\"\"\n    response: Union[Joke, Fact]\n\n# Initialize the ChatOpenAI model\n# model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, api_key=user_secrets.get_secret(\"my-openai-api-key\"))\nmodel = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, base_url=\"http://20.243.34.136:2999/v1\",\n                        openai_api_key=\"sk-j8r3Pxztstd3wBjF8fEe44E63f69486bAdC2C4562bD1E1F3\")\n\n# Configure the model to return structured output using the Union schema\nstructured_llm = model.with_structured_output(FinalResponse)\n\n# Generate a joke\nresult = structured_llm.invoke(\"Tell me a joke about cats\")\nprint(result)\n\n# Access fields of the nested response\nif isinstance(result[\"response\"], dict):  # Check if response is a dictionary (TypedDict)\n    if \"setup\" in result[\"response\"]:  # Check if it's a Joke\n        print(\"Setup:\", result[\"response\"][\"setup\"])\n        print(\"Punchline:\", result[\"response\"][\"punchline\"])\n        print(\"Rating:\", result[\"response\"][\"rating\"])\n\n# Generate a fact\nresult = structured_llm.invoke(\"Tell me a fact about the moon\")\nprint(result)\n\n# Access fields of the nested response\nif isinstance(result[\"response\"], dict):  # Check if response is a dictionary (TypedDict)\n    if \"topic\" in result[\"response\"]:  # Check if it's a Fact\n        print(\"Topic:\", result[\"response\"][\"topic\"])\n        print(\"Fact:\", result[\"response\"][\"fact\"])\n        print(\"Source:\", result[\"response\"][\"source\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T15:42:46.833543Z","iopub.execute_input":"2025-01-18T15:42:46.833975Z","iopub.status.idle":"2025-01-18T15:42:49.377026Z","shell.execute_reply.started":"2025-01-18T15:42:46.833944Z","shell.execute_reply":"2025-01-18T15:42:49.375658Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n## 3. JSON Schema\n\n### 3.1 Using Standard Fields with JSON Schema\nJSON Schema provides a way to describe the structure and validation constraints of JSON data. It's especially useful for interoperability across different systems.","metadata":{}},{"cell_type":"code","source":"from langchain_anthropic import ChatAnthropic\n\n# Define a JSON Schema for the joke\njson_schema = {\n    \"title\": \"joke\",\n    \"description\": \"Joke to tell user.\",\n    \"type\": \"object\",\n    \"properties\": {\n        \"setup\": {\"type\": \"string\", \"description\": \"The setup of the joke\"},\n        \"punchline\": {\"type\": \"string\", \"description\": \"The punchline to the joke\"},\n        \"rating\": {\"type\": \"integer\", \"description\": \"How funny the joke is, from 1 to 10\", \"default\": None},\n    },\n    \"required\": [\"setup\", \"punchline\"],\n}\n\n# Initialize the ChatAnthropic model\n# model = ChatAnthropic(model=\"claude-3-5-sonnet-latest\", temperature=0, api_key=user_secrets.get_secret(\"my-anthropic-api-key\"))\nmodel = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, base_url=\"http://20.243.34.136:2999/v1\",\n                        openai_api_key=\"sk-j8r3Pxztstd3wBjF8fEe44E63f69486bAdC2C4562bD1E1F3\")\n\n# Configure the model to return structured output using the JSON Schema\nstructured_llm = model.with_structured_output(json_schema)\n\n# Generate a joke about cats\nresult = structured_llm.invoke(\"Tell me a joke about cats\")\nprint(result)\n\n# Access fields using key-based access\nprint(\"Setup:\", result[\"setup\"])\nprint(\"Punchline:\", result[\"punchline\"])\nprint(\"Rating:\", result[\"rating\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T15:43:13.193776Z","iopub.execute_input":"2025-01-18T15:43:13.194201Z","iopub.status.idle":"2025-01-18T15:43:15.360919Z","shell.execute_reply.started":"2025-01-18T15:43:13.194168Z","shell.execute_reply":"2025-01-18T15:43:15.359839Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3.2 Using JSON Schema with Union\nJSON Schema also supports Union through the `oneOf` keyword, enabling models to return one of several predefined schemas.","metadata":{}},{"cell_type":"code","source":"from langchain_anthropic import ChatAnthropic\n\n# Define JSON Schemas for different response types\njoke_schema = {\n    \"title\": \"joke\",\n    \"description\": \"Joke to tell user.\",\n    \"type\": \"object\",\n    \"properties\": {\n        \"setup\": {\"type\": \"string\", \"description\": \"The setup of the joke\"},\n        \"punchline\": {\"type\": \"string\", \"description\": \"The punchline to the joke\"},\n        \"rating\": {\"type\": \"integer\", \"description\": \"How funny the joke is, from 1 to 10\", \"default\": None},\n    },\n    \"required\": [\"setup\", \"punchline\"],\n}\n\nfact_schema = {\n    \"title\": \"fact\",\n    \"description\": \"Fact to tell user.\",\n    \"type\": \"object\",\n    \"properties\": {\n        \"topic\": {\"type\": \"string\", \"description\": \"The topic of the fact\"},\n        \"fact\": {\"type\": \"string\", \"description\": \"The fact itself\"},\n        \"source\": {\"type\": \"string\", \"description\": \"The source of the fact\", \"default\": None},\n    },\n    \"required\": [\"topic\", \"fact\"],\n}\n\nfinal_schema = {\n    \"title\": \"final_response\",\n    \"description\": \"Final response that can be either a joke or a fact.\",\n    \"type\": \"object\",\n    \"properties\": {\n        \"response\": {\n            \"oneOf\": [joke_schema, fact_schema],\n        },\n    },\n    \"required\": [\"response\"],\n}\n\n# Initialize the ChatAnthropic model\n# model = ChatAnthropic(model=\"claude-3-5-sonnet-latest\", temperature=0, api_key=user_secrets.get_secret(\"my-anthropic-api-key\"))\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, base_url=\"http://20.243.34.136:2999/v1\",\n                        openai_api_key=\"sk-j8r3Pxztstd3wBjF8fEe44E63f69486bAdC2C4562bD1E1F3\")\n\n# Configure the model to return structured output using the JSON Schema\nstructured_llm = model.with_structured_output(final_schema)\n\n# Generate a joke\nresult = structured_llm.invoke(\"Tell me a joke about cats\")\nprint(result)\n\n# Generate a fact\nresult = structured_llm.invoke(\"Tell me a fact about the moon\")\nprint(result)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T15:43:37.888674Z","iopub.execute_input":"2025-01-18T15:43:37.889092Z","iopub.status.idle":"2025-01-18T15:43:42.709267Z","shell.execute_reply.started":"2025-01-18T15:43:37.88906Z","shell.execute_reply":"2025-01-18T15:43:42.708227Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n## 4. Advanced Techniques\n\n### 4.1 Choosing Between Multiple Schemas\nIn scenarios where different types of responses are expected, defining multiple schemas and allowing the model to choose between them ensures flexibility and adaptability.","metadata":{}},{"cell_type":"code","source":"from typing import Union\nfrom pydantic import BaseModel, Field\nfrom langchain_openai import ChatOpenAI\n\n# Define Pydantic classes for different response types\nclass Joke(BaseModel):\n    \"\"\"Joke to tell user.\"\"\"\n    setup: str = Field(description=\"The setup of the joke\")\n    punchline: str = Field(description=\"The punchline to the joke\")\n    rating: Optional[int] = Field(default=None, description=\"How funny the joke is, from 1 to 10\")\n\nclass ConversationalResponse(BaseModel):\n    \"\"\"Respond in a conversational manner. Be kind and helpful.\"\"\"\n    response: str = Field(description=\"A conversational response to the user's query\")\n\nclass FinalResponse(BaseModel):\n    final_output: Union[Joke, ConversationalResponse]\n\n# Initialize the ChatOpenAI model\n# model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, api_key=user_secrets.get_secret(\"my-openai-api-key\"))\nmodel = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, base_url=\"http://20.243.34.136:2999/v1\",\n                        openai_api_key=\"sk-j8r3Pxztstd3wBjF8fEe44E63f69486bAdC2C4562bD1E1F3\")\n\n# Configure the model to return structured output using the Union schema\nstructured_llm = model.with_structured_output(FinalResponse)\n\n# Generate a joke about cats\nresult = structured_llm.invoke(\"Tell me a joke about cats\")\nprint(result)\n\n# Generate a conversational response\nresult = structured_llm.invoke(\"How are you today?\")\nprint(result)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T15:44:05.541566Z","iopub.execute_input":"2025-01-18T15:44:05.542001Z","iopub.status.idle":"2025-01-18T15:44:07.498724Z","shell.execute_reply.started":"2025-01-18T15:44:05.541965Z","shell.execute_reply":"2025-01-18T15:44:07.497745Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4.2 Streaming Structured Output\nStreaming allows for the gradual delivery of structured data as it is generated, which is beneficial for large responses or real-time applications.","metadata":{}},{"cell_type":"code","source":"from typing_extensions import Annotated, TypedDict\nfrom langchain_anthropic import ChatAnthropic\n\n# Define a TypedDict for the joke schema\nclass Joke(TypedDict):\n    \"\"\"Joke to tell user.\"\"\"\n    setup: Annotated[str, ..., \"The setup of the joke\"]\n    punchline: Annotated[str, ..., \"The punchline of the joke\"]\n    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]\n\n# Initialize the ChatAnthropic model\n# model = ChatAnthropic(model=\"claude-3-5-sonnet-latest\", temperature=0, api_key=user_secrets.get_secret(\"my-anthropic-api-key\"))\nmodel = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, base_url=\"http://20.243.34.136:2999/v1\",\n                        openai_api_key=\"sk-j8r3Pxztstd3wBjF8fEe44E63f69486bAdC2C4562bD1E1F3\")\n\n# Configure the model to return structured output using the TypedDict\nstructured_llm = model.with_structured_output(Joke)\n\n# Stream the output for a joke about cats\nfor chunk in structured_llm.stream(\"Tell me a joke about cats\"):\n    print(chunk)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T15:44:23.159222Z","iopub.execute_input":"2025-01-18T15:44:23.159573Z","iopub.status.idle":"2025-01-18T15:44:25.587328Z","shell.execute_reply.started":"2025-01-18T15:44:23.159547Z","shell.execute_reply":"2025-01-18T15:44:25.586202Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4.3 Few-Shot Prompting with Structured Output\nFew-shot prompting involves providing the model with examples to guide its responses. When combined with structured outputs, it enhances the model's ability to generate consistent and accurate data structures.","metadata":{}},{"cell_type":"code","source":"from langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\n\n# Define a system message with few-shot examples\nsystem = \"\"\"You are a hilarious comedian. Your specialty is knock-knock jokes. \\\nReturn a joke which has the setup (the response to \"Who's there?\") and the final punchline (the response to \"<setup> who?\").\n\nHere are some examples of jokes:\n\nexample_user: Tell me a joke about planes\nexample_assistant: {{\"setup\": \"Why don't planes ever get tired?\", \"punchline\": \"Because they have rest wings!\", \"rating\": 2}}\n\nexample_user: Tell me another joke about planes\nexample_assistant: {{\"setup\": \"Cargo\", \"punchline\": \"Cargo 'vroom vroom', but planes go 'zoom zoom'!\", \"rating\": 10}}\n\nexample_user: Now about caterpillars\nexample_assistant: {{\"setup\": \"Caterpillar\", \"punchline\": \"Caterpillar really slow, but watch me turn into a butterfly and steal the show!\", \"rating\": 5}}\"\"\"\n\n# Create a ChatPromptTemplate\nprompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", \"{input}\")])\n\n# Initialize the ChatOpenAI model\n# model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, api_key=user_secrets.get_secret(\"my-openai-api-key\"))\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, base_url=\"http://20.243.34.136:2999/v1\",\n                        openai_api_key=\"sk-j8r3Pxztstd3wBjF8fEe44E63f69486bAdC2C4562bD1E1F3\")\n\n# Configure the model to return structured output\nstructured_llm = model.with_structured_output(Joke)\n\n# Combine the prompt and structured LLM\nfew_shot_structured_llm = prompt | structured_llm\n\n# Generate a joke about woodpeckers\nresult = few_shot_structured_llm.invoke(\"what's something funny about woodpeckers\")\nprint(result)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T15:44:43.352539Z","iopub.execute_input":"2025-01-18T15:44:43.35291Z","iopub.status.idle":"2025-01-18T15:44:44.251103Z","shell.execute_reply.started":"2025-01-18T15:44:43.352875Z","shell.execute_reply":"2025-01-18T15:44:44.249782Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Conclusion\n\nStructured outputs significantly enhance the reliability and efficiency of interactions with large language models. By enforcing predefined data formats, developers can ensure seamless integration with various systems, reduce ambiguity, and automate data validation processes. LangChain's `.with_structured_output()` method provides a versatile and powerful toolset for implementing structured outputs using Pydantic, TypedDict, or JSON Schema, catering to a wide range of applications and use cases.\n\nWhether you're building enterprise-grade applications, APIs, or interactive systems, leveraging structured outputs can lead to more predictable and maintainable solutions. The practical examples showcased in this article demonstrate the ease and flexibility with which structured outputs can be integrated into your workflows, paving the way for more robust and scalable AI-driven applications.","metadata":{}}]}