{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **LangChain `MultiQueryRetriever` Quick Reference**\n\n## **Introduction**\n\nThe `MultiQueryRetriever` is a powerful tool in the LangChain framework designed to enhance document retrieval by generating multiple queries from a single input query. Using a language model (LLM), it creates alternative versions of the original query, retrieves documents for each version, and returns the unique union of all retrieved documents. This approach helps overcome limitations in traditional retrieval methods, such as those relying solely on distance-based similarity search, by providing a more comprehensive set of results.\n\nThis article explores the capabilities of the `MultiQueryRetriever` through practical examples, covering key functionalities such as initialization, query generation, document retrieval, streaming, and advanced features like retry mechanisms and lifecycle listeners. Whether you're building a question-answering system, a knowledge base, or a search engine, the `MultiQueryRetriever` can significantly improve the relevance and diversity of your search results.","metadata":{}},{"cell_type":"markdown","source":"---\n\n## Preparation\n\n### Installing Required Libraries\nThis section installs the necessary Python libraries for working with LangChain, OpenAI embeddings, and Chroma vector store. These libraries include:\n- `langchain-openai`: Provides integration with OpenAI's embedding models.\n- `langchain_community`: Contains community-contributed modules and tools for LangChain.\n- `langchain_experimental`: Includes experimental features and utilities for LangChain.\n- `langchain-chroma`: Enables integration with the Chroma vector database.\n- `chromadb`: The core library for the Chroma vector database.","metadata":{}},{"cell_type":"code","source":"!pip install -qU langchain-openai\n!pip install -qU langchain_community\n!pip install -qU langchain_experimental\n!pip install -qU langchain-chroma>=0.1.2\n!pip install -qU chromadb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T12:14:00.743406Z","iopub.execute_input":"2025-01-19T12:14:00.743657Z","iopub.status.idle":"2025-01-19T12:15:13.195969Z","shell.execute_reply.started":"2025-01-19T12:14:00.743634Z","shell.execute_reply":"2025-01-19T12:15:13.194534Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Initializing OpenAI Embeddings\nThis section demonstrates how to securely fetch an OpenAI API key using Kaggle's `UserSecretsClient` and initialize the OpenAI embedding model. The `OpenAIEmbeddings` class is used to create an embedding model instance, which will be used to convert text into numerical embeddings.\n\nKey steps:\n1. **Fetch API Key**: The OpenAI API key is securely retrieved using Kaggle's `UserSecretsClient`.\n2. **Initialize Embeddings**: The `OpenAIEmbeddings` class is initialized with the `text-embedding-3-small` model and the fetched API key.\n\nThis setup ensures that the embedding model is ready for use in downstream tasks, such as caching embeddings or creating vector stores.","metadata":{}},{"cell_type":"code","source":"from langchain_openai import OpenAIEmbeddings, ChatOpenAI\nfrom kaggle_secrets import UserSecretsClient\n\n# Fetch API key securely\nuser_secrets = UserSecretsClient()\nmy_api_key = user_secrets.get_secret(\"api-key-openai\")\n\n# Initialize OpenAI embeddings\nembed = OpenAIEmbeddings(model=\"text-embedding-3-small\", api_key=my_api_key)\n\n# Initialize LLM\nmodel = ChatOpenAI(model=\"gpt-4o-mini\", api_key=my_api_key)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T12:47:37.99058Z","iopub.execute_input":"2025-01-19T12:47:37.990973Z","iopub.status.idle":"2025-01-19T12:47:38.413363Z","shell.execute_reply.started":"2025-01-19T12:47:37.990947Z","shell.execute_reply":"2025-01-19T12:47:38.412305Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n## **1. Initialization and Configuration**\n\n### **Example 1: Basic Initialization**\nThis example demonstrates how to initialize the `MultiQueryRetriever` with a vector store (`Chroma`) and an embedding model (`OpenAIEmbeddings`). It also adds sample documents to the vector store and retrieves relevant documents for a query.","metadata":{}},{"cell_type":"code","source":"from langchain_chroma import Chroma\nfrom langchain.retrievers.multi_query import MultiQueryRetriever\n\n# Initialize vector store and embeddings\nvectorstore = Chroma(embedding_function=embed)\nretriever = vectorstore.as_retriever()\n\n# Initialize MultiQueryRetriever\nmulti_query_retriever = MultiQueryRetriever.from_llm(\n    retriever=retriever,\n    llm=model,\n    include_original=True  # Include the original query\n)\n\n# Add documents to the vector store (for demonstration)\ndocuments = [\n    \"Exercise improves cardiovascular health.\",\n    \"A healthy diet reduces the risk of chronic diseases.\",\n    \"Meditation helps reduce stress and anxiety.\"\n]\nvectorstore.add_texts(documents)\n\n# Use the retriever to fetch documents using `invoke`\nquery = \"What are the benefits of exercise?\"\nrelevant_docs = multi_query_retriever.invoke(query)\n\nprint(\"Retrieved Documents:\")\nfor doc in relevant_docs:\n    print(doc.page_content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T12:47:43.389318Z","iopub.execute_input":"2025-01-19T12:47:43.389695Z","iopub.status.idle":"2025-01-19T12:47:47.329482Z","shell.execute_reply.started":"2025-01-19T12:47:43.389657Z","shell.execute_reply":"2025-01-19T12:47:47.328348Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Example 2: Custom Prompt Template**\nThis example shows how to use a custom prompt template with the `MultiQueryRetriever`. The custom prompt generates alternative versions of the input query, and the retriever fetches documents based on these queries.","metadata":{}},{"cell_type":"code","source":"from langchain_core.prompts import PromptTemplate\n\n# Define a custom prompt template\ncustom_prompt = PromptTemplate(\n    input_variables=[\"question\"],\n    template=\"Generate 3 different versions of this question: {question}\"\n)\n\n# Initialize MultiQueryRetriever with custom prompt\nmulti_query_retriever_custom = MultiQueryRetriever.from_llm(\n    retriever=retriever,\n    llm=model,\n    prompt=custom_prompt,\n    include_original=False  # Exclude the original query\n)\n\n# Use the custom retriever to fetch documents\nquery = \"How does meditation improve mental health?\"\nrelevant_docs = multi_query_retriever_custom.get_relevant_documents(query)\n\nprint(\"Retrieved Documents (Custom Prompt):\")\nfor doc in relevant_docs:\n    print(doc.page_content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T12:47:53.397065Z","iopub.execute_input":"2025-01-19T12:47:53.397389Z","iopub.status.idle":"2025-01-19T12:47:55.549492Z","shell.execute_reply.started":"2025-01-19T12:47:53.397362Z","shell.execute_reply":"2025-01-19T12:47:55.548342Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n## **2. Document Retrieval**\n\n### **Example 1: Retrieve Documents for a Single Query**\nThis example retrieves documents relevant to a single query using the `get_relevant_documents` method.","metadata":{}},{"cell_type":"code","source":"query = \"What are the benefits of exercise?\"\nrelevant_docs = multi_query_retriever.get_relevant_documents(query)\n\nprint(\"Retrieved Documents:\")\nfor doc in relevant_docs:\n    print(doc.page_content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T12:48:08.355495Z","iopub.execute_input":"2025-01-19T12:48:08.355983Z","iopub.status.idle":"2025-01-19T12:48:11.36389Z","shell.execute_reply.started":"2025-01-19T12:48:08.355943Z","shell.execute_reply":"2025-01-19T12:48:11.362608Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Example 2: Retrieve Documents for Multiple Queries**\nThis example demonstrates how to retrieve documents for multiple queries in a loop. It processes each query individually and prints the retrieved documents.","metadata":{}},{"cell_type":"code","source":"queries = [\n    \"What are the benefits of exercise?\",\n    \"How does meditation improve mental health?\"\n]\n\nfor query in queries:\n    relevant_docs = multi_query_retriever.get_relevant_documents(query)\n    print(f\"Retrieved Documents for: {query}\")\n    for doc in relevant_docs:\n        print(doc.page_content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T12:48:24.666315Z","iopub.execute_input":"2025-01-19T12:48:24.666703Z","iopub.status.idle":"2025-01-19T12:48:29.975167Z","shell.execute_reply.started":"2025-01-19T12:48:24.666673Z","shell.execute_reply":"2025-01-19T12:48:29.974119Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Example 3: Retrieve Unique Documents**\nThis example retrieves documents for a query and ensures that the results are unique using the `unique_union` method.","metadata":{}},{"cell_type":"code","source":"query = \"What are the benefits of exercise?\"\nrelevant_docs = multi_query_retriever.get_relevant_documents(query)\nunique_docs = multi_query_retriever.unique_union(relevant_docs)\n\nprint(\"Unique Retrieved Documents:\")\nfor doc in unique_docs:\n    print(doc.page_content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T12:48:37.706525Z","iopub.execute_input":"2025-01-19T12:48:37.706941Z","iopub.status.idle":"2025-01-19T12:48:39.892208Z","shell.execute_reply.started":"2025-01-19T12:48:37.706912Z","shell.execute_reply":"2025-01-19T12:48:39.890904Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n## **3. Invocation Methods**\n\n### **Example 1: Use `invoke` for Single Query**\nThis example demonstrates how to use the `invoke` method to retrieve documents for a single query. This is the recommended way to retrieve documents synchronously.","metadata":{}},{"cell_type":"code","source":"query = \"What are the benefits of exercise?\"\nrelevant_docs = multi_query_retriever.invoke(query)\n\nprint(\"Retrieved Documents (via invoke):\")\nfor doc in relevant_docs:\n    print(doc.page_content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T12:48:50.535328Z","iopub.execute_input":"2025-01-19T12:48:50.535763Z","iopub.status.idle":"2025-01-19T12:48:52.838612Z","shell.execute_reply.started":"2025-01-19T12:48:50.535732Z","shell.execute_reply":"2025-01-19T12:48:52.837401Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Example 2: Use `batch` for Multiple Queries**\nThis example shows how to use the `batch` method to process multiple queries in parallel. It retrieves documents for each query and prints the results.","metadata":{}},{"cell_type":"code","source":"queries = [\n    \"What are the benefits of exercise?\",\n    \"How does meditation improve mental health?\"\n]\nbatch_results = multi_query_retriever.batch(queries)\n\nprint(\"Batch Results:\")\nfor i, result in enumerate(batch_results):\n    print(f\"Results for Query {i + 1}:\")\n    for doc in result:\n        print(doc.page_content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T12:49:03.465327Z","iopub.execute_input":"2025-01-19T12:49:03.465793Z","iopub.status.idle":"2025-01-19T12:49:06.529225Z","shell.execute_reply.started":"2025-01-19T12:49:03.465757Z","shell.execute_reply":"2025-01-19T12:49:06.528012Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Example 3: Use `batch_as_completed` for Parallel Processing**\nThis example demonstrates how to use the `batch_as_completed` method to process multiple queries in parallel and yield results as they complete.","metadata":{}},{"cell_type":"code","source":"queries = [\n    \"What are the benefits of exercise?\",\n    \"How does meditation improve mental health?\"\n]\nfor idx, result in multi_query_retriever.batch_as_completed(queries):\n    print(f\"Results for Query {idx + 1}:\")\n    for doc in result:\n        print(doc.page_content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T12:49:17.485498Z","iopub.execute_input":"2025-01-19T12:49:17.48589Z","iopub.status.idle":"2025-01-19T12:49:23.320295Z","shell.execute_reply.started":"2025-01-19T12:49:17.485857Z","shell.execute_reply":"2025-01-19T12:49:23.319199Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n## **4. Query Generation**\n\n### **Example 1: Generate Queries for a Single Question**\nThis example demonstrates how to generate multiple queries from a single input question using the `generate_queries` method. It uses a callback manager for logging.\n","metadata":{}},{"cell_type":"code","source":"from langchain_core.callbacks.manager import CallbackManagerForRetrieverRun\nfrom langchain_core.callbacks.base import BaseCallbackHandler\nimport uuid\n\n# Define a question\nquestion = \"What are the benefits of a healthy diet?\"\n\n# Create a basic callback handler (optional)\nclass SimpleCallbackHandler(BaseCallbackHandler):\n    def on_retriever_start(self, serialized, query, **kwargs):\n        print(f\"Retriever started with query: {query}\")\n\n# Initialize CallbackManagerForRetrieverRun\nrun_id = str(uuid.uuid4())  # Generate a unique run ID\nhandlers = [SimpleCallbackHandler()]  # Add your callback handlers\ninheritable_handlers = []  # Inheritable handlers (optional)\n\nrun_manager = CallbackManagerForRetrieverRun(\n    run_id=run_id,\n    handlers=handlers,\n    inheritable_handlers=inheritable_handlers\n)\n\n# Generate queries using MultiQueryRetriever\ngenerated_queries = multi_query_retriever.generate_queries(\n    question=question,\n    run_manager=run_manager  # Provide the callback manager\n)\n\nprint(\"Generated Queries:\")\nfor query in generated_queries:\n    print(query)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T12:30:23.294506Z","iopub.execute_input":"2025-01-19T12:30:23.294866Z","iopub.status.idle":"2025-01-19T12:30:25.017676Z","shell.execute_reply.started":"2025-01-19T12:30:23.29484Z","shell.execute_reply":"2025-01-19T12:30:25.016659Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Example 2: Generate Queries for Multiple Questions**\nThis example demonstrates how to generate queries for multiple questions in a loop. It initializes a new callback manager for each question.\n","metadata":{}},{"cell_type":"code","source":"from langchain_core.callbacks.manager import CallbackManagerForRetrieverRun\nfrom langchain_core.callbacks.base import BaseCallbackHandler\nimport uuid\n\n# Define a list of questions\nquestions = [\n    \"What are the benefits of exercise?\",\n    \"How does meditation improve mental health?\"\n]\n\n# Create a basic callback handler (optional)\nclass SimpleCallbackHandler(BaseCallbackHandler):\n    def on_retriever_start(self, serialized, query, **kwargs):\n        print(f\"Retriever started with query: {query}\")\n\n# Generate queries for each question\nfor question in questions:\n    # Initialize CallbackManagerForRetrieverRun for each question\n    run_id = str(uuid.uuid4())  # Generate a unique run ID\n    handlers = [SimpleCallbackHandler()]  # Add your callback handlers\n    inheritable_handlers = []  # Inheritable handlers (optional)\n\n    run_manager = CallbackManagerForRetrieverRun(\n        run_id=run_id,\n        handlers=handlers,\n        inheritable_handlers=inheritable_handlers\n    )\n\n    # Generate queries using MultiQueryRetriever\n    generated_queries = multi_query_retriever.generate_queries(\n        question=question,\n        run_manager=run_manager  # Provide the callback manager\n    )\n    print(f\"Generated Queries for: {question}\")\n    for query in generated_queries:\n        print(query)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T12:32:08.763301Z","iopub.execute_input":"2025-01-19T12:32:08.763714Z","iopub.status.idle":"2025-01-19T12:32:11.109828Z","shell.execute_reply.started":"2025-01-19T12:32:08.763681Z","shell.execute_reply":"2025-01-19T12:32:11.108423Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n## **5. Retry Mechanism and Lifecycle Listeners**\n\n### **Example 1: Add Retry Mechanism**\nThis example demonstrates how to add a retry mechanism to the `MultiQueryRetriever`. The retry mechanism will retry the operation up to 3 times if an exception occurs.","metadata":{}},{"cell_type":"code","source":"retriever_with_retry = multi_query_retriever.with_retry(\n    retry_if_exception_type=(Exception,),  # Retry on any exception\n    stop_after_attempt=3  # Maximum number of retries\n)\n\nquery = \"What are the benefits of exercise?\"\nrelevant_docs = retriever_with_retry.invoke(query)\n\nprint(\"Retrieved Documents (with retry):\")\nfor doc in relevant_docs:\n    print(doc.page_content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T12:43:16.166345Z","iopub.execute_input":"2025-01-19T12:43:16.166771Z","iopub.status.idle":"2025-01-19T12:43:18.875856Z","shell.execute_reply.started":"2025-01-19T12:43:16.166741Z","shell.execute_reply":"2025-01-19T12:43:18.874709Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Example 2: Add Lifecycle Listeners**\nThis example shows how to add lifecycle listeners to the `MultiQueryRetriever`. The `on_start` and `on_end` listeners are triggered when the retriever starts and finishes processing a query, respectively.","metadata":{}},{"cell_type":"code","source":"def on_start(run_obj):\n    print(f\"Retriever started with input: {run_obj.input}\")\n\ndef on_end(run_obj):\n    print(f\"Retriever finished with output: {run_obj.output}\")\n\nretriever_with_listeners = multi_query_retriever.with_listeners(\n    on_start=on_start,\n    on_end=on_end\n)\n\nquery = \"What are the benefits of exercise?\"\nrelevant_docs = retriever_with_listeners.invoke(query)\n\nprint(\"Retrieved Documents (with listeners):\")\nfor doc in relevant_docs:\n    print(doc.page_content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T12:43:26.846252Z","iopub.execute_input":"2025-01-19T12:43:26.846693Z","iopub.status.idle":"2025-01-19T12:43:29.092023Z","shell.execute_reply.started":"2025-01-19T12:43:26.846659Z","shell.execute_reply":"2025-01-19T12:43:29.090783Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Example 3: Combine Retry and Listeners**\nThis example combines the retry mechanism and lifecycle listeners into a single retriever. The retriever will retry on exceptions and trigger the `on_start` and `on_end` listeners during its execution.","metadata":{}},{"cell_type":"code","source":"retriever_with_retry_and_listeners = multi_query_retriever.with_retry(\n    retry_if_exception_type=(Exception,),\n    stop_after_attempt=3\n).with_listeners(\n    on_start=on_start,\n    on_end=on_end\n)\n\nquery = \"What are the benefits of exercise?\"\nrelevant_docs = retriever_with_retry_and_listeners.invoke(query)\n\nprint(\"Retrieved Documents (with retry and listeners):\")\nfor doc in relevant_docs:\n    print(doc.page_content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T12:43:35.728097Z","iopub.execute_input":"2025-01-19T12:43:35.728428Z","iopub.status.idle":"2025-01-19T12:43:38.305269Z","shell.execute_reply.started":"2025-01-19T12:43:35.728404Z","shell.execute_reply":"2025-01-19T12:43:38.304316Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n## **6. Good Practices**\n\n### **Key Takeaways**\n\n- **Building a Vector Database**: Load, split, and embed documents to create a searchable vector database.\n- **Simple Usage of MultiQueryRetriever**: Use the retriever with a language model to generate multiple queries and retrieve documents.\n- **Customizing the Prompt and Output Parser**: Define custom prompts and parsers to tailor the query generation process for specific use cases.\n\n### **Code for Building a Sample Vector Database**\nThis code demonstrates how to build a vector database using a blog post as the data source. It loads the blog post, splits it into smaller chunks, and creates a vector database using `Chroma` and `OpenAIEmbeddings`.","metadata":{}},{"cell_type":"code","source":"# Build a sample vectorDB\nfrom langchain_chroma import Chroma\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\n# Load blog post\nloader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\ndata = loader.load()\n\n# Split\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\nsplits = text_splitter.split_documents(data)\n\n# VectorDB\nembedding = OpenAIEmbeddings(model=\"text-embedding-3-small\", api_key=my_api_key)\nvectordb = Chroma.from_documents(documents=splits, embedding=embedding)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T12:58:21.388256Z","iopub.execute_input":"2025-01-19T12:58:21.388664Z","iopub.status.idle":"2025-01-19T12:58:25.165925Z","shell.execute_reply.started":"2025-01-19T12:58:21.388632Z","shell.execute_reply":"2025-01-19T12:58:25.164734Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Code for Simple Usage of MultiQueryRetriever**\nThis example shows how to use the `MultiQueryRetriever` with a pre-built vector database. It initializes the retriever with a language model (`ChatOpenAI`) and retrieves documents for a specific question. Logging is enabled to display the generated queries.","metadata":{}},{"cell_type":"code","source":"from langchain.retrievers.multi_query import MultiQueryRetriever\nfrom langchain_openai import ChatOpenAI\n\n# Initial LLM\nmodel = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, api_key=my_api_key)\n\nquestion = \"What are the approaches to Task Decomposition?\"\nretriever_from_llm = MultiQueryRetriever.from_llm(retriever=vectordb.as_retriever(), llm=model)\n\n# Set logging for the queries\nimport logging\n\nlogging.basicConfig()\nlogging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)\n\nunique_docs = retriever_from_llm.invoke(question)\nlen(unique_docs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T12:59:33.661761Z","iopub.execute_input":"2025-01-19T12:59:33.662114Z","iopub.status.idle":"2025-01-19T12:59:36.429187Z","shell.execute_reply.started":"2025-01-19T12:59:33.662088Z","shell.execute_reply":"2025-01-19T12:59:36.428253Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Code for Customizing the Prompt and Output Parser**\nThis example demonstrates how to customize the prompt and output parser for the `MultiQueryRetriever`. It defines a custom prompt template and an output parser to generate multiple queries from a single input question. The retriever is then used to fetch documents based on the generated queries.","metadata":{}},{"cell_type":"code","source":"from typing import List\nfrom langchain_core.output_parsers import BaseOutputParser\nfrom langchain_core.prompts import PromptTemplate\nfrom pydantic import BaseModel, Field\n\n# Output parser will split the LLM result into a list of queries\nclass LineListOutputParser(BaseOutputParser[List[str]]):\n    \"\"\"Output parser for a list of lines.\"\"\"\n\n    def parse(self, text: str) -> List[str]:\n        lines = text.strip().split(\"\\n\")\n        return list(filter(None, lines))  # Remove empty lines\n\noutput_parser = LineListOutputParser()\n\nQUERY_PROMPT = PromptTemplate(\n    input_variables=[\"question\"],\n    template=\"\"\"You are an AI language model assistant. Your task is to generate five \n    different versions of the given user question to retrieve relevant documents from a vector \n    database. By generating multiple perspectives on the user question, your goal is to help\n    the user overcome some of the limitations of the distance-based similarity search. \n    Provide these alternative questions separated by newlines.\n    Original question: {question}\"\"\",\n)\n\n# Initial LLM\nmodel = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, api_key=my_api_key)\n\n# Chain\nllm_chain = QUERY_PROMPT | model | output_parser\n\n# Other inputs\nquestion = \"What are the approaches to Task Decomposition?\"\n\n# Run\nretriever = MultiQueryRetriever(\n    retriever=vectordb.as_retriever(), llm_chain=llm_chain, parser_key=\"lines\"\n)  # \"lines\" is the key (attribute name) of the parsed output\n\n# Results\nunique_docs = retriever.invoke(\"What does the course say about regression?\")\nlen(unique_docs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T13:00:16.686197Z","iopub.execute_input":"2025-01-19T13:00:16.686561Z","iopub.status.idle":"2025-01-19T13:00:19.837349Z","shell.execute_reply.started":"2025-01-19T13:00:16.686533Z","shell.execute_reply":"2025-01-19T13:00:19.836197Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Conclusion**\n\nThe `MultiQueryRetriever` is a versatile and robust component of the LangChain framework, offering advanced document retrieval capabilities through query generation and retrieval optimization. By generating multiple versions of a query and retrieving documents for each, it ensures a more comprehensive and diverse set of results, making it ideal for applications requiring high-quality search functionality.\n\nThrough the examples provided in this article, we’ve demonstrated how to initialize the retriever, generate queries, retrieve documents, and leverage advanced features like streaming, retry mechanisms, and lifecycle listeners. These tools empower developers to build more resilient and efficient retrieval systems, capable of handling complex queries and delivering accurate results.\n\nWhether you're working on a small project or a large-scale application, the `MultiQueryRetriever` provides the flexibility and power needed to enhance your document retrieval workflows. By integrating these techniques into your projects, you can unlock new possibilities for improving search accuracy, user experience, and system reliability.","metadata":{}}]}