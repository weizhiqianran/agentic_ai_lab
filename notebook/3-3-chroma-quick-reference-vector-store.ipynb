{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Chroma Vector Store Quick Reference (LangChain)\n\n## Introduction\n\nThe Chroma Vector Store API is a powerful tool for managing and querying vectorized data, enabling seamless integration with machine learning models and natural language processing tasks. By leveraging Chroma, developers can efficiently store, retrieve, and manipulate high-dimensional embeddings, making it an essential component for building intelligent applications. This guide provides a detailed walkthrough of Chroma's core functionalities, including database persistence, document operations, search capabilities, and utility functions. Whether you're working with text, images, or other data types, Chroma offers a robust and scalable solution for vector storage and retrieval.\n\n","metadata":{}},{"cell_type":"code","source":"!pip install -qU langchain-openai\n!pip install -qU langchain_community\n!pip install -qU langchain_experimental\n!pip install -qU langchain-chroma>=0.1.2\n!pip install -qU chromadb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-21T17:30:30.756766Z","iopub.execute_input":"2025-01-21T17:30:30.757006Z","iopub.status.idle":"2025-01-21T17:31:29.543781Z","shell.execute_reply.started":"2025-01-21T17:30:30.756982Z","shell.execute_reply":"2025-01-21T17:31:29.542557Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.3/54.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.2/412.2 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m455.5/455.5 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 5.29.3 which is incompatible.\ngoogle-cloud-bigtable 2.27.0 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\ngoogle-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.3 which is incompatible.\npandas-gbq 0.25.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ntensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.3 which is incompatible.\ntensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\ntensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.29.3 which is incompatible.\ntransformers 4.47.0 requires tokenizers<0.22,>=0.21, but you have tokenizers 0.20.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.1/611.1 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlangchain-chroma 0.2.0 requires chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.6.0,>=0.4.0, but you have chromadb 0.6.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"---\n\n## **1. Database Persistence and Loading**\n\n### **1.1 Saving the Chroma Database**\nSave the Chroma database to disk using the `persist_directory` parameter.","metadata":{}},{"cell_type":"code","source":"from langchain_chroma import Chroma\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_core.documents import Document\nfrom kaggle_secrets import UserSecretsClient\n\n# Fetch API key securely\nuser_secrets = UserSecretsClient()\nmy_api_key = user_secrets.get_secret(\"api-key-openai\")\n\n# Initialize OpenAI embeddings\nembed = OpenAIEmbeddings(model=\"text-embedding-3-small\", api_key=my_api_key)\n\n# Initialize Chroma with persist_directory\nvector_store = Chroma(\n    collection_name=\"my_collection\",\n    embedding_function=embed,\n    persist_directory=\"./chroma_db\"  # Data will be saved here\n)\n\n# Create documents\ndocuments = [\n    Document(page_content=\"The quick brown fox jumps over the lazy dog.\", metadata={\"source\": \"fable\"}),\n    Document(page_content=\"Artificial intelligence is transforming the world.\", metadata={\"source\": \"tech\"}),\n]\n\n# Add documents to the vector store\nvector_store.add_documents(documents=documents, ids=[\"doc1\", \"doc2\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T17:33:20.213088Z","iopub.execute_input":"2025-01-21T17:33:20.213523Z","iopub.status.idle":"2025-01-21T17:33:20.474016Z","shell.execute_reply.started":"2025-01-21T17:33:20.213491Z","shell.execute_reply":"2025-01-21T17:33:20.472532Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mBackendError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-32cb65603bc5>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Fetch API key securely\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0muser_secrets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUserSecretsClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmy_api_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_secrets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_secret\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"api-key-openai\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Initialize OpenAI embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/kaggle_secrets.py\u001b[0m in \u001b[0;36mget_secret\u001b[0;34m(self, label)\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;34m'Label'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         }\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mresponse_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweb_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_post_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_USER_SECRET_BY_LABEL_ENDPOINT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'secret'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             raise BackendError(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/kaggle_web_client.py\u001b[0m in \u001b[0;36mmake_post_request\u001b[0;34m(self, data, endpoint, timeout)\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mresponse_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wasSuccessful'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'result'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                     raise BackendError(\n\u001b[0m\u001b[1;32m     50\u001b[0m                         f'Unexpected response from the service. Response: {response_json}.')\n\u001b[1;32m     51\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'result'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mBackendError\u001b[0m: Unexpected response from the service. Response: {'errors': ['No user secrets exist for kernel id 75082080 and label api-key-openai.'], 'error': {'code': 5, 'details': []}, 'wasSuccessful': False}."],"ename":"BackendError","evalue":"Unexpected response from the service. Response: {'errors': ['No user secrets exist for kernel id 75082080 and label api-key-openai.'], 'error': {'code': 5, 'details': []}, 'wasSuccessful': False}.","output_type":"error"}],"execution_count":3},{"cell_type":"markdown","source":"### **1.2 Loading the Chroma Database**\nLoad a previously saved Chroma database from disk.","metadata":{}},{"cell_type":"code","source":"from langchain_chroma import Chroma\nfrom langchain_openai import OpenAIEmbeddings\nfrom kaggle_secrets import UserSecretsClient\n\n# Fetch API key securely\nuser_secrets = UserSecretsClient()\nmy_api_key = user_secrets.get_secret(\"api-key-openai\")\n\n# Initialize OpenAI embeddings\nembed = OpenAIEmbeddings(model=\"text-embedding-3-small\", api_key=my_api_key)\n\n# Load the Chroma database from the persist_directory\nvector_store = Chroma(\n    collection_name=\"my_collection\",\n    embedding_function=embed,\n    persist_directory=\"./chroma_db\"  # Same directory used to save the database\n)\n\n# Perform a similarity search to verify loading\nresults = vector_store.similarity_search(query=\"AI\", k=2)\nfor doc in results:\n    print(f\"Loaded Document: {doc.page_content}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T17:31:32.701369Z","iopub.status.idle":"2025-01-21T17:31:32.701736Z","shell.execute_reply":"2025-01-21T17:31:32.701608Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **1.3 Checking if a Collection Exists**\nCheck if a collection exists before loading it.","metadata":{}},{"cell_type":"code","source":"# Access the internal Chroma client used by langchain_chroma\nchroma_client = vector_store._client\n\n# Check if the collection exists\ntry:\n    collection = chroma_client.get_collection(name=\"my_collection\")\n    print(\"Collection exists and is loaded.\")\nexcept Exception as e:\n    print(\"Collection does not exist.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T17:31:32.702859Z","iopub.status.idle":"2025-01-21T17:31:32.703298Z","shell.execute_reply":"2025-01-21T17:31:32.703117Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **1.4 Deleting a Persisted Collection**\nDelete a persisted collection by removing its directory.","metadata":{}},{"cell_type":"code","source":"import shutil\n\n# Delete the persisted collection directory\nshutil.rmtree(\"./chroma_db\")\nprint(\"Persisted collection directory deleted.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T17:31:32.704354Z","iopub.status.idle":"2025-01-21T17:31:32.704804Z","shell.execute_reply":"2025-01-21T17:31:32.704619Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n## **2. Document Operations**\n\n### **2.1 Adding Documents with `add_documents()`**\nAdd documents to the Chroma vector store.","metadata":{}},{"cell_type":"code","source":"import os\nfrom langchain_chroma import Chroma\nfrom langchain_core.documents import Document\nfrom langchain_openai import OpenAIEmbeddings\n\n# Use /kaggle/working/ for persist_directory\npersist_directory = \"/kaggle/working/chroma_db\"\n\n# Ensure the persist_directory exists\nif not os.path.exists(persist_directory):\n    os.makedirs(persist_directory)\n\n# Initialize OpenAI embeddings\nembed = OpenAIEmbeddings(model=\"text-embedding-3-small\", api_key=my_api_key)\n\n# Initialize Chroma with persist_directory\nvector_store = Chroma(\n    collection_name=\"my_collection\",\n    embedding_function=embed,\n    persist_directory=persist_directory\n)\n\n# Create documents\ndocuments = [\n    Document(page_content=\"The quick brown fox jumps over the lazy dog.\", metadata={\"source\": \"fable\"}),\n    Document(page_content=\"Artificial intelligence is transforming the world.\", metadata={\"source\": \"tech\"}),\n]\n\n# Add documents to the vector store\ntry:\n    ids = [\"doc1\", \"doc2\"]\n    added_ids = vector_store.add_documents(documents=documents, ids=ids)\n    if added_ids == ids:\n        print(\"Documents added successfully.\")\n    else:\n        print(\"Failed to add documents. Returned IDs do not match.\")\nexcept Exception as e:\n    print(f\"Error adding documents: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T17:31:32.705814Z","iopub.status.idle":"2025-01-21T17:31:32.706260Z","shell.execute_reply":"2025-01-21T17:31:32.706051Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **2.2 Adding Texts with `add_texts()`**\nThis example demonstrates how to add textual data to the Chroma vector store using the `add_texts` method. The provided texts are embedded and stored along with optional metadata and IDs.","metadata":{}},{"cell_type":"code","source":"# Example of using add_texts\ntexts = [\n    \"The quick brown fox jumps over the lazy dog.\",\n    \"Artificial intelligence is transforming the world.\"\n]\n\nmetadatas = [\n    {\"source\": \"fable\"},\n    {\"source\": \"tech\"}\n]\n\nids = [\"text1\", \"text2\"]\n\ntry:\n    added_ids = vector_store.add_texts(texts=texts, metadatas=metadatas, ids=ids)\n    if added_ids == ids:\n        print(\"Texts added successfully.\")\n    else:\n        print(\"Failed to add texts. Returned IDs do not match.\")\nexcept Exception as e:\n    print(f\"Error adding texts: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T17:31:32.707295Z","iopub.status.idle":"2025-01-21T17:31:32.707648Z","shell.execute_reply":"2025-01-21T17:31:32.707522Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **2.3 Retrieving Documents Using `get` and `get_by_ids`**\nThis example demonstrates how to retrieve documents from a Chroma vector store using the `get` and `get_by_ids` functions. The `get` function allows filtering by metadata, limiting results, and pagination, while `get_by_ids` retrieves specific documents by their IDs.","metadata":{}},{"cell_type":"code","source":"# Using `get` function\nprint(\"\\nUsing `get` function:\")\nresults = vector_store.get(\n    ids=[\"doc1\", \"text1\"],      # Retrieve specific documents by their IDs\n    where={\"source\": \"fable\"},  # Filter by metadata\n    limit=5,                    # Limit the number of results\n    offset=0                    # Skip the first N results\n)\n\n# Print the results\nprint(\"Retrieved Documents:\")\nfor doc_id, document in zip(results[\"ids\"], results[\"documents\"]):\n    print(f\"ID: {doc_id}, Content: {document}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T17:31:32.708419Z","iopub.status.idle":"2025-01-21T17:31:32.708742Z","shell.execute_reply":"2025-01-21T17:31:32.708622Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Using `get_by_ids` function\nprint(\"\\nUsing `get_by_ids` function:\")\ndocument_ids = [\"doc2\", \"text2\"]\nresults = vector_store.get_by_ids(document_ids)\n\n# Print the results\nprint(\"Retrieved Documents by IDs:\")\nfor doc_id, document in zip(document_ids, results):\n    print(f\"ID: {doc_id}, Content: {document}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T17:31:32.709361Z","iopub.status.idle":"2025-01-21T17:31:32.709718Z","shell.execute_reply":"2025-01-21T17:31:32.709595Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **2.4 Updating Documents**\nUpdate an existing document in the vector store.","metadata":{}},{"cell_type":"code","source":"# Update a document\nupdated_document = Document(\n    page_content=\"AI is revolutionizing industries.\",\n    metadata={\"source\": \"tech\"}\n)\n\ntry:\n    vector_store.update_documents(ids=[\"doc2\"], documents=[updated_document])\n    updated_doc = vector_store.get(ids=[\"doc2\"])[\"documents\"][0]\n    if updated_doc == updated_document.page_content:\n        print(\"Document updated successfully.\")\n    else:\n        print(\"Failed to update document. Content does not match.\")\nexcept Exception as e:\n    print(f\"Error updating document: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T17:31:32.710541Z","iopub.status.idle":"2025-01-21T17:31:32.710839Z","shell.execute_reply":"2025-01-21T17:31:32.710718Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **2.5 Deleting Documents**\nDelete documents by their IDs.","metadata":{}},{"cell_type":"code","source":"# Delete a document\ntry:\n    vector_store.delete(ids=[\"doc1\"])\n    deleted_doc = vector_store.get(ids=[\"doc1\"])\n    if not deleted_doc[\"documents\"]:\n        print(\"Document deleted successfully.\")\n    else:\n        print(\"Failed to delete document. Document still exists.\")\nexcept Exception as e:\n    print(f\"Error deleting document: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T17:31:32.711572Z","iopub.status.idle":"2025-01-21T17:31:32.711882Z","shell.execute_reply":"2025-01-21T17:31:32.711762Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n## **3. Search Operations**\n\n1. **Similarity Search**:\n   - Ideal for retrieving the most relevant documents based on semantic similarity.\n   - Use this when you want straightforward, top-k results.\n\n2. **Similarity Search with Scores**:\n   - Provides additional insight into how closely each document matches the query.\n   - Useful for ranking or filtering results based on similarity thresholds.\n\n3. **Maximal Marginal Relevance (MMR)**:\n   - Balances relevance and diversity in search results.\n   - Use this when you want to avoid redundant or overly similar documents.\n\n### **3.1 Similarity Search**\nSearch for documents similar to a query.\n\nThe `similarity_search` method retrieves documents from the vector store that are most similar to the given query. This is useful for finding relevant information based on semantic similarity.\n\n#### **Parameters**:\n- `query` (str): The input query to search for.\n- `k` (int): The number of documents to return. Defaults to 4.","metadata":{}},{"cell_type":"code","source":"# Perform a similarity search\nquery = \"What is AI?\"\nresults = vector_store.similarity_search(query, k=2)\n\n# Print results\nfor doc in results:\n    print(f\"Content: {doc.page_content}, Metadata: {doc.metadata}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T17:31:32.712624Z","iopub.status.idle":"2025-01-21T17:31:32.712948Z","shell.execute_reply":"2025-01-21T17:31:32.712830Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **3.2 Similarity Search with Scores**\nSearch for documents and retrieve similarity scores.\n\nThe `similarity_search_with_score` method returns documents along with their similarity scores. The scores indicate how closely each document matches the query, with lower scores representing higher similarity.\n\n#### **Parameters**:\n- `query` (str): The input query to search for.\n- `k` (int): The number of documents to return. Defaults to 4.","metadata":{}},{"cell_type":"code","source":"# Perform a similarity search with scores\nresults = vector_store.similarity_search_with_score(query=\"AI\", k=2)\n\nfor doc, score in results:\n    print(f\"Score: {score}, Content: {doc.page_content}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T17:31:32.713553Z","iopub.status.idle":"2025-01-21T17:31:32.713828Z","shell.execute_reply":"2025-01-21T17:31:32.713712Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **3.3 Maximal Marginal Relevance (MMR) Search**\nUse MMR to balance similarity and diversity in search results.\n\nThe `max_marginal_relevance_search` method optimizes for both similarity to the query and diversity among the selected documents. This is useful when you want to avoid redundant results and ensure a variety of relevant documents.\n\n#### **Parameters**:\n- `query` (str): The input query to search for.\n- `k` (int): The number of documents to return. Defaults to 4.\n- `fetch_k` (int): The number of documents to fetch before applying MMR. Defaults to 20.\n- `lambda_mult` (float): A value between 0 and 1 that determines the trade-off between similarity and diversity. Higher values favor similarity, while lower values favor diversity. Defaults to 0.5.","metadata":{}},{"cell_type":"code","source":"# Perform MMR search\nresults = vector_store.max_marginal_relevance_search(\n    query=\"AI\",\n    k=3,\n    fetch_k=10,\n    lambda_mult=0.5  # Higher values favor similarity, lower values favor diversity\n)\n\nfor doc in results:\n    print(f\"MMR Result: {doc.page_content}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T17:31:32.714517Z","iopub.status.idle":"2025-01-21T17:31:32.714819Z","shell.execute_reply":"2025-01-21T17:31:32.714702Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n## **4. Store Retriever**\n\n### **4.1 Using Chroma as a Retriever**\nConvert the vector store into a retriever for use in LangChain pipelines.","metadata":{}},{"cell_type":"code","source":"# Create a retriever\nretriever = vector_store.as_retriever()\n\n# Use the retriever\nquery = \"What is AI?\"\ndocs = retriever.invoke(query)\nfor doc in docs:\n    print(f\"Retrieved Document: {doc.page_content}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T17:31:32.715657Z","iopub.status.idle":"2025-01-21T17:31:32.715959Z","shell.execute_reply":"2025-01-21T17:31:32.715839Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **4.2 Retrieve More Documents with Higher Diversity (MMR)**\nUse the Maximal Marginal Relevance (MMR) algorithm to retrieve documents with a balance of relevance and diversity.","metadata":{}},{"cell_type":"code","source":"# Create a retriever with MMR\nretriever = vector_store.as_retriever(\n    search_type=\"mmr\",\n    search_kwargs={\"k\": 3, \"lambda_mult\": 0.5}\n)\n\n# Use the retriever\nquery = \"What is AI?\"\ndocs = retriever.invoke(query)\nfor doc in docs:\n    print(f\"Retrieved Document: {doc.page_content}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T17:31:32.716597Z","iopub.status.idle":"2025-01-21T17:31:32.716891Z","shell.execute_reply":"2025-01-21T17:31:32.716772Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **4.3 Fetch More Documents for MMR but Return Only Top 5**\nFetch a larger pool of documents for MMR to consider but return only the top 5 most relevant and diverse documents.","metadata":{}},{"cell_type":"code","source":"# Create a retriever with MMR and a larger fetch pool\nretriever = vector_store.as_retriever(\n    search_type=\"mmr\",\n    search_kwargs={\"k\": 5, \"fetch_k\": 50}\n)\n\n# Use the retriever\nquery = \"What is AI?\"\ndocs = retriever.invoke(query)\nfor doc in docs:\n    print(f\"Retrieved Document: {doc.page_content}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T17:31:32.717836Z","iopub.status.idle":"2025-01-21T17:31:32.718236Z","shell.execute_reply":"2025-01-21T17:31:32.718053Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **4.4 Retrieve Documents with a Relevance Score Threshold**\nRetrieve only documents that have a similarity score above a specified threshold.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\n# Normalize scores to [0, 1]\ndef normalize_scores(docs_with_scores):\n    scores = [score for _, score in docs_with_scores]\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    normalized_scores = scaler.fit_transform([[score] for score in scores]).flatten()\n    return [(doc, score) for (doc, _), score in zip(docs_with_scores, normalized_scores)]\n\n# Fetch documents with relevance scores\nquery = \"What is the color of the sky?\"\ndocs_with_scores = vector_store.similarity_search_with_relevance_scores(query)\n\n# Normalize the scores\nnormalized_docs_with_scores = normalize_scores(docs_with_scores)\n\n# Filter documents based on the normalized score threshold\nscore_threshold = 0.8\nfiltered_docs = [doc for doc, score in normalized_docs_with_scores if score >= score_threshold]\n\n# Print the filtered documents\nfor doc in filtered_docs:\n    print(f\"Retrieved Document: {doc.page_content}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T17:31:32.718949Z","iopub.status.idle":"2025-01-21T17:31:32.719247Z","shell.execute_reply":"2025-01-21T17:31:32.719127Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **4.5 Retrieve Only the Single Most Similar Document**\nRetrieve only the single most relevant document to the query.","metadata":{}},{"cell_type":"code","source":"# Create a retriever to fetch only the top document\nretriever = vector_store.as_retriever(search_kwargs={\"k\": 1})\n\n# Use the retriever\nquery = \"What is AI?\"\ndocs = retriever.invoke(query)\nfor doc in docs:\n    print(f\"Retrieved Document: {doc.page_content}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T17:31:32.719968Z","iopub.status.idle":"2025-01-21T17:31:32.720320Z","shell.execute_reply":"2025-01-21T17:31:32.720150Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **4.6 Filter Documents by Metadata**\nRetrieve documents that match specific metadata filters, such as a paper title or publication year.","metadata":{}},{"cell_type":"code","source":"# Create a retriever with a metadata filter\nretriever = vector_store.as_retriever(\n    search_kwargs={\"filter\": {\"paper_title\": \"GPT-4 Technical Report\"}}\n)\n\n# Use the retriever\nquery = \"What is AI?\"\ndocs = retriever.invoke(query)\nfor doc in docs:\n    print(f\"Retrieved Document: {doc.page_content}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T17:31:32.721758Z","iopub.status.idle":"2025-01-21T17:31:32.722038Z","shell.execute_reply":"2025-01-21T17:31:32.721925Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n## **5. Class Methods**\n\n### **5.1 Creating a Vector Store from Documents**\nCreate a Chroma vector store directly from a list of documents.","metadata":{}},{"cell_type":"code","source":"from langchain_chroma import Chroma\nfrom langchain_core.documents import Document\nfrom langchain_openai import OpenAIEmbeddings\n\n# Initialize OpenAI embeddings\nembed = OpenAIEmbeddings(model=\"text-embedding-3-small\", api_key=my_api_key)\n\n# Create documents\ndocuments = [\n    Document(page_content=\"The sun rises in the east.\", metadata={\"source\": \"science\"}),\n    Document(page_content=\"The moon orbits the Earth.\", metadata={\"source\": \"science\"}),\n]\n\n# Create a Chroma vector store from documents\ntry:\n    vector_store = Chroma.from_documents(\n        documents=documents,\n        embedding=embed,\n        collection_name=\"science_collection\",\n        persist_directory=\"./chroma_db_science\"\n    )\n    \n    # Verify success by checking if the collection exists\n    if vector_store._collection:\n        print(\"Vector store created successfully.\")\n    else:\n        print(\"Failed to create vector store. Collection is empty.\")\nexcept Exception as e:\n    print(f\"Error creating vector store: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T17:31:32.722967Z","iopub.status.idle":"2025-01-21T17:31:32.723430Z","shell.execute_reply":"2025-01-21T17:31:32.723255Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **5.2 Creating a Vector Store from Texts**\nCreate a Chroma vector store directly from raw texts.","metadata":{}},{"cell_type":"code","source":"from langchain_chroma import Chroma\nfrom langchain_openai import OpenAIEmbeddings\n\n# Initialize OpenAI embeddings\nembed = OpenAIEmbeddings(model=\"text-embedding-3-small\", api_key=my_api_key)\n\n# Create texts and metadata\ntexts = [\"The sky is blue.\", \"The grass is green.\"]\nmetadatas = [{\"source\": \"nature\"}, {\"source\": \"nature\"}]\n\n# Create a Chroma vector store from texts\ntry:\n    vector_store = Chroma.from_texts(\n        texts=texts,\n        embedding=embed,\n        metadatas=metadatas,\n        collection_name=\"nature_collection\",\n        persist_directory=\"./chroma_db_nature\"\n    )\n    \n    # Verify success by checking if the collection exists\n    if vector_store._collection:\n        print(\"Vector store created successfully.\")\n    else:\n        print(\"Failed to create vector store. Collection is empty.\")\nexcept Exception as e:\n    print(f\"Error creating vector store: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T17:31:32.724182Z","iopub.status.idle":"2025-01-21T17:31:32.724558Z","shell.execute_reply":"2025-01-21T17:31:32.724386Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Conclusion\n\nIn this guide, we explored the versatility of the Chroma Vector Store API through practical examples, from saving and loading databases to performing advanced search operations. By following these examples, you can effectively manage vectorized data, integrate Chroma into your workflows, and build intelligent systems that leverage the power of embeddings. Whether you're a beginner or an experienced developer, Chroma's intuitive API and powerful features make it an invaluable tool for modern AI applications. Start experimenting with Chroma today and unlock the full potential of vector-based data management.","metadata":{}}]}