{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10513109,"sourceType":"datasetVersion","datasetId":6507514}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Caching Embeddings in LangChain: Boosting Performance for NLP Applications\n\n## Introduction\n\nEmbeddings are a fundamental component of modern natural language processing (NLP) systems, enabling machines to understand and process text data by converting it into numerical representations. However, computing embeddings for large datasets or frequently repeated texts can be computationally expensive and time-consuming. To address this challenge, **caching embeddings** has emerged as a powerful technique to optimize performance and resource utilization. By storing precomputed embeddings in a key-value store, systems can avoid redundant computations and significantly speed up operations.\n\nThe `CacheBackedEmbeddings` class in LangChain provides a seamless way to implement caching for embeddings. It acts as a wrapper around an embedding model, allowing embeddings to be cached in a variety of storage backends, such as in-memory stores or disk-based stores. This approach not only improves efficiency but also enhances scalability, making it ideal for applications that require repeated embedding computations, such as vector store creation, semantic search, and retrieval-augmented generation (RAG) systems.\n\nIn this guide, we explore how to use `CacheBackedEmbeddings` to cache embeddings, demonstrate its integration with vector stores like Chroma, and highlight its benefits through practical examples. Whether you're working with large datasets or building real-time NLP applications, caching embeddings can help you achieve faster and more efficient workflows.","metadata":{}},{"cell_type":"markdown","source":"---\n\n## Preparation\n\n### Installing Required Libraries\nThis section installs the necessary Python libraries for working with LangChain, OpenAI embeddings, and Chroma vector store. These libraries include:\n- `langchain-openai`: Provides integration with OpenAI's embedding models.\n- `langchain_community`: Contains community-contributed modules and tools for LangChain.\n- `langchain_experimental`: Includes experimental features and utilities for LangChain.\n- `langchain-chroma`: Enables integration with the Chroma vector database.\n- `chromadb`: The core library for the Chroma vector database.","metadata":{}},{"cell_type":"code","source":"!pip install -qU langchain-openai\n!pip install -qU langchain_community\n!pip install -qU langchain_experimental\n!pip install -qU langchain-chroma>=0.1.2\n!pip install -qU chromadb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T07:20:55.688731Z","iopub.execute_input":"2025-01-19T07:20:55.689139Z","iopub.status.idle":"2025-01-19T07:22:01.3579Z","shell.execute_reply.started":"2025-01-19T07:20:55.689099Z","shell.execute_reply":"2025-01-19T07:22:01.356677Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Initializing OpenAI Embeddings\nThis section demonstrates how to securely fetch an OpenAI API key using Kaggle's `UserSecretsClient` and initialize the OpenAI embedding model. The `OpenAIEmbeddings` class is used to create an embedding model instance, which will be used to convert text into numerical embeddings.\n\nKey steps:\n1. **Fetch API Key**: The OpenAI API key is securely retrieved using Kaggle's `UserSecretsClient`.\n2. **Initialize Embeddings**: The `OpenAIEmbeddings` class is initialized with the `text-embedding-3-small` model and the fetched API key.\n\nThis setup ensures that the embedding model is ready for use in downstream tasks, such as caching embeddings or creating vector stores.","metadata":{}},{"cell_type":"code","source":"from langchain_openai import OpenAIEmbeddings\nfrom kaggle_secrets import UserSecretsClient\n\n# # Fetch API key securely\n# user_secrets = UserSecretsClient()\n# my_api_key = user_secrets.get_secret(\"api-key-openai\")\n\n# # Initialize OpenAI embeddings\n# embed = OpenAIEmbeddings(model=\"text-embedding-3-small\", api_key=my_api_key)\n\n\nembed = OpenAIEmbeddings(model=\"text-embedding-3-large\", base_url=\"请输入地址\",\n                        api_key=\"sk-RapHwqOGWbKT68V1531b7011388549F3Bb4316EcF8Ac28De\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T07:32:59.332469Z","iopub.execute_input":"2025-01-19T07:32:59.332901Z","iopub.status.idle":"2025-01-19T07:32:59.721059Z","shell.execute_reply.started":"2025-01-19T07:32:59.332871Z","shell.execute_reply":"2025-01-19T07:32:59.720169Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n## CacheBackedEmbeddings\n\n### Example 1: Using `embed_documents()`\nThis example demonstrates how to use the `embed_documents()` function to embed a list of texts. It caches the embeddings if they are not already in the cache.","metadata":{}},{"cell_type":"markdown","source":"### Example 2: Using `embed_query()`\nThis example demonstrates how to use the `embed_query()` function to embed a single query text. It caches the query embedding if a `query_embedding_store` is provided.","metadata":{}},{"cell_type":"code","source":"from langchain.embeddings import CacheBackedEmbeddings\nfrom langchain.storage import InMemoryStore\n\n# Initialize the document embedding store (e.g., InMemoryStore)\ndocument_embedding_store = InMemoryStore()\n\n# Create the CacheBackedEmbeddings instance\ncached_embedder = CacheBackedEmbeddings(underlying_embeddings=embed, document_embedding_store=document_embedding_store)\n\n# List of texts to embed\ntexts = [\"Hello, world!\", \"This is a test.\", \"Caching embeddings is useful.\"]\n\n# Embed the documents\nembeddings = cached_embedder.embed_documents(texts)\n\n# Print the embeddings\nfor text, embedding in zip(texts, embeddings):\n    print(f\"Text: {text}\\nEmbedding Length: {len(embedding)}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T07:59:17.656246Z","iopub.execute_input":"2025-01-19T07:59:17.65664Z","iopub.status.idle":"2025-01-19T07:59:18.17254Z","shell.execute_reply.started":"2025-01-19T07:59:17.656607Z","shell.execute_reply":"2025-01-19T07:59:18.171345Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain.embeddings import CacheBackedEmbeddings\nfrom langchain.storage import InMemoryStore\n\n# Initialize the document and query embedding stores (e.g., InMemoryStore)\ndocument_embedding_store = InMemoryStore()\nquery_embedding_store = InMemoryStore()\n\n# Create the CacheBackedEmbeddings instance with query caching enabled\ncached_embedder = CacheBackedEmbeddings(\n    underlying_embeddings=embed,\n    document_embedding_store=document_embedding_store,\n    query_embedding_store=query_embedding_store\n)\n\n# Query text to embed\nquery_text = \"What is the meaning of life?\"\n\n# Embed the query\nquery_embedding = cached_embedder.embed_query(query_text)\n\n# Print the query embedding\nprint(f\"Query: {query_text}\\nEmbedding Length: {len(query_embedding)}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T07:59:20.232708Z","iopub.execute_input":"2025-01-19T07:59:20.233046Z","iopub.status.idle":"2025-01-19T07:59:20.297086Z","shell.execute_reply.started":"2025-01-19T07:59:20.233019Z","shell.execute_reply":"2025-01-19T07:59:20.295614Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Example 3: Using `from_bytes_store()`\nThis example demonstrates how to use the `from_bytes_store()` function to create a `CacheBackedEmbeddings` instance with a byte-based store.","metadata":{}},{"cell_type":"code","source":"from langchain.embeddings import CacheBackedEmbeddings\nfrom langchain.storage import InMemoryByteStore\n\n# Initialize the byte-based document embedding store (e.g., InMemoryByteStore)\ndocument_embedding_cache = InMemoryByteStore()\n\n# Create the CacheBackedEmbeddings instance using from_bytes_store\ncached_embedder = CacheBackedEmbeddings.from_bytes_store(\n    underlying_embeddings=embed,\n    document_embedding_cache=document_embedding_cache,\n    namespace=\"openai_embeddings\"  # Optional namespace to avoid cache collisions\n)\n\n# List of texts to embed\ntexts = [\"Caching embeddings with bytes store.\", \"This is another example.\"]\n\n# Embed the documents\nembeddings = cached_embedder.embed_documents(texts)\n\n# Print the embeddings\nfor text, embedding in zip(texts, embeddings):\n    print(f\"Text: {text}\\nEmbedding Length: {len(embedding)}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T07:28:10.051087Z","iopub.execute_input":"2025-01-19T07:28:10.051539Z","iopub.status.idle":"2025-01-19T07:28:10.51582Z","shell.execute_reply.started":"2025-01-19T07:28:10.051506Z","shell.execute_reply":"2025-01-19T07:28:10.514733Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n## InMemoryByteStore\n\n### Example 1: Using `mset()` to Store Key-Value Pairs\nThis example demonstrates how to use the `mset()` function to store multiple key-value pairs in the `InMemoryByteStore`.","metadata":{}},{"cell_type":"code","source":"from langchain.storage import InMemoryByteStore\n\n# Initialize an empty store\nstore = InMemoryByteStore()\n\n# Set multiple key-value pairs\nstore.mset([('key1', b'value1'), ('key2', b'value2'), ('key3', b'value3')])\n\n# Verify the keys and values\nprint(\"Store after mset():\", store.store)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T07:28:30.446873Z","iopub.execute_input":"2025-01-19T07:28:30.447203Z","iopub.status.idle":"2025-01-19T07:28:30.453403Z","shell.execute_reply.started":"2025-01-19T07:28:30.447177Z","shell.execute_reply":"2025-01-19T07:28:30.452101Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Example 2: Using `mget()` to Retrieve Values\nThis example demonstrates how to use the `mget()` function to retrieve values associated with multiple keys.","metadata":{}},{"cell_type":"code","source":"from langchain.storage import InMemoryByteStore\n\n# Initialize a store with some data\nstore = InMemoryByteStore()\nstore.mset([('key1', b'value1'), ('key2', b'value2'), ('key3', b'value3')])\n\n# Retrieve values for multiple keys\nvalues = store.mget(['key1', 'key2', 'key4'])\n\n# Print the retrieved values\nprint(\"Retrieved values:\", values)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T07:28:41.299801Z","iopub.execute_input":"2025-01-19T07:28:41.300146Z","iopub.status.idle":"2025-01-19T07:28:41.306804Z","shell.execute_reply.started":"2025-01-19T07:28:41.30012Z","shell.execute_reply":"2025-01-19T07:28:41.30572Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Example 3: Using `mdelete()` to Delete Keys\nThis example demonstrates how to use the `mdelete()` function to delete specific keys and their associated values.","metadata":{}},{"cell_type":"code","source":"from langchain.storage import InMemoryByteStore\n\n# Initialize a store with some data\nstore = InMemoryByteStore()\nstore.mset([('key1', b'value1'), ('key2', b'value2'), ('key3', b'value3')])\n\n# Delete specific keys\nstore.mdelete(['key1', 'key3'])\n\n# Verify the store after deletion\nprint(\"Store after mdelete():\", store.store)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T07:28:53.390011Z","iopub.execute_input":"2025-01-19T07:28:53.390386Z","iopub.status.idle":"2025-01-19T07:28:53.396743Z","shell.execute_reply.started":"2025-01-19T07:28:53.390357Z","shell.execute_reply":"2025-01-19T07:28:53.395531Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Example 4: Combining `mset()`, `mget()`, and `mdelete()`\nThis example demonstrates how to use `mset()`, `mget()`, and `mdelete()` together.","metadata":{}},{"cell_type":"code","source":"from langchain.storage import InMemoryByteStore\n\n# Initialize an empty store\nstore = InMemoryByteStore()\n\n# Set multiple key-value pairs\nstore.mset([('key1', b'value1'), ('key2', b'value2'), ('key3', b'value3')])\n\n# Retrieve values for multiple keys\nvalues_before_deletion = store.mget(['key1', 'key2', 'key3'])\nprint(\"Values before deletion:\", values_before_deletion)\n# Output: [b'value1', b'value2', b'value3']\n\n# Delete specific keys\nstore.mdelete(['key1', 'key3'])\n\n# Retrieve values after deletion\nvalues_after_deletion = store.mget(['key1', 'key2', 'key3'])\nprint(\"Values after deletion:\", values_after_deletion)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T07:29:05.012789Z","iopub.execute_input":"2025-01-19T07:29:05.013156Z","iopub.status.idle":"2025-01-19T07:29:05.020495Z","shell.execute_reply.started":"2025-01-19T07:29:05.013127Z","shell.execute_reply":"2025-01-19T07:29:05.019329Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Example 5: Using `yield_keys()` to Iterate Over Keys\nThis example demonstrates how to use the `yield_keys()` function to iterate over keys in the store, optionally filtered by a prefix.","metadata":{}},{"cell_type":"code","source":"from langchain.storage import InMemoryByteStore\n\n# Initialize a store with some data\nstore = InMemoryByteStore()\nstore.mset([('key1', b'value1'), ('key2', b'value2'), ('key3', b'value3'), ('other_key', b'other_value')])\n\n# Iterate over all keys\nprint(\"All keys:\")\nfor key in store.yield_keys():\n    print(key)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T07:54:17.764657Z","iopub.execute_input":"2025-01-19T07:54:17.765075Z","iopub.status.idle":"2025-01-19T07:54:17.772471Z","shell.execute_reply.started":"2025-01-19T07:54:17.765041Z","shell.execute_reply":"2025-01-19T07:54:17.771465Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Iterate over keys with a specific prefix\nprint(\"Keys with prefix 'key':\")\nfor key in store.yield_keys(prefix='key'):\n    print(key)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T07:54:19.499035Z","iopub.execute_input":"2025-01-19T07:54:19.499406Z","iopub.status.idle":"2025-01-19T07:54:19.505731Z","shell.execute_reply.started":"2025-01-19T07:54:19.499374Z","shell.execute_reply":"2025-01-19T07:54:19.50449Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n## Caching Embeddings\n\n### Part 1: Using `LocalFileStore` for Caching Embeddings\n\n#### Purpose:\nThis section demonstrates how to cache embeddings on disk using `LocalFileStore` and create a vector store with **Chroma**. The caching mechanism avoids recomputing embeddings for the same text, significantly speeding up repeated operations.\n\n#### Steps:\n1. **Initialize the Cache**:\n   - A `LocalFileStore` is created to store embeddings on disk in the `./cache/` directory.\n   - A `CacheBackedEmbeddings` instance is created using the `from_bytes_store()` method, which wraps the embedding model and caches embeddings in the specified store.\n\n2. **Load and Split Documents**:\n   - A document (`state_of_the_union.txt`) is loaded using `TextLoader`.\n   - The document is split into smaller chunks using `CharacterTextSplitter`.\n\n3. **Create Chroma Vector Store**:\n   - The `Chroma.from_documents()` method is used to create a vector store from the document chunks.\n   - The time taken to create the vector store is measured using Python's `time` module.\n\n4. **Reuse Cached Embeddings**:\n   - The vector store is created again using the same documents and cached embeddings.\n   - The time taken for the second creation is measured to demonstrate the performance improvement due to caching.\n\n5. **Check Cached Embeddings**:\n   - The keys of the cached embeddings are printed to verify that embeddings are being stored.","metadata":{}},{"cell_type":"code","source":"import os\nimport time\nimport shutil\nfrom langchain.storage import LocalFileStore\nfrom langchain_community.document_loaders import TextLoader\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_text_splitters import CharacterTextSplitter\n\n# Define the cache directory\ncache_dir = \"./cache/\"\n\n# Check if the cache folder exists and delete it if it does\nif os.path.exists(cache_dir):\n    print(f\"Deleting existing cache folder: {cache_dir}\")\n    shutil.rmtree(cache_dir)\n\n# Create a LocalFileStore for caching embeddings\nstore = LocalFileStore(cache_dir)\n\n# Create a CacheBackedEmbeddings instance\ncached_embedder = CacheBackedEmbeddings.from_bytes_store(embed, store, namespace=embed.model)\n\n# Check the cache (it should be empty initially)\nprint(\"Initial cache keys:\", list(store.yield_keys()))  # Output: []\n\n# Load the document and split it into chunks\nraw_documents = TextLoader(\"/kaggle/input/state-of-the-union-txt/state_of_the_union.txt\").load()\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\ndocuments = text_splitter.split_documents(raw_documents)\n\n# Create the Chroma vector store with the cached embedder\nstart_time = time.time()  # Start timing\ndb = Chroma.from_documents(documents, cached_embedder, persist_directory=\"./chroma_db\")\nend_time = time.time()    # End timing\nprint(f\"\\nTime taken to create vector store (initial): {end_time - start_time:.2f} seconds\")\n\n# Try creating the vector store again (it should be much faster due to caching)\nstart_time = time.time()  # Start timing\ndb2 = Chroma.from_documents(documents, cached_embedder, persist_directory=\"./chroma_db\")\nend_time = time.time()    # End timing\nprint(f\"Time taken to create vector store (cached): {end_time - start_time:.2f} seconds\")\n\n# Check some of the cached embeddings\nprint(\"\\nCached embeddings keys:\", list(store.yield_keys())[:5])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T07:46:19.536499Z","iopub.execute_input":"2025-01-19T07:46:19.536899Z","iopub.status.idle":"2025-01-19T07:46:21.601478Z","shell.execute_reply.started":"2025-01-19T07:46:19.536867Z","shell.execute_reply":"2025-01-19T07:46:21.600313Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Part 2: Swapping to `InMemoryByteStore` for Caching Embeddings\n\n#### Purpose:\nThis section demonstrates how to use an in-memory store (`InMemoryByteStore`) for caching embeddings instead of a disk-based store. It also shows how to retrieve, inspect, and delete cached embeddings.\n\n#### Steps:\n1. **Initialize the In-Memory Cache**:\n   - An `InMemoryByteStore` is created to store embeddings in memory.\n   - A `CacheBackedEmbeddings` instance is created using the in-memory store.\n\n2. **Embed Documents**:\n   - A list of texts is embedded using the `embed_documents()` method.\n   - The embeddings are cached in the in-memory store.\n\n3. **Print Embeddings**:\n   - The embeddings for the texts are printed (only the first 10 values are shown for brevity).\n\n4. **Check Cache Keys**:\n   - The keys of the cached embeddings are printed to verify that the embeddings have been stored.\n\n5. **Retrieve Cached Embeddings**:\n   - The cached embeddings are retrieved using the `mget()` method and printed (only the first 10 values are shown for brevity).\n\n6. **Delete Keys from Cache**:\n   - Some keys are deleted from the cache using the `mdelete()` method.\n   - The remaining cache keys are printed to verify the deletion.","metadata":{}},{"cell_type":"code","source":"from langchain.embeddings import CacheBackedEmbeddings\nfrom langchain.storage import InMemoryByteStore\n\n# Create an in-memory store\nstore = InMemoryByteStore()\n\n# Create a CacheBackedEmbeddings instance with the in-memory store\ncached_embedder = CacheBackedEmbeddings.from_bytes_store(embed, store, namespace=embed.model)\n\n# List of texts to embed\ntexts = [\"Caching embeddings with in-memory store.\", \"This is another example.\"]\n\n# Embed the documents\nembeddings_list = cached_embedder.embed_documents(texts)\n\n# Print the embeddings\nprint(\"Embeddings for the texts:\")\nfor text, embedding in zip(texts, embeddings_list):\n    print(f\"Text: {text}\\nEmbedding (first 10 values): {embedding[:10]}\\n\")  # Print first 10 values for brevity\n\n# Check the cache keys after embedding\nprint(\"Cache keys after embedding:\")\ncache_keys = list(store.yield_keys())\nprint(cache_keys)\n\n# Retrieve cached embeddings using mget\nprint(\"\\nRetrieving cached embeddings:\")\ncached_embeddings = store.mget(cache_keys)\nfor key, cached_embedding in zip(cache_keys, cached_embeddings):\n    print(f\"Key: {key}\\nCached Embedding (first 10 values): {cached_embedding[:10]}\\n\")  # Print first 10 values for brevity\n\n# Delete some keys from the cache\nkeys_to_delete = cache_keys[:1]  # Delete the first key\nstore.mdelete(keys_to_delete)\n\n# Check the cache keys after deletion\nprint(\"Cache keys after deletion:\")\nprint(list(store.yield_keys()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T07:40:29.504355Z","iopub.execute_input":"2025-01-19T07:40:29.504755Z","iopub.status.idle":"2025-01-19T07:40:30.377007Z","shell.execute_reply.started":"2025-01-19T07:40:29.504723Z","shell.execute_reply":"2025-01-19T07:40:30.375875Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Conclusion\n\nCaching embeddings is a game-changer for optimizing NLP workflows, especially when dealing with large datasets or repeated computations. By leveraging `CacheBackedEmbeddings`, developers can significantly reduce the time and resources required to compute embeddings, enabling faster and more scalable applications. Whether you're building a vector store, performing semantic search, or implementing retrieval-augmented generation, caching embeddings ensures that your system remains efficient and responsive.\n\nThe flexibility to use different storage backends, such as in-memory stores or disk-based stores, makes `CacheBackedEmbeddings` a versatile tool for a wide range of use cases. Additionally, features like namespace support and query embedding caching further enhance its utility, allowing developers to tailor the caching mechanism to their specific needs.\n\nAs NLP systems continue to grow in complexity and scale, techniques like caching embeddings will play an increasingly important role in ensuring optimal performance. By adopting these strategies, you can build faster, more efficient, and more reliable NLP applications that deliver value to users and stakeholders.","metadata":{}}]}