{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6376211,"sourceType":"datasetVersion","datasetId":3674210}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Using LangChain's Pandas DataFrame Agent\n\n## Introduction\n\nThe `create_pandas_dataframe_agent` function from the `langchain_experimental.agents.agent_toolkits.pandas.base` module is a powerful tool designed to enable language models (LLMs) to interact with and analyze data stored in Pandas DataFrames. By combining the capabilities of LLMs with the flexibility of Pandas, this function allows users to perform complex data analysis tasks through natural language queries. The agent can handle a variety of operations, such as filtering, aggregation, and visualization, making it an invaluable tool for data scientists, analysts, and developers who want to streamline their workflows.\n\nHowever, it is important to note that this functionality comes with significant security considerations. The agent relies on a Python REPL (Read-Eval-Print Loop) tool, which can execute arbitrary code. This capability, while powerful, introduces potential risks, such as arbitrary code execution vulnerabilities, if not used in a properly sandboxed environment. Users must explicitly opt-in to this functionality by setting the `allow_dangerous_code` parameter to `True`, acknowledging the risks and ensuring that appropriate safeguards are in place.","metadata":{}},{"cell_type":"markdown","source":"---\n\n## Preparation\n\n### Installing Required Libraries\nThis section installs the necessary Python libraries for working with LangChain, OpenAI embeddings, Anthropic models, and other utilities. These libraries include:\n- `langchain-openai`: Provides integration with OpenAI's embedding models and APIs.\n- `langchain-anthropic`: Enables integration with Anthropic's models and APIs.\n- `langchain_community`: Contains community-contributed modules and tools for LangChain.\n- `langchain_experimental`: Includes experimental features and utilities for LangChain.","metadata":{}},{"cell_type":"code","source":"!pip install -qU langchain-openai\n!pip install -qU langchain-anthropic\n!pip install -qU langchain_community\n!pip install -qU langchain_experimental","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T16:07:25.625016Z","iopub.execute_input":"2025-01-23T16:07:25.625395Z","iopub.status.idle":"2025-01-23T16:07:52.513695Z","shell.execute_reply.started":"2025-01-23T16:07:25.625364Z","shell.execute_reply":"2025-01-23T16:07:52.512566Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Initializing OpenAI and Anthropic Chat Models\nThis section demonstrates how to securely fetch API keys for OpenAI and Anthropic using Kaggle's `UserSecretsClient` and initialize their respective chat models. The `ChatOpenAI` and `ChatAnthropic` classes are used to create instances of these models, which can be used for natural language processing tasks such as text generation and conversational AI.\n\n**Key steps:**\n1. **Fetch API Keys**: The OpenAI and Anthropic API keys are securely retrieved using Kaggle's `UserSecretsClient`.\n2. **Initialize Chat Models**:\n   - The `ChatOpenAI` class is initialized with the `gpt-4o-mini` model and the fetched OpenAI API key.\n   - The `ChatAnthropic` class is initialized with the `claude-3-5-sonnet-latest` model and the fetched Anthropic API key.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom langchain_openai import ChatOpenAI\nfrom langchain_anthropic import ChatAnthropic\nfrom kaggle_secrets import UserSecretsClient\n\n# Fetch API key securely\nuser_secrets = UserSecretsClient()\n\n# Initialize LLM\nmodel = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, api_key=user_secrets.get_secret(\"my-openai-api-key\"))\n#model = ChatAnthropic(model=\"claude-3-5-sonnet-latest\", temperature=0, api_key=user_secrets.get_secret(\"my-anthropic-api-key\"))\n\n# Load the Titanic dataset\ndf = pd.read_csv(\"/kaggle/input/titanic-dataset/titanic.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T16:10:37.355867Z","iopub.execute_input":"2025-01-23T16:10:37.356189Z","iopub.status.idle":"2025-01-23T16:10:37.6567Z","shell.execute_reply.started":"2025-01-23T16:10:37.356164Z","shell.execute_reply":"2025-01-23T16:10:37.655829Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The provided code defines a utility function, `pretty_print_response`, designed to format and display the output of an agent's response in a clean and readable manner.","metadata":{}},{"cell_type":"code","source":"def pretty_print_response(question, response):\n    \"\"\"\n    Formats and prints the agent's response in a structured and readable manner.\n\n    This function takes a question and the agent's response, extracts the 'output' key\n    from the response, and prints it with clear separators for improved readability.\n    It is useful for debugging, logging, or presenting results in a user-friendly format.\n\n    Args:\n        question (str): The question or query posed to the agent.\n        response (dict): The agent's response, expected to contain an 'output' key\n                         with the result of the query.\n\n    Returns:\n        None: This function prints the formatted output directly to the console.\n    \"\"\"\n    print(f\"Question: {question}\")\n    print(\"\\n\" + \"=\" * 80 + \"\\n\")\n    print(response[\"output\"])  # Extract the 'output' key for pretty printing\n    print(\"\\n\" + \"=\" * 80 + \"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T16:10:39.601109Z","iopub.execute_input":"2025-01-23T16:10:39.601426Z","iopub.status.idle":"2025-01-23T16:10:39.606939Z","shell.execute_reply.started":"2025-01-23T16:10:39.601401Z","shell.execute_reply":"2025-01-23T16:10:39.605764Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n## Examples\n\nThe `create_pandas_dataframe_agent` function is a powerful tool for analyzing and interacting with Pandas DataFrames using a language model (LLM). It allows you to perform complex data analysis tasks, such as filtering, aggregation, and visualization, using natural language queries. Below are examples demonstrating the use of key parameters in `create_pandas_dataframe_agent`.\n\n### **Example 1: Basic Usage with Default Parameters**\nThis example demonstrates how to create a Pandas agent with default parameters. The agent is initialized with a DataFrame and a language model (`llm`). It can answer questions about the dataset, such as column names, data types, and basic statistics.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom langchain_experimental.agents import create_pandas_dataframe_agent\n\n# Load the Titanic dataset\ndf = pd.read_csv(\"/kaggle/input/titanic-dataset/titanic.csv\")\n\n# Create the agent with default parameters\nagent = create_pandas_dataframe_agent(\n    model,\n    df,\n    agent_type=\"tool-calling\",  # Use the modern \"tool-calling\" agent type\n    allow_dangerous_code=True,  # Allow execution of Python code (use with caution)\n    verbose=True,               # Enable verbose logging for debugging\n)\n\n# Ask the agent a question\nresponse = agent.invoke(\"What are the columns in the dataset and their data types?\")\nprint(response[\"output\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T16:22:00.227484Z","iopub.execute_input":"2025-01-23T16:22:00.227873Z","iopub.status.idle":"2025-01-23T16:22:02.836094Z","shell.execute_reply.started":"2025-01-23T16:22:00.227845Z","shell.execute_reply":"2025-01-23T16:22:02.835179Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Example 2: Custom Prompt Prefix and Suffix**\nThis example shows how to use the `prefix` and `suffix` parameters to customize the agent's behavior. The `prefix` provides context for the agent, while the `suffix` specifies the format of the output.","metadata":{}},{"cell_type":"code","source":"# Create the agent with a custom prefix and suffix\nagent = create_pandas_dataframe_agent(\n    model,\n    df,\n    agent_type=\"tool-calling\",\n    prefix=\"You are a data analyst. Analyze the Titanic dataset and provide concise answers.\",\n    suffix=\"Provide the final answer in a clear and structured format.\",\n    allow_dangerous_code=True,\n    verbose=True,\n)\n\n# Ask the agent a question\nresponse = agent.invoke(\"What is the average age of passengers?\")\nprint(response[\"output\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T16:23:22.464588Z","iopub.execute_input":"2025-01-23T16:23:22.465001Z","iopub.status.idle":"2025-01-23T16:23:33.122422Z","shell.execute_reply.started":"2025-01-23T16:23:22.464974Z","shell.execute_reply":"2025-01-23T16:23:33.121579Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Example 3: Including DataFrame Head in the Prompt**\nThis example demonstrates how to include the first few rows of the DataFrame in the prompt using the `include_df_in_prompt` and `number_of_head_rows` parameters. This helps the agent understand the structure of the data.","metadata":{}},{"cell_type":"code","source":"# Create the agent with the first 5 rows included in the prompt\nagent = create_pandas_dataframe_agent(\n    model,\n    df,\n    agent_type=\"tool-calling\",\n    include_df_in_prompt=True,  # Include the DataFrame head in the prompt\n    number_of_head_rows=5,      # Number of rows to include\n    allow_dangerous_code=True,\n    verbose=True,\n)\n\n# Ask the agent a question\nresponse = agent.invoke(\"What are the unique values in the 'Pclass' column?\")\nprint(response[\"output\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T16:23:37.449168Z","iopub.execute_input":"2025-01-23T16:23:37.449551Z","iopub.status.idle":"2025-01-23T16:23:39.086902Z","shell.execute_reply.started":"2025-01-23T16:23:37.449521Z","shell.execute_reply":"2025-01-23T16:23:39.085892Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Example 4: Adding Extra Tools**\nThis example shows how to add extra tools to the agent using the `extra_tools` parameter. These tools can extend the agent's capabilities, such as performing custom calculations or interacting with external APIs.","metadata":{}},{"cell_type":"code","source":"from langchain.tools import Tool\n\n# Define a custom tool\ndef custom_calculation_tool(input: str) -> str:\n    return f\"Custom calculation result for: {input}\"\n\n# Add the custom tool to the agent\nextra_tools = [\n    Tool(\n        name=\"custom_calculation\",\n        func=custom_calculation_tool,\n        description=\"A tool for performing custom calculations.\"\n    )\n]\n\n# Create the agent with extra tools\nagent = create_pandas_dataframe_agent(\n    model,\n    df,\n    agent_type=\"tool-calling\",\n    extra_tools=extra_tools,  # Add custom tools\n    allow_dangerous_code=True,\n    verbose=True,\n)\n\n# Ask the agent to use the custom tool\nresponse = agent.invoke(\"Use the custom_calculation tool to process 'example input'.\")\nprint(response[\"output\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T16:23:54.199329Z","iopub.execute_input":"2025-01-23T16:23:54.19977Z","iopub.status.idle":"2025-01-23T16:23:56.276584Z","shell.execute_reply.started":"2025-01-23T16:23:54.199738Z","shell.execute_reply":"2025-01-23T16:23:56.275595Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Example 5: Limiting Execution Time and Iterations**\nThis example demonstrates how to limit the agent's execution time and the number of iterations using the `max_execution_time` and `max_iterations` parameters. This is useful for controlling resource usage.","metadata":{}},{"cell_type":"code","source":"# Create the agent with execution limits\nagent = create_pandas_dataframe_agent(\n    model,\n    df,\n    agent_type=\"tool-calling\",\n    max_execution_time=10,  # Limit execution time to 10 seconds\n    max_iterations=5,       # Limit the number of iterations to 5\n    allow_dangerous_code=True,\n    verbose=True,\n)\n\n# Ask the agent a question\nresponse = agent.invoke(\"What is the survival rate of passengers?\")\nprint(response[\"output\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T16:24:06.647286Z","iopub.execute_input":"2025-01-23T16:24:06.647681Z","iopub.status.idle":"2025-01-23T16:24:08.398882Z","shell.execute_reply.started":"2025-01-23T16:24:06.647644Z","shell.execute_reply":"2025-01-23T16:24:08.397432Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Example 6: Handling Missing Data**\nThis example demonstrates how to use the agent to identify and handle missing data in the DataFrame.","metadata":{}},{"cell_type":"code","source":"# Create the agent\nagent = create_pandas_dataframe_agent(\n    model,\n    df,\n    agent_type=\"tool-calling\",\n    allow_dangerous_code=True,\n    verbose=True,\n)\n\n# Ask the agent to identify missing data\nresponse = agent.invoke(\"Which columns have missing data, and how should we handle them?\")\nprint(response[\"output\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T16:25:36.032002Z","iopub.execute_input":"2025-01-23T16:25:36.032347Z","iopub.status.idle":"2025-01-23T16:25:42.280114Z","shell.execute_reply.started":"2025-01-23T16:25:36.032324Z","shell.execute_reply":"2025-01-23T16:25:42.279012Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Example 7: Customizing Early Stopping**\nThis example shows how to customize the early stopping behavior using the `early_stopping_method` parameter. The agent will stop processing if it encounters an error or reaches a stopping condition.","metadata":{}},{"cell_type":"code","source":"# Create the agent with custom early stopping\nagent = create_pandas_dataframe_agent(\n    model,\n    df,\n    agent_type=\"tool-calling\",\n    early_stopping_method=\"force\",  # Force stop on errors\n    allow_dangerous_code=True,\n    verbose=True,\n)\n\n# Ask the agent a question\nresponse = agent.invoke(\"What is the average fare for each passenger class?\")\nprint(response[\"output\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T16:25:48.208399Z","iopub.execute_input":"2025-01-23T16:25:48.208803Z","iopub.status.idle":"2025-01-23T16:25:50.56875Z","shell.execute_reply.started":"2025-01-23T16:25:48.20877Z","shell.execute_reply":"2025-01-23T16:25:50.567814Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n## Best Practices\n\n### Example 1: Using `create_pandas_dataframe_agent` for Titanic Dataset Analysis\nThis example demonstrates how to use the `create_pandas_dataframe_agent` to analyze the Titanic dataset. The agent is capable of performing various data analysis tasks, such as data exploration, filtering, aggregation, and visualization, using natural language queries.\n\n#### **Step 1: Load the Dataset and Initialize the Agent**\nThis code block loads the Titanic dataset and initializes the `create_pandas_dataframe_agent`. The agent is configured to use a language model (`model`) and is set to allow the execution of potentially dangerous code (e.g., Python code execution). The `verbose` flag enables detailed logging of the agent's operations.","metadata":{}},{"cell_type":"code","source":"# Load the Titanic dataset\ndf = pd.read_csv(\"/kaggle/input/titanic-dataset/titanic.csv\")\n\n# Create the agent\nagent_executor = create_pandas_dataframe_agent(\n    model,\n    df,\n    agent_type=\"tool-calling\",\n    allow_dangerous_code=True,\n    verbose=True\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **Step 2: Perform Data Analysis with the Agent**\nThis code block demonstrates how to use the agent to answer various questions about the Titanic dataset. The questions range from basic exploration to complex analysis, and the agent dynamically processes the data to provide insights. The `pretty_print_response` function is used to format and print the responses in a clean and readable way.","metadata":{}},{"cell_type":"code","source":"# Question 1: Basic Data Exploration\nresponse_1 = agent_executor.invoke(\"What are the columns in the dataset and their data types?\")\npretty_print_response(\"1. What are the columns in the dataset and their data types?\", response_1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Question 2: Filtering Data\nresponse_2 = agent_executor.invoke(\"Find all passengers who survived.\")\npretty_print_response(\"2. Find all passengers who survived.\", response_2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Question 3: Aggregation\nresponse_3 = agent_executor.invoke(\"What is the average age of passengers?\")\npretty_print_response(\"3. What is the average age of passengers?\", response_3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Question 4: Grouping and Aggregation\nresponse_4 = agent_executor.invoke(\"What is the average fare for each passenger class?\")\npretty_print_response(\"4. What is the average fare for each passenger class?\", response_4)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Question 5: Visualization\nresponse_5 = agent_executor.invoke(\"Create a bar chart showing the number of passengers in each class.\")\npretty_print_response(\"5. Create a bar chart showing the number of passengers in each class.\", response_5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Question 6: Conditional Analysis\nresponse_6 = agent_executor.invoke(\"What is the survival rate of female passengers?\")\npretty_print_response(\"6. What is the survival rate of female passengers?\", response_6)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Question 7: Handling Missing Data\nresponse_7 = agent_executor.invoke(\"Which columns have missing data, and how should we handle them?\")\npretty_print_response(\"7. Which columns have missing data, and how should we handle them?\", response_7)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Question 8: Complex Query\nresponse_8 = agent_executor.invoke(\"Find the names of all passengers who survived, were in first class, and are older than 30.\")\npretty_print_response(\"8. Find the names of all passengers who survived, were in first class, and are older than 30.\", response_8)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **Step 3: Custom Prompt for Advanced Analysis**\nThis code block demonstrates how to use a custom prompt to guide the agent's behavior. The agent is initialized with a prefix that instructs it to act as a data analyst and provide concise answers. This is useful for tailoring the agent's responses to specific tasks or audiences.","metadata":{}},{"cell_type":"code","source":"# Question 9: Custom Prompt\nagent_executor_custom = create_pandas_dataframe_agent(\n    model,\n    df,\n    agent_type=\"tool-calling\",\n    prefix=\"You are a data analyst. Analyze the Titanic dataset and provide concise answers.\",\n    allow_dangerous_code=True,\n    verbose=True\n)\nresponse_9 = agent_executor_custom.invoke(\"What is the distribution of passengers by gender?\")\npretty_print_response(\"9. What is the distribution of passengers by gender?\", response_9)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Example 2: Using `create_pandas_dataframe_agent` for Data Analysis\n\nThis example demonstrates how to use the `create_pandas_dataframe_agent` to analyze multiple DataFrames without merging them in advance. The agent is capable of performing operations like filtering, aggregation, and conditional analysis across multiple DataFrames dynamically.\n\n#### **Step 1: Create DataFrames**\nThis code block creates three DataFrames:\n1. **`df_titanic`**: Contains passenger details like `PassengerId`, `Name`, `Pclass`, and `Fare`.\n2. **`df_fare_category`**: Maps `Pclass` to `FareCategory` (e.g., First Class, Second Class, Economy).\n3. **`df_discount`**: Contains `PassengerId` and `Discount` percentages.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Create the first DataFrame: Titanic passenger data\ndata_titanic = {\n    \"PassengerId\": [1, 2, 3, 4, 5],\n    \"Name\": [\"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John Bradley\", \"Heikkinen, Miss. Laina\", \"Futrelle, Mrs. Jacques Heath\", \"Allen, Mr. William Henry\"],\n    \"Pclass\": [3, 1, 3, 1, 3],\n    \"Fare\": [7.25, 71.2833, 7.925, 53.1, 8.05],\n}\n\ndf_titanic = pd.DataFrame(data_titanic)\n\n# Create the second DataFrame: Fare category data\ndata_fare_category = {\n    \"Pclass\": [1, 2, 3],\n    \"FareCategory\": [\"First Class\", \"Second Class\", \"Economy\"],\n}\n\ndf_fare_category = pd.DataFrame(data_fare_category)\n\n# Create the third DataFrame: Discount data\ndata_discount = {\n    \"PassengerId\": [1, 2, 3, 4, 5],\n    \"Discount\": [0.0, 5.0, 0.0, 10.0, 0.0],  # Discounts in percentage\n}\n\ndf_discount = pd.DataFrame(data_discount)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T16:08:17.982576Z","iopub.execute_input":"2025-01-23T16:08:17.982997Z","iopub.status.idle":"2025-01-23T16:08:17.991932Z","shell.execute_reply.started":"2025-01-23T16:08:17.982969Z","shell.execute_reply":"2025-01-23T16:08:17.990892Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **Step 2: Initialize the Agent**\nThis code block initializes the `create_pandas_dataframe_agent` with multiple DataFrames. The agent can dynamically join, filter, and analyze data across these DataFrames.","metadata":{}},{"cell_type":"code","source":"from langchain_experimental.agents import create_pandas_dataframe_agent\n\n# Create the agent with multiple DataFrames\nagent = create_pandas_dataframe_agent(\n    model,\n    [df_titanic, df_fare_category, df_discount],  # Pass multiple DataFrames as a list\n    agent_type=\"tool-calling\",\n    allow_dangerous_code=True,\n    verbose=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T16:08:17.993222Z","iopub.execute_input":"2025-01-23T16:08:17.993583Z","iopub.status.idle":"2025-01-23T16:08:18.99508Z","shell.execute_reply.started":"2025-01-23T16:08:17.993542Z","shell.execute_reply":"2025-01-23T16:08:18.994204Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **Step 3: Perform Data Analysis**\nThis code block demonstrates how to use the agent to answer various questions about the data. The questions range from basic exploration to complex analysis, and the agent dynamically processes the DataFrames to provide insights.","metadata":{}},{"cell_type":"code","source":"# Question 1: Basic Data Exploration\nresponse_1 = agent.invoke(\"What are the columns in each DataFrame and their data types?\")\npretty_print_response(\"1. What are the columns in each DataFrame and their data types?\", response_1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T16:08:18.996744Z","iopub.execute_input":"2025-01-23T16:08:18.997242Z","iopub.status.idle":"2025-01-23T16:08:23.784312Z","shell.execute_reply.started":"2025-01-23T16:08:18.997214Z","shell.execute_reply":"2025-01-23T16:08:23.783229Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Question 2: Filtering Data\nresponse_2 = agent.invoke(\"Find all passengers in First Class by joining df_titanic and df_fare_category.\")\npretty_print_response(\"2. Find all passengers in First Class by joining df_titanic and df_fare_category.\", response_2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T16:08:23.785574Z","iopub.execute_input":"2025-01-23T16:08:23.785876Z","iopub.status.idle":"2025-01-23T16:08:31.999105Z","shell.execute_reply.started":"2025-01-23T16:08:23.785851Z","shell.execute_reply":"2025-01-23T16:08:31.998092Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Question 3: Aggregation\nresponse_3 = agent.invoke(\"What is the average fare for each fare category? Use df_titanic and df_fare_category.\")\npretty_print_response(\"3. What is the average fare for each fare category? Use df_titanic and df_fare_category.\", response_3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T16:08:32.00035Z","iopub.execute_input":"2025-01-23T16:08:32.000693Z","iopub.status.idle":"2025-01-23T16:08:39.011547Z","shell.execute_reply.started":"2025-01-23T16:08:32.000666Z","shell.execute_reply":"2025-01-23T16:08:39.01056Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Question 4: Conditional Analysis\nresponse_4 = agent.invoke(\"What is the total fare for passengers who received a discount? Use df_titanic and df_discount.\")\npretty_print_response(\"4. What is the total fare for passengers who received a discount? Use df_titanic and df_discount.\", response_4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T16:08:39.012488Z","iopub.execute_input":"2025-01-23T16:08:39.012771Z","iopub.status.idle":"2025-01-23T16:08:54.934664Z","shell.execute_reply.started":"2025-01-23T16:08:39.012748Z","shell.execute_reply":"2025-01-23T16:08:54.933712Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Question 5: Complex Query\nresponse_5 = agent.invoke(\"Find the names of passengers who paid more than $50 in total fare. Use all DataFrames.\")\npretty_print_response(\"5. Find the names of passengers who paid more than $50 in total fare. Use all DataFrames.\", response_5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T16:08:54.935575Z","iopub.execute_input":"2025-01-23T16:08:54.935958Z","iopub.status.idle":"2025-01-23T16:09:23.015722Z","shell.execute_reply.started":"2025-01-23T16:08:54.935924Z","shell.execute_reply":"2025-01-23T16:09:23.014784Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Question 6: Handling Missing Data\nresponse_6 = agent.invoke(\"Are there any missing values in df_titanic?\")\npretty_print_response(\"6. Are there any missing values in df_titanic?\", response_6)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T16:09:23.017537Z","iopub.execute_input":"2025-01-23T16:09:23.017847Z","iopub.status.idle":"2025-01-23T16:09:27.548363Z","shell.execute_reply.started":"2025-01-23T16:09:23.017823Z","shell.execute_reply":"2025-01-23T16:09:27.547498Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Question 7: Custom Prompt\nresponse_7 = agent.invoke(\"You are a data analyst. Analyze the DataFrames and provide insights about fare distribution.\")\npretty_print_response(\"7. Analyze the DataFrames and provide insights about fare distribution.\", response_7)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T16:09:27.549317Z","iopub.execute_input":"2025-01-23T16:09:27.549562Z","iopub.status.idle":"2025-01-23T16:09:48.842448Z","shell.execute_reply.started":"2025-01-23T16:09:27.549542Z","shell.execute_reply":"2025-01-23T16:09:48.841414Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n## Conclusion\nThe `create_pandas_dataframe_agent` function bridges the gap between natural language processing and data analysis, enabling users to interact with Pandas DataFrames in an intuitive and efficient manner. By leveraging the strengths of LLMs, this tool simplifies complex data tasks and makes them accessible to a broader audience. However, the power of this functionality comes with a responsibility to use it securely. Users must ensure that they operate in a safe, sandboxed environment and understand the risks associated with executing arbitrary code. When used responsibly, this tool can significantly enhance productivity and unlock new possibilities for data-driven decision-making.","metadata":{}}]}