{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Adaptive RAG with LangGraph\n\n## Introduction\n\nAdaptive RAG is an advanced strategy for Retrieval-Augmented Generation (RAG) that combines **query analysis** with **active/self-corrective RAG** to dynamically adapt the retrieval and generation process based on the nature of the user's query. This approach ensures that the system can handle a wide range of questions effectively, from simple factual queries to complex, multi-step reasoning tasks. \n\nIn this implementation, we use **LangGraph** to build a workflow that routes queries between two primary paths:\n1. **Web Search**: For questions related to recent events or topics not covered in the indexed documents.\n2. **Self-Corrective RAG**: For questions related to the indexed documents, where the system retrieves relevant information, generates an answer, and iteratively refines the response to ensure accuracy and relevance.\n\nBy leveraging LangGraph's graph-based workflow, we create a flexible and adaptive RAG pipeline that can dynamically switch between retrieval strategies, evaluate the quality of generated answers, and self-correct when necessary. This approach mirrors the principles outlined in recent research, where query analysis is used to route queries across different retrieval strategies, such as **No Retrieval**, **Single-shot RAG**, and **Iterative RAG**.","metadata":{}},{"cell_type":"markdown","source":"## Step 0: Installation Commands\n\nThe provided code installs several Python libraries commonly used in building and working with language models, agents, and related tools. Here's a brief description of each library:\n\n1. **`langchain-openai`**: Integrates OpenAI's models (like GPT) with the LangChain framework for building language model applications.\n2. **`langchain-anthropic`**: Integrates Anthropic's models (like Claude) with LangChain.\n3. **`langchain_community`**: Provides community-contributed tools, integrations, and utilities for LangChain.\n4. **`langchain_experimental`**: Contains experimental features and tools for LangChain that are still under development.\n5. **`langgraph`**: A library for building and visualizing graph-based workflows, often used in conjunction with LangChain.\n6. **`tiktoken`**: A tokenizer for OpenAI models, used to count and manage tokens in text.\n7. **`chromadb`**: A vector database for storing and querying embeddings, often used in semantic search and retrieval-augmented generation (RAG) pipelines.\n8. **`duckduckgo_search`**: A Python wrapper for the DuckDuckGo search engine, useful for retrieving real-time information from the web.\n\nThese libraries are essential for building advanced language model applications, including chatbots, agents, and retrieval systems.","metadata":{}},{"cell_type":"code","source":"!pip install -qU langchain-openai\n!pip install -qU langchain-anthropic\n!pip install -qU langchain_community\n!pip install -qU langchain_experimental\n!pip install -qU langgraph\n!pip install -qU tiktoken\n!pip install -qU chromadb\n!pip install -qU duckduckgo_search","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-15T17:50:32.006046Z","iopub.execute_input":"2025-01-15T17:50:32.006462Z","iopub.status.idle":"2025-01-15T17:51:36.289552Z","shell.execute_reply.started":"2025-01-15T17:50:32.006435Z","shell.execute_reply":"2025-01-15T17:51:36.288137Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 1: Build Index\n\nThis step sets up the document index by loading web-based documents, splitting them into chunks, and storing them in a vector store (Chroma) using OpenAI embeddings. The index is used for efficient retrieval of relevant documents during query processing.","metadata":{}},{"cell_type":"code","source":"# Phase 1: Build Index\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_openai import OpenAIEmbeddings\nfrom kaggle_secrets import UserSecretsClient\n\n# Fetch API key securely\nuser_secrets = UserSecretsClient()\nmy_api_key = user_secrets.get_secret(\"my-openai-api-key\")\n#my_api_key = user_secrets.get_secret(\"my-deepseek-api-key\")\n\n# Initialize OpenAI embeddings\nembd = OpenAIEmbeddings(model=\"text-embedding-3-small\", api_key=my_api_key)\n\n# URLs of documents to index\nurls = [\n    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n]\n\n# Load documents from URLs\ndocs = [WebBaseLoader(url).load() for url in urls]\ndocs_list = [item for sublist in docs for item in sublist]\n\n# Split documents into smaller chunks for efficient processing\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n    chunk_size=500, chunk_overlap=0\n)\ndoc_splits = text_splitter.split_documents(docs_list)\n\n# Store documents in Chroma vector store\nvectorstore = Chroma.from_documents(\n    documents=doc_splits, collection_name=\"rag-chroma\", embedding=embd\n)\nretriever = vectorstore.as_retriever()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T20:00:47.651631Z","iopub.execute_input":"2025-01-15T20:00:47.65212Z","iopub.status.idle":"2025-01-15T20:00:50.94099Z","shell.execute_reply.started":"2025-01-15T20:00:47.652088Z","shell.execute_reply":"2025-01-15T20:00:50.940129Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 2: Router\n\nThe router determines whether a user query should be answered using the vector store (for domain-specific questions) or web search (for general questions). It uses a structured LLM (GPT-4o-mini) to classify the query.","metadata":{}},{"cell_type":"code","source":"# Phase 2: Router\nfrom typing import Literal\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\nfrom pydantic import BaseModel, Field\n\n# Define a Pydantic model for routing decisions\nclass RouteQuery(BaseModel):\n    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n\n    datasource: Literal[\"vectorstore\", \"web_search\"] = Field(\n        ...,\n        description=\"Given a user question, choose to route it to web search or a vectorstore.\",\n    )\n\n# Initialize LLM for routing\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, api_key=my_api_key)\nstructured_llm_router = llm.with_structured_output(RouteQuery)\n\n# Define the routing prompt\nsystem = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\nThe vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.\nUse the vectorstore for questions on these topics. Otherwise, use web-search.\"\"\"\nroute_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system),\n        (\"human\", \"{question}\"),\n    ]\n)\n\n# Create the question router chain\nquestion_router = route_prompt | structured_llm_router\n\n# Test the router with sample questions\nprint(question_router.invoke({\"question\": \"Who will the Bears draft first in the NFL draft?\"}))\nprint(question_router.invoke({\"question\": \"What are the types of agent memory?\"}))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T20:00:50.942055Z","iopub.execute_input":"2025-01-15T20:00:50.942342Z","iopub.status.idle":"2025-01-15T20:00:51.95789Z","shell.execute_reply.started":"2025-01-15T20:00:50.942317Z","shell.execute_reply":"2025-01-15T20:00:51.956868Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 3: Retrieval Grader\n\nThis step evaluates the relevance of retrieved documents to the user's query. It uses a binary grading system (yes/no) to filter out irrelevant documents, ensuring only contextually appropriate content is used for answer generation.","metadata":{}},{"cell_type":"code","source":"# Phase 3: Retrieval Grader\nclass GradeDocuments(BaseModel):\n    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n\n    binary_score: str = Field(description=\"Documents are relevant to the question, 'yes' or 'no'\")\n\n# Initialize LLM for grading\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, api_key=my_api_key)\nstructured_llm_grader = llm.with_structured_output(GradeDocuments)\n\n# Define the grading prompt\nsystem = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\ngrade_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system),\n        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n    ]\n)\n\n# Create the retrieval grader chain\nretrieval_grader = grade_prompt | structured_llm_grader\n\n# Test the retrieval grader\nquestion = \"agent memory\"\ndocs = retriever.invoke(question)\ndoc_txt = docs[1].page_content\n\nresult = retrieval_grader.invoke({\"question\": question, \"document\": doc_txt})\nprint(result)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T20:00:51.959593Z","iopub.execute_input":"2025-01-15T20:00:51.960008Z","iopub.status.idle":"2025-01-15T20:00:53.892589Z","shell.execute_reply.started":"2025-01-15T20:00:51.959956Z","shell.execute_reply":"2025-01-15T20:00:53.891614Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 4: Generate\n\nThe generate phase constructs a RAG chain to produce answers based on the retrieved documents and the user's query. It uses a pre-defined prompt and GPT-4o-mini to generate coherent and contextually accurate responses.","metadata":{}},{"cell_type":"code","source":"# Phase 4: Generate\nfrom langchain import hub\nfrom langchain_core.output_parsers import StrOutputParser\n\n# Pull the RAG prompt from the hub\nprompt = hub.pull(\"rlm/rag-prompt\")\n\n# Initialize LLM for generation\nllm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0, api_key=my_api_key)\n\n# Create the RAG chain\nrag_chain = prompt | llm | StrOutputParser()\n\n# Generate an answer using the RAG chain\ngeneration = rag_chain.invoke({\"context\": docs, \"question\": question})\nprint(generation)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T20:00:53.89383Z","iopub.execute_input":"2025-01-15T20:00:53.894092Z","iopub.status.idle":"2025-01-15T20:00:55.528057Z","shell.execute_reply.started":"2025-01-15T20:00:53.894069Z","shell.execute_reply":"2025-01-15T20:00:55.527125Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Show what the prompt looks like\nprompt = hub.pull(\"rlm/rag-prompt\").pretty_print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T20:00:55.52907Z","iopub.execute_input":"2025-01-15T20:00:55.529403Z","iopub.status.idle":"2025-01-15T20:00:55.628504Z","shell.execute_reply.started":"2025-01-15T20:00:55.529364Z","shell.execute_reply":"2025-01-15T20:00:55.627414Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 5: Hallucination Grader\n\nThis step checks if the generated answer is grounded in the retrieved documents. It ensures the answer is factually supported and not hallucinated, using a binary grading system.","metadata":{}},{"cell_type":"code","source":"# Phase 5: Hallucination Grader\nclass GradeHallucinations(BaseModel):\n    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n\n    binary_score: str = Field(description=\"Answer is grounded in the facts, 'yes' or 'no'\")\n\n# Initialize LLM for hallucination grading\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, api_key=my_api_key)\nstructured_llm_grader = llm.with_structured_output(GradeHallucinations)\n\n# Define the hallucination grading prompt\nsystem = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n \n     Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\"\"\nhallucination_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system),\n        (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n    ]\n)\n\n# Create the hallucination grader chain\nhallucination_grader = hallucination_prompt | structured_llm_grader\n\n# Test the hallucination grader\nresult = hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})\nprint(result)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T20:00:55.629582Z","iopub.execute_input":"2025-01-15T20:00:55.629943Z","iopub.status.idle":"2025-01-15T20:00:56.406659Z","shell.execute_reply.started":"2025-01-15T20:00:55.629907Z","shell.execute_reply":"2025-01-15T20:00:56.405742Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 6: Answer Grader\n\nThe answer grader evaluates whether the generated answer fully addresses the user's query. It ensures the response is relevant and resolves the question effectively.","metadata":{}},{"cell_type":"code","source":"# Phase 6: Answer Grader\nclass GradeAnswer(BaseModel):\n    \"\"\"Binary score to assess answer addresses question.\"\"\"\n\n    binary_score: str = Field(description=\"Answer addresses the question, 'yes' or 'no'\")\n\n# Initialize LLM for answer grading\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, api_key=my_api_key)\nstructured_llm_grader = llm.with_structured_output(GradeAnswer)\n\n# Define the answer grading prompt\nsystem = \"\"\"You are a grader assessing whether an answer addresses / resolves a question \\n \n     Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question.\"\"\"\nanswer_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system),\n        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n    ]\n)\n\n# Create the answer grader chain\nanswer_grader = answer_prompt | structured_llm_grader\n\n# Test the answer grader\nresult = answer_grader.invoke({\"question\": question, \"generation\": generation})\nprint(result)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T20:00:56.407577Z","iopub.execute_input":"2025-01-15T20:00:56.407827Z","iopub.status.idle":"2025-01-15T20:00:57.271996Z","shell.execute_reply.started":"2025-01-15T20:00:56.407806Z","shell.execute_reply":"2025-01-15T20:00:57.271003Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 7: Question Re-writer\n\nThis step rewrites the user's query to optimize it for vector store retrieval. It improves the semantic understanding of the query, enhancing the relevance of retrieved documents.","metadata":{}},{"cell_type":"code","source":"# Phase 7: Question Re-writer\n# Initialize LLM for question rewriting\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, api_key=my_api_key)\n\n# Define the question rewriting prompt\nsystem = \"\"\"You are a question re-writer that converts an input question to a better version that is optimized \\n \n     for vectorstore retrieval. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\nre_write_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system),\n        (\n            \"human\",\n            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n        ),\n    ]\n)\n\n# Create the question rewriter chain\nquestion_rewriter = re_write_prompt | llm | StrOutputParser()\n\n# Test the question rewriter\nresult = question_rewriter.invoke({\"question\": question})\nprint(result)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T20:00:57.274154Z","iopub.execute_input":"2025-01-15T20:00:57.274434Z","iopub.status.idle":"2025-01-15T20:00:57.860775Z","shell.execute_reply.started":"2025-01-15T20:00:57.274409Z","shell.execute_reply":"2025-01-15T20:00:57.859541Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 8: Search\n\nThe search phase integrates DuckDuckGo Search for web-based queries. It retrieves web results when the router determines that a question is better answered using external sources.","metadata":{}},{"cell_type":"code","source":"# Phase 8: Search\nfrom langchain_community.tools import DuckDuckGoSearchRun\n\n# Initialize DuckDuckGo search tool\nweb_search_tool = DuckDuckGoSearchRun()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T20:00:57.86223Z","iopub.execute_input":"2025-01-15T20:00:57.862647Z","iopub.status.idle":"2025-01-15T20:00:57.867454Z","shell.execute_reply.started":"2025-01-15T20:00:57.862608Z","shell.execute_reply":"2025-01-15T20:00:57.866331Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 9: Define Graph State and Flow\n\nThis step defines the **state** and **flow** of the Retrieval-Augmented Generation (RAG) pipeline as a **graph-based workflow**. The graph is composed of interconnected **nodes**, each representing a specific task in the pipeline, such as document retrieval, question routing, answer generation, and evaluation. The **state** of the graph is maintained throughout the workflow, ensuring that each node has access to the necessary information (e.g., the user's question, retrieved documents, generated answers).","metadata":{}},{"cell_type":"code","source":"# Phase 9: Define Graph State and Flow\nfrom typing import List, Dict, Any\nfrom typing_extensions import TypedDict\nfrom langchain.schema import Document\nfrom langgraph.graph import END, StateGraph, START\n\nclass GraphState(TypedDict):\n    \"\"\"\n    Represents the state of our graph.\n\n    Attributes:\n        question: question\n        generation: LLM generation\n        documents: list of documents\n    \"\"\"\n\n    question: str\n    generation: str\n    documents: List[str]\n\ndef retrieve_node(state: GraphState) -> Dict[str, Any]:\n    \"\"\"\n    Retrieve documents based on the user's question.\n\n    Args:\n        state (GraphState): The current graph state.\n\n    Returns:\n        Dict[str, Any]: Updated state with retrieved documents.\n    \"\"\"\n    print(\"---RETRIEVE---\")\n    question = state[\"question\"]\n    documents = retriever.invoke(question)\n    return {\"documents\": documents, \"question\": question}\n\ndef generate_node(state: GraphState) -> Dict[str, Any]:\n    \"\"\"\n    Generate an answer using the RAG chain.\n\n    Args:\n        state (GraphState): The current graph state.\n\n    Returns:\n        Dict[str, Any]: Updated state with generated answer.\n    \"\"\"\n    print(\"---GENERATE---\")\n    question = state[\"question\"]\n    documents = state[\"documents\"]\n    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n\ndef grade_documents_node(state: GraphState) -> Dict[str, Any]:\n    \"\"\"\n    Grade the relevance of retrieved documents to the question.\n\n    Args:\n        state (GraphState): The current graph state.\n\n    Returns:\n        Dict[str, Any]: Updated state with filtered relevant documents.\n    \"\"\"\n    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n    question = state[\"question\"]\n    documents = state[\"documents\"]\n    filtered_docs = []\n    for d in documents:\n        score = retrieval_grader.invoke({\"question\": question, \"document\": d.page_content})\n        grade = score.binary_score\n        if grade == \"yes\":\n            print(\"---GRADE: DOCUMENT RELEVANT---\")\n            filtered_docs.append(d)\n        else:\n            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n            continue\n    return {\"documents\": filtered_docs, \"question\": question}\n\ndef transform_query_node(state: GraphState) -> Dict[str, Any]:\n    \"\"\"\n    Rewrite the user's question for better retrieval.\n\n    Args:\n        state (GraphState): The current graph state.\n\n    Returns:\n        Dict[str, Any]: Updated state with rephrased question.\n    \"\"\"\n    print(\"---TRANSFORM QUERY---\")\n    question = state[\"question\"]\n    documents = state[\"documents\"]\n    better_question = question_rewriter.invoke({\"question\": question})\n    return {\"documents\": documents, \"question\": better_question}\n\ndef web_search_node(state: GraphState) -> Dict[str, Any]:\n    \"\"\"\n    Perform a web search using DuckDuckGo.\n\n    Args:\n        state (GraphState): The current graph state.\n\n    Returns:\n        Dict[str, Any]: Updated state with web search results.\n    \"\"\"\n    print(\"---WEB SEARCH---\")\n    question = state[\"question\"]\n    docs = web_search_tool.invoke({\"query\": question})\n    \n    # Handle the case where docs is a list of strings or a single string\n    if isinstance(docs, str):\n        web_results = docs  # If docs is a single string, use it directly\n    elif isinstance(docs, list):\n        web_results = \"\\n\".join(docs)  # If docs is a list of strings, join them\n    else:\n        raise ValueError(f\"Unexpected type for web search results: {type(docs)}\")\n    \n    web_results = Document(page_content=web_results)\n    return {\"documents\": [web_results], \"question\": question}\n\ndef route_search_store(state: GraphState) -> Literal[\"web_search_node\", \"retrieve_node\"]:\n    \"\"\"\n    Route the question to either web search or RAG.\n\n    Args:\n        state (GraphState): The current graph state.\n\n    Returns:\n        str: Next node to call (\"web_search_node\", \"vectorstore_node\").\n    \"\"\"\n    print(\"---ROUTE QUESTION---\")\n    question = state[\"question\"]\n    source = question_router.invoke({\"question\": question})\n    if source.datasource == \"web_search\":\n        print(\"---ROUTE QUESTION TO WEB SEARCH---\")\n        return \"web_search_node\"\n    elif source.datasource == \"vectorstore\":\n        print(\"---ROUTE QUESTION TO RAG---\")\n        return \"retrieve_node\"\n\ndef decide_to_generate(state: GraphState) -> Literal[\"transform_query_node\", \"generate_node\"]:\n    \"\"\"\n    Decide whether to generate an answer or re-generate the question.\n\n    Args:\n        state (GraphState): The current graph state.\n\n    Returns:\n        str: Next node to call (\"transform_query_node\", \"generate_node\").\n    \"\"\"\n    print(\"---ASSESS GRADED DOCUMENTS---\")\n    filtered_documents = state[\"documents\"]\n    if not filtered_documents:\n        print(\"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\")\n        return \"transform_query_node\"\n    else:\n        print(\"---DECISION: GENERATE---\")\n        return \"generate_node\"\n\ndef route_generate_transform(state: GraphState) -> Literal[\"generate_node\", \"transform_query_node\", END]:\n    \"\"\"\n    Grade the generation against the documents and the question.\n\n    Args:\n        state (GraphState): The current graph state.\n\n    Returns:\n        str: Decision for next node (\"generate_node\", \"transform_query_node\", END).\n    \"\"\"\n    print(\"---CHECK HALLUCINATIONS---\")\n    question = state[\"question\"]\n    documents = state[\"documents\"]\n    generation = state[\"generation\"]\n    score = hallucination_grader.invoke({\"documents\": documents, \"generation\": generation})\n    grade = score.binary_score\n    if grade == \"yes\":\n        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n        grade = score.binary_score\n        if grade == \"yes\":\n            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n            return END\n        else:\n            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n            return \"transform_query_node\"\n    else:\n        print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n        return \"generate_node\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T20:11:03.306081Z","iopub.execute_input":"2025-01-15T20:11:03.30647Z","iopub.status.idle":"2025-01-15T20:11:03.322636Z","shell.execute_reply.started":"2025-01-15T20:11:03.306436Z","shell.execute_reply":"2025-01-15T20:11:03.321455Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 10: Compile and Use Graph\n\nThe graph is compiled into an executable workflow. It orchestrates the interaction between nodes, ensuring the pipeline processes queries efficiently and produces accurate results.\n\n1. **Graph State (`GraphState`):**\n   - A `TypedDict` that represents the state of the graph at any point in the workflow.\n   - Contains:\n     - **`question`**: The user's input question.\n     - **`generation`**: The LLM-generated answer.\n     - **`documents`**: A list of retrieved documents relevant to the question.\n\n2. **Nodes:**\n   - Each node is a function that performs a specific task and updates the graph state.\n   - Key nodes include:\n     - **`retrieve_node`**: Retrieves documents from the vector store based on the user's question.\n     - **`generate_node`**: Generates an answer using the RAG chain.\n     - **`grade_documents_node`**: Grades the relevance of retrieved documents to the question.\n     - **`transform_query_node`**: Rewrites the user's question for better retrieval.\n     - **`web_search_node`**: Performs a web search using DuckDuckGo for external information.\n     - **`route_search_store`**: Routes the question to either web search or the vector store.\n     - **`decide_to_generate`**: Decides whether to generate an answer or rephrase the question.\n     - **`route_generate_transform`**: Evaluates the generated answer and decides the next step (e.g., end the workflow, rephrase the question, or regenerate the answer).\n\n3. **Conditional Edges:**\n   - The graph uses **conditional edges** to dynamically route the workflow based on the results of each node.\n   - For example:\n     - If the retrieved documents are irrelevant, the workflow routes to `transform_query_node` to rephrase the question.\n     - If the generated answer is not grounded in the documents, the workflow routes back to `generate_node` to regenerate the answer.\n\n4. **Flow Logic:**\n   - The workflow starts with the user's question and routes it to either the vector store or web search based on relevance.\n   - Retrieved documents are graded for relevance, and irrelevant documents are filtered out.\n   - The RAG chain generates an answer, which is then evaluated for hallucinations and relevance to the question.\n   - If the answer is satisfactory, the workflow ends. Otherwise, it loops back to rephrase the question or regenerate the answer.\n","metadata":{}},{"cell_type":"code","source":"# Phase 10: Compile and Use Graph\n# Initialize the workflow\nworkflow = StateGraph(GraphState)\n\n# Define the nodes\nworkflow.add_node(\"web_search_node\", web_search_node)\nworkflow.add_node(\"retrieve_node\", retrieve_node)\nworkflow.add_node(\"grade_documents_node\", grade_documents_node)\nworkflow.add_node(\"generate_node\", generate_node)\nworkflow.add_node(\"transform_query_node\", transform_query_node)\n\n# Build the graph\nworkflow.add_conditional_edges(START, route_search_store, [\"web_search_node\", \"retrieve_node\"])\nworkflow.add_edge(\"web_search_node\", \"generate_node\")\nworkflow.add_edge(\"retrieve_node\", \"grade_documents_node\")\nworkflow.add_conditional_edges(\"grade_documents_node\", decide_to_generate, [\"transform_query_node\", \"generate_node\"])\nworkflow.add_edge(\"transform_query_node\", \"retrieve_node\")\nworkflow.add_conditional_edges(\"generate_node\", route_generate_transform, [\"generate_node\", \"transform_query_node\", END])\n\n# Compile the workflow\napp = workflow.compile()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T20:11:03.324146Z","iopub.execute_input":"2025-01-15T20:11:03.324449Z","iopub.status.idle":"2025-01-15T20:11:03.345752Z","shell.execute_reply.started":"2025-01-15T20:11:03.324424Z","shell.execute_reply":"2025-01-15T20:11:03.34466Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 11: Visualization\n\nThis optional phase visualizes the graph structure using Mermaid and IPython. It provides a graphical representation of the pipeline's flow and decision points.","metadata":{}},{"cell_type":"code","source":"# Phase 11: Visualization\nfrom IPython.display import Image, display\n\n# Visualize the graph (optional, requires additional dependencies)\ntry:\n    display(Image(app.get_graph(xray=True).draw_mermaid_png()))\nexcept Exception:\n    pass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T20:11:03.347403Z","iopub.execute_input":"2025-01-15T20:11:03.347799Z","iopub.status.idle":"2025-01-15T20:11:03.453663Z","shell.execute_reply.started":"2025-01-15T20:11:03.347758Z","shell.execute_reply":"2025-01-15T20:11:03.452685Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 12: Execute the Graph\n\nThe final step executes the graph with a user query. It processes the query through the pipeline, prints intermediate outputs in JSON format, and displays the final generated answer. Non-serializable objects are converted to dictionaries for clean output.","metadata":{}},{"cell_type":"code","source":"# Phase 12: Execute the Graph with an Input\nimport json\nimport warnings\n\n# Suppress LangSmith API key warning (if not using LangSmith)\nwarnings.filterwarnings(\"ignore\", category=UserWarning, message=\"API key must be provided when using hosted LangSmith API\")\n\n# Helper function to convert non-serializable objects to dictionaries\ndef convert_to_serializable(obj):\n    if hasattr(obj, \"dict\"):  # Check if the object has a .dict() method\n        return obj.dict()\n    elif isinstance(obj, (list, tuple)):  # Handle lists and tuples\n        return [convert_to_serializable(item) for item in obj]\n    elif isinstance(obj, dict):  # Handle dictionaries\n        return {key: convert_to_serializable(value) for key, value in obj.items()}\n    else:  # Return the object as-is if it's already serializable\n        return obj\n\n# Run the graph with an input question\ninputs = {\"question\": \"What are the types of agent memory?\"}\nfor output in app.stream(inputs):\n    for key, value in output.items():\n        print(f\"Output from node '{key}':\")\n        print(\"-\"*80)\n        # Convert non-serializable objects to dictionaries\n        serializable_value = convert_to_serializable(value)\n        # Print the serialized value as a JSON string\n        print(json.dumps(serializable_value, indent=2))\n    print(\"=\"*80)\n\n# Print the final generated answer\nprint(f\"final generation:\\n{value['generation']}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T20:11:24.153452Z","iopub.execute_input":"2025-01-15T20:11:24.153876Z","iopub.status.idle":"2025-01-15T20:11:31.291576Z","shell.execute_reply.started":"2025-01-15T20:11:24.153845Z","shell.execute_reply":"2025-01-15T20:11:31.290595Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Run the graph with an input question related to agents\ninputs = {\"question\": \"What are the key components of an AI agent?\"}\nfor output in app.stream(inputs):\n    for key, value in output.items():\n        print(f\"Output from node '{key}':\")\n        print(\"-\"*80)\n        # Convert non-serializable objects to dictionaries\n        serializable_value = convert_to_serializable(value)\n        # Print the serialized value as a JSON string\n        print(json.dumps(serializable_value, indent=2))\n    print(\"=\"*80)\n\n# Print the final generated answer\nprint(f\"final generation:\\n{value['generation']}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T20:12:09.940739Z","iopub.execute_input":"2025-01-15T20:12:09.941096Z","iopub.status.idle":"2025-01-15T20:12:17.613565Z","shell.execute_reply.started":"2025-01-15T20:12:09.941067Z","shell.execute_reply":"2025-01-15T20:12:17.612434Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Run the graph with an input question unrelated to agents\ninputs = {\"question\": \"What is the capital of France?\"}\nfor output in app.stream(inputs):\n    for key, value in output.items():\n        print(f\"Output from node '{key}':\")\n        print(\"-\"*80)\n        # Convert non-serializable objects to dictionaries\n        serializable_value = convert_to_serializable(value)\n        # Print the serialized value as a JSON string\n        print(json.dumps(serializable_value, indent=2))\n    print(\"=\"*80)\n\n# Print the final generated answer\nprint(f\"final generation:\\n{value['generation']}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T20:11:03.524121Z","iopub.execute_input":"2025-01-15T20:11:03.52446Z","iopub.status.idle":"2025-01-15T20:11:07.077845Z","shell.execute_reply.started":"2025-01-15T20:11:03.524432Z","shell.execute_reply":"2025-01-15T20:11:07.076816Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Conclusion\n\nThe Adaptive RAG pipeline built with LangGraph demonstrates the power of combining **query analysis** with **self-corrective mechanisms** to create a robust and flexible question-answering system. By dynamically routing queries between **web search** and **self-corrective RAG**, the system ensures that users receive accurate and contextually relevant answers, regardless of the complexity or recency of the topic.\n\nThis implementation highlights the importance of modularity and adaptability in modern RAG systems. The use of LangGraph's graph-based workflow allows for seamless integration of multiple retrieval strategies, real-time evaluation of generated answers, and iterative refinement to improve response quality. As RAG systems continue to evolve, strategies like Adaptive RAG will play a crucial role in enhancing their ability to handle diverse and challenging queries effectively.\n\nIn conclusion, Adaptive RAG with LangGraph represents a significant step forward in building intelligent, self-correcting question-answering systems that can adapt to the needs of users and the nature of their queries.\n","metadata":{}}]}